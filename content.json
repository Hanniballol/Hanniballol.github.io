{"posts":[{"title":"ADT实现状态模式","text":"ADT(代数数据类型)是函数式语音中一种强大的语言特性，这次用它来实现状态模式 状态模式与策略模式存在某些相似性，它们都可以实现某种算法、业务逻辑的切换。以下是状态模式的定义： 状态模式允许一个对象在其内部状态改变时改变的行为，对象看起来似乎修改了它的类。 状态模式具体表现在： 状态决定行为，对象的行为由它内部的状态决定 对象的状态在运行期被改变时，它的行为也会因此而改变。从表面上看，同一个对象，在不同的运行时刻，行为是不一样的，就像是类被修改了一样 再次与策略模式做对比，策略模式通过在客户端切换不同的策略实现来改变算法；而在状态模式中，对象通过修改内部的状态来切换不同的行为方法。 现在我们举一个饮水机的例子，假如一个饮水机有3种工作状态，分别为未启动、制冷模式、制热模式，那么可以用密封类来封装一个代表不同饮水机状态的ADT。 sealed class WaterMachineState(open val machine: WaterMachine) { fun turnHeating() { if (this !is Heating) { print(&quot;turn heating&quot;) machine.state = machine.heating } else { print(&quot;The state is already heating mode.&quot;) } } fun turnCooling() { if (this !is Cooling) { print(&quot;turn cooling&quot;) machine.state = machine.cooling } else { print(&quot;The state is already cooling mode.&quot;) } } fun turnOff() { if (this !is Off) { print(&quot;turn off&quot;) machine.state = machine.off } else { print(&quot;The state is already off mode.&quot;) } }}class Off(override val machine: WaterMachine) : WatermachineState(machine)class Heating(override val machine: WaterMachine) : WatermachineState(machine)class Cooling(override val machine: WaterMachine) : WatermachineState(machine) 以上代码分析： WatermachineState是一个密封类，拥有一个构造参数为WaterMachine类对象 在WatermachineState类外部我们分别定义了Off,Heating,Cooling来代表饮水机的3种不同的工作状态，它们都继承了WaterMachineState类的machine成员属性及3个状态切换的方法 在每个切换状态的方法中，我们通过改变machine对象的state，来实现切换饮水机状态的目的 接着看WaterMachine类： class WaterMachine { var state: WaterMachineState val off = Off(this) val heating = Heating(this) val cooling = Cooling(this) init { this.state = off } fun turnHeating() { this.state.turnHeating() } fun turnCooling() { this.state.turnCooling() } fun turnOff() { this.state.turnOff() }} WaterMachine很简单，内部主要包含了一下成员属性和方法： 引用可变的WaterMachineState类对象state，用来表示当前饮水机所处的工作状态 分别表示3种不同状态的成员属性，off、heating、cooling，它们也是WaterMahineState类的3种子类对象；它们通过传入this进行构造，从而实现在WaterMachineState状态类内部，改变WaterMachine类的state引用值；当WaterMachine类对象初始化时，state默认为off，即饮水机处于未启动状态 3个直接调用的饮水机操作方法，分别执行对应state对象的3种操作，供客户端调用 如果办公室的小伙伴都喜欢喝冷水，早上一来就会把饮水机调整为制冷模式，但Shaw有吃泡面的习惯，他想泡面的时候，就会把饮水机变为制热，所以每次他吃了泡面，下一个喝水的同事就需要再切换回制冷。最后要下班了，Kim就会关闭饮水机的电源。 为了满足这一需求，我们设计了一个waterMachineOps函数： enum class Moment { EARLY_MORNING, DRINKING_WATER, INSTANCE_NOODLES, AFTER_WORK}fun waterMachineOps(machine: WaterMachine,moment: Moment) { when(moment) { EARLY_MORNING, DRINKING_WATER -&gt; when(machine.state) { !is WatermachineState.Cooling -&gt;machine.turnCooling() } INSTANCE_NOODLES -&gt; when(machine.state) { !is WatermachineState.Heating -&gt; machine.turnHeating() } AFTER_WORK -&gt; when(machine.state) { !is WatermachineState.Off -&gt; machine.turnOff() } }} 这个方法很好地处理了不同角色在不同需求场景下，应该对饮水机执行的不同操作。此外，当用when表达式处理枚举类时，默认的情况必须用else进行处理。然而，由于密封类在类型安全上的额外设计，我们在处理machine对象的state对象时，则不需要考虑这一细节，在语言表达上要简洁得多。","link":"/2020/01/21/ADT%E5%AE%9E%E7%8E%B0%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F/"},{"title":"Android平台WebRTC开启H264软编解码","text":"由于版权问题WebRTC一直不支持H264，直到Cisco宣布旗下的H264 Codec开源为OpenH264，并且替所有OpenH264的使用者支付了H264的专利费,以此为契机，在IETF的WebRTC会议中，把H264和VP8都列入了WebRTC所必须要支持的视频编码器，接下来Google终于在WebRTC中增加了对H264的支持，对于Android平台, 编码器是用OpenH264, 解码器是用FFMPEG, 也可以用 MediaCodec. 这个对于广大需要H264的公司来说是一大福音. 在下载的WebRTC代码中做稍许配置， 就可以使用H264了，本文介绍针对M86分支增加H264软编的步骤。 开启H264编解码打开开关 修改third_party/ffmpeg/ffmpeg_generated.gni： use_linux_config = is_linux || is_fuchsia-&gt; use_linux_config = is_linux || is_fuchsia || is_android 修改third_party/ffmpeg/chromium/config/Chrome/android/arm64/config.h: #define CONFIG_H264_DECODER 0-&gt; #define CONFIG_H264_DECODER 1 修改third_party/ffmpeg/chromium/config/Chrome/android/arm64/libavcodec/parser_list.c static const AVCodecParser * const parser_list[] = { ... &amp;ff_vp9_parser,+ &amp;ff_h264_parser, NULL }; 修改third_party/ffmpeg/chromium/config/Chrome/android/arm64/libavcodec/codec_list.c static const AVCodec * const codec_list[] = { ... &amp;ff_libopus_decoder,+ &amp;ff_h264_decoder, NULL }; 增加H264编解码支持 在sdk/android/api/org/webrtc中增加LibH264Decoder.java、LibH264Encoder.java，格式与原有Decoder、Encoder一致： LibH264Decoder.javapackage org.webrtc;public class LibH264Decoder extends WrappedNativeVideoDecoder { @Override public long createNativeVideoDecoder() { return nativeCreateDecoder(); } static native long nativeCreateDecoder();} LibH264Encoder.javapackage org.webrtc;public class LibH264Encoder extends WrappedNativeVideoEncoder { @Override public long createNativeVideoEncoder() { return nativeCreateEncoder(); } static native long nativeCreateEncoder(); @Override public boolean isHardwareEncoder() { return false; }} 在sdk/android/src/jni中添加h264_codec.cc，格式与原有vp8_codec.cc/vp9_codec.cc一致： #include &lt;jni.h&gt;#include &quot;modules/video_coding/codecs/h264/include/h264.h&quot;#include &quot;sdk/android/generated_libH264_jni/LibH264Decoder_jni.h&quot;#include &quot;sdk/android/generated_libH264_jni/LibH264Encoder_jni.h&quot;#include &quot;sdk/android/src/jni/jni_helpers.h&quot;namespace webrtc {namespace jni {static jlong JNI_LibH264Encoder_CreateEncoder(JNIEnv* jni) { return jlongFromPointer(H264Encoder::Create().release());}static jlong JNI_LibH264Decoder_CreateDecoder(JNIEnv* jni) { return jlongFromPointer(H264Decoder::Create().release());}} // namespace jni} // namespace webrtc 在modules/video_coding/codecs/h264/h264.cc中增加上H264Encoder的构造函数： std::unique_ptr&lt;H264Encoder&gt; H264Encoder::Create() {#if defined(WEBRTC_USE_H264) RTC_LOG(LS_INFO) &lt;&lt; &quot;Creating H264EncoderImpl.&quot;; return std::make_unique&lt;H264EncoderImpl&gt;(cricket::VideoCodec(&quot;H264&quot;));#else RTC_NOTREACHED(); return nullptr;#endif} 在modules/video_coding/codecs/h264/include/h264.h头文件中声明函数： class RTC_EXPORT H264Encoder : public VideoEncoder { public:+ static std::unique_ptr&lt;H264Encoder&gt; Create(); static std::unique_ptr&lt;H264Encoder&gt; Create(const cricket::VideoCodec&amp; codec); // If H.264 is supported (any implementation). static bool IsSupported(); ~H264Encoder() override {}}; 在sdk/android/api/org/webrtc/中修改SoftwareVideoDecoderFactory.java、SoftwareVideoEncoderFactory.java增加对H264的支持： SoftwareVideoDecoderFactory.java public VideoDecoder createDecoder(VideoCodecInfo codecType) {+ if (codecType.getName().equalsIgnoreCase(&quot;H264&quot;)) {+ return new LibH264Decoder();+ } ... return null; } static VideoCodecInfo[] supportedCodecs() { List&lt;VideoCodecInfo&gt; codecs = new ArrayList&lt;VideoCodecInfo&gt;(); + codecs.add(new VideoCodecInfo(&quot;H264&quot;, new HashMap&lt;&gt;())); codecs.add(new VideoCodecInfo(&quot;VP8&quot;, new HashMap&lt;&gt;())); ... return codecs.toArray(new VideoCodecInfo[codecs.size()]); } SoftwareVideoEncoderFactory.java public VideoEncoder createEncoder(VideoCodecInfo info) {+ if(info.name.equalsIgnoreCase(&quot;H264&quot;)) {+ return new LibH264Encoder(); } ... return null; } static VideoCodecInfo[] supportedCodecs() { List&lt;VideoCodecInfo&gt; codecs = new ArrayList&lt;VideoCodecInfo&gt;();+ codecs.add(new VideoCodecInfo(&quot;H264&quot;, new HashMap&lt;&gt;())); codecs.add(new VideoCodecInfo(&quot;VP8&quot;, new HashMap&lt;&gt;())); ... return codecs.toArray(new VideoCodecInfo[codecs.size()]); } 增加编译支持在sdk/android/BUILD.gn中关联上述新加逻辑： BUILD.gn+ rtc_android_library(&quot;libH264_java&quot;) {+ visibility = [ &quot;*&quot; ]+ sources = [+ &quot;api/org/webrtc/LibH264Decoder.java&quot;,+ &quot;api/org/webrtc/LibH264Encoder.java&quot;,+ ]+ deps = [+ &quot;:base_java&quot;,+ &quot;:video_api_java&quot;,+ &quot;:video_java&quot;,+ &quot;//rtc_base:base_java&quot;,+ ]+ } rtc_android_library(&quot;libvpx_vp9_java&quot;) { visibility = [ &quot;*&quot; ] sources = [ &quot;api/org/webrtc/LibvpxVp9Decoder.java&quot;, &quot;api/org/webrtc/LibvpxVp9Encoder.java&quot;, ] deps = [ &quot;:base_java&quot;, &quot;:video_api_java&quot;, &quot;:video_java&quot;, &quot;//rtc_base:base_java&quot;, ] }...+ rtc_library(&quot;libH264_jni&quot;) {+ visibility = [ &quot;*&quot; ]+ allow_poison = [ &quot;software_video_codecs&quot; ]+ sources = [ &quot;src/jni/h264_codec.cc&quot; ]+ deps = [+ &quot;:base_jni&quot;,+ &quot;:generated_libH264_jni&quot;,+ &quot;:video_jni&quot;,+ &quot;../../modules/video_coding:webrtc_h264&quot;,+ ]+ } rtc_library(&quot;libvpx_vp9_jni&quot;) { visibility = [ &quot;*&quot; ] allow_poison = [ &quot;software_video_codecs&quot; ] sources = [ &quot;src/jni/vp9_codec.cc&quot; ] deps = [ &quot;:base_jni&quot;, &quot;:generated_libvpx_vp9_jni&quot;, &quot;:video_jni&quot;, &quot;../../modules/video_coding:webrtc_vp9&quot;, ] } rtc_library(&quot;swcodecs_jni&quot;) { visibility = [ &quot;*&quot; ] allow_poison = [ &quot;software_video_codecs&quot; ] deps = [+ &quot;:libH264_jni&quot;, &quot;:libvpx_vp8_jni&quot;, &quot;:libvpx_vp9_jni&quot;, ] }...+ generate_jni(&quot;generated_libH264_jni&quot;) {+ sources = [+ &quot;api/org/webrtc/LibH264Decoder.java&quot;,+ &quot;api/org/webrtc/LibH264Encoder.java&quot;,+ ]+ namespace = &quot;webrtc::jni&quot;+ jni_generator_include = &quot;//sdk/android/src/jni/jni_generator_helper.h&quot;+ } generate_jni(&quot;generated_libvpx_vp9_jni&quot;) { sources = [ &quot;api/org/webrtc/LibvpxVp9Decoder.java&quot;, &quot;api/org/webrtc/LibvpxVp9Encoder.java&quot;, ] namespace = &quot;webrtc::jni&quot; jni_generator_include = &quot;//sdk/android/src/jni/jni_generator_helper.h&quot; } 编译命令./tools_webrtc/android/build_aar.py --build-dir Build --arch arm64-v8a --extra-gn-args 'rtc_use_h264=true ffmpeg_branding=&quot;Chrome&quot;' endWebRTC的音视频数据传输使用的是RTP协议，RTP报文分为报头和载荷两部分，不同类型的载荷有不同的格式，所以就需要单独实现把编码数据打包为RTP报文的逻辑以及从RTP报文解析已编码数据的逻辑。庆幸的是WebRTC以及帮我们处理好了H264的封包解包逻辑，我们只需要添加支持，并把逻辑打包进aar即可，而若要支持H265,则需要自己处理封包解包逻辑，好消息是OWT中包含了H265的支持，我们不用从0️⃣开始。 参考： 安卓 webrtc 开启h264 软编解码","link":"/2022/09/28/Android%E5%B9%B3%E5%8F%B0WebRTC%E5%BC%80%E5%90%AFH264%E8%BD%AF%E7%BC%96%E8%A7%A3%E7%A0%81/"},{"title":"Android Unit Case","text":"Android单元测试编写原则及相关三方库详解 概念单元测试只是测试一个方法单元，它不是测试一个整个流程。举一个🌰 一个Login页面，上面有两个EditText和一个Button。两个EditText分别用于输入用户名和密码。点击Button以后，有一个UserManager会去执行performlogin操作，然后将结果返回，更新页面。那么我们给这个东西做单元测试的时候，不是测这个login流程。这种整个流程的测试：给两个输入框设置正确的用户名和密码，点击login button，最后页面得到更新，叫做 集成测试，而不是单元测试。当然，集成测试是有必要的，但这不是程序员应该花精力的地方。 Test PyramidTest Pyramid理论基本大意是，单元测试是基础，是我们应该花绝大多数时间去写的部分，而集成测试等应该是冰山上面能看见的那一小部分。 为什么是这样呢？因为集成测试设置起来很麻烦，运行起来很慢，发现的bug少，在保证代码质量、改善代码设计方面更起不到任何作用，因此它的重要程度并不是那么高，也无法将它纳入我们正常的工作流程中。而单元测试则刚好相反，它运行速度超快，能发现的bug更多，在开发时能引导更好的代码设计，在重构时能保证重构的正确性，因此它能保证我们的代码在一个比较高的质量水平上。同时因为运行速度快，我们很容易把它纳入到我们正常的开发流程中。至于为什么集成测试发现的bug少，而单元测试发现的bug多，这里也稍作解释，因为集成测试不能测试到其中每个环节的每个方面，某一个集成测试运行正确了，不代表另一个集成测试也能运行正确。而单元测试会比较完整的测试每个单元的各种不同的状况、临界条件等等。一般来说，如果每一个环节是对的，那么在很大的概率上，整个流程就是对的。虽然不能保证整个流程100%一定是对的。所以，集成测试需要有，但应该是少量，单元测试是我们应该花重点去做的事情。 为什么如何你在编写单元测试的时候发现当前类不好测，说明该类设计有问题 提升软件质量 方便重构 节约时间 提升代码设计 多种工具JUnitJUnit4是Java界用的最广泛的一个基础框架。 一个测试方法包括三个部分： setup 执行操作 验证结果 举一个🌰: public class LoginTest { Calculator mCalculator; @Before public void setup(){ mCalculator = new Calculator(); } @Test public void addTest() throws Exception { int sum = mCalculator.add(1,2); assertEquals(3,sum); } @Test @Ignore(&quot;not implemented yet&quot;) public void multiplyTest() throws Exception { int product = mCalculator.multiply(2,4); assertEquals(8,product); }} @Before : 每个测试函数在调用之前都会先调用@Before注解的函数，比如 addTest 运行前会执行 setup , multiplyTest 运行前会执行 setup,类似逻辑的还有@After、@BeforeClass、@AfterClass。即在跑一个测试类的所有测试方法之前，会执行一个被@BeforeClass修饰的函数。 @Ignore : 如果需要忽略某些方法可以使用该注解，例如正式代码还没有实现 @Test(expected = IllegalArgumentException.class) : 表示验证这个测试方法将抛出异常，如果没有抛出的话，则测试失败。 public class Calculator { public double divide(double divident,double dividor) { if (dividor == 0) throw new IllegalArgumentException(&quot;Dividor can't be 0&quot;); return divident / dividor; }}@Test(expected = IllegalArgumentException.class) public void test () { mCalculator.dividor(4,0);} Mock/MockitoMock是创建一个类的虚假对象，在测试环境中，用来替换真实对象，以达到两个目的： 验证这个对象的某些方法的调用情况，调用了多少次，参数是什么 指定这个对象的某些方法的行为，返回特定的值，或者是执行特定的动作 Mockito是最Java界使用最广泛的Mock框架 1.验证方法调用 @Testpublic void testLogin() throws Exception { UserManager mockUserManager = Mockito.mock(UserManager.class); LoginPresenter loginPresenter = new LoginPresenter(); loginPresenter.setUserManager(mockUserManager); //&lt;== loginPresenter.login(&quot;xiaochuang&quot;, &quot;xiaochuang password&quot;); Mockito.verify(mockUserManager).performLogin(&quot;xiaochuang&quot;, &quot;xiaochuang password&quot;);} 如果需要验证mockUserManager的performLogin()得到了调用，同时参数是”username”,”password” Mockito.verify(mockUserManager).performLogin(&quot;username&quot;,&quot;password&quot;); 当然也可以验证该函数调用次数 Mockito.verify(mockUserManager, Mockito.times(3)).performLogin(...); //验证mockUserManager的performLogin得到了三次调用。Mockito.verify(mockUserManager, Mockito.atLeast(3)).performLogin(...); //验证mockUserManager的performLogin最少得到了三次调用。Mockito.verify(mockUserManager).performLogin(Mockito.anyString(),Mockito.anyString()); //并不关心参数，任意参数皆可 2.指定mock对象的某些方法的行为举一个🌰： public void login(String username, String password) { if (username == null || username.length() == 0) return; //假设我们对密码强度有一定要求，使用一个专门的validator来验证密码的有效性 if (mPasswordValidator.verifyPassword(password)) return; //&lt;== mUserManager.performLogin(null, password);} 这里我们需要PasswordValidator来验证密码的有效性，但是这个类的verifyPassword()方法需要联网，其实我们只需要给一些阈值判断即可，因为我们要测的是login()，跟PasswordValidator内部逻辑没有关系，这才是单元测试真正该有的颗粒度，比如可以这么写： //先创建一个mock对象PasswordValidator mockValidator = Mockito.mock(PasswordValidator.class);//当调用mockValidator的verifyPassword方法，同时传入&quot;xiaochuang_is_handsome&quot;时，返回trueMockito.when(mockValidator.verifyPassword(&quot;xiaochuang_is_handsome&quot;)).thenReturn(true);//当调用mockValidator的verifyPassword方法，同时传入&quot;xiaochuang_is_not_handsome&quot;时，返回falseMockito.when(validator.verifyPassword(&quot;xiaochuang_is_not_handsome&quot;)).thenReturn(false); 又比如有如下逻辑: public void loginCallbackVersion(String username, String password) { if (username == null || username.length() == 0) return; //假设我们对密码强度有一定要求，使用一个专门的validator来验证密码的有效性 if (mPasswordValidator.verifyPassword(password)) return; //login的结果将通过callback传递回来。 mUserManager.performLogin(username, password, new NetworkCallback() { //&lt;== @Override public void onSuccess(Object data) { //update view with data } @Override public void onFailure(int code, String msg) { //show error msg } });} 想进一步测试传给mUserManager.performLogin的NetworkCallback里面的代码，验证view得到了更新，测试环境里，我们并不想依赖mUserManager.performLogin的真实逻辑，而是让mUserManager直接调用传入的NetworkCallback的onSuccess或onFailure方法，这种指定mock对象执行特定的动作的写法如下：Mockito.doAnswer(desiredAnswer).when(mockObject).targetMethod(args);举一个🌰： Mockito.doAnswer(new Answer() { @Override public Object answer(InvocationOnMock invocation) throws Throwable { //这里可以获得传给performLogin的参数 Object[] arguments = invocation.getArguments(); //callback是第三个参数 NetworkCallback callback = (NetworkCallback) arguments[2]; callback.onFailure(500, &quot;Server error&quot;); return 500; }}).when(mockUserManager).performLogin(anyString(), anyString(), any(NetworkCallback.class));","link":"/2019/08/06/Android-Unit-Case/"},{"title":"Dive Into Flutter","text":"首先从过去的CRT显示器原理说起。CRT的电子枪按照上面方式，从上到下一行行扫描，扫描完成后显示器就呈现一帧画面，随后电子枪回到初始位置继续下一次扫描。为了把显示器的显示过程和系统的视频控制器进行同步，显示器（或其他硬件）会用硬件时钟产生一系列的定时信号。当电子枪换到新的一行，准备进行扫描时，显示器会发出一个水平同步信号（horizonal synchronization），简称HSync；而当一帧画面绘制完成后，电子枪回复到原位，准备画下一帧前，显示器会发出一个垂直同步信号（vertial synchronization），简称VSync。显示器通常以固定频率进行刷新，这个刷新率就是VSync信号产生的频率。尽管现在的设备大都是液晶显示屏了，但原理仍然没有变。 原理屏幕显示图片的原理 首先从过去的CRT显示器原理说起。CRT的电子枪按照上面方式，从上到下一行行扫描，扫描完成后显示器就呈现一帧画面，随后电子枪回到初始位置继续下一次扫描。为了把显示器的显示过程和系统的视频控制器进行同步，显示器（或其他硬件）会用硬件时钟产生一系列的定时信号。当电子枪换到新的一行，准备进行扫描时，显示器会发出一个水平同步信号（horizonal synchronization），简称HSync；而当一帧画面绘制完成后，电子枪回复到原位，准备画下一帧前，显示器会发出一个垂直同步信号（vertial synchronization），简称VSync。显示器通常以固定频率进行刷新，这个刷新率就是VSync信号产生的频率。尽管现在的设备大都是液晶显示屏了，但原理仍然没有变。 通常来说，计算机系统中CPU,GPU,显示器是以上面这种方式协同工作的。CPU计算好显示内容提交到GPU,GPU渲染完成后将渲染结果放入帧缓冲区，随后视频控制器会按照VSync信号逐行读取帧缓冲区的数据，经过可能的数模转换传递给显示器显示。CPU和GPU的任务是各有偏重的，CPU主要用于基本数学和逻辑计算，而GPU主要执行和图形处理相关的复杂的数学，如矩阵变化和几何计算，GPU的主要作用就是确定最终输送给显示器的各个像素点的色值。 在最简单的情况下，帧缓存区只有一个，这时帧缓存区的读取和刷新都会有比较大的效率问题。为了解决效率问题，显示系统通常会引入两个缓冲区，让视频控制器读取，当下一帧渲染好后，GPU会直接把视频控制器的指针指向第二个缓冲器。如此一来效率会有很大的提高。 双缓冲虽然能解决效率问题，但会引入一个新的问题。当视频控制器还未读取完成时，即屏幕内容刚显示一半时，GPU将新的一帧内容提交到帧缓存区并把两个缓冲区进行交换后，视频控制器就会把新的一帧数据的下半段显示到屏幕上，造成画面撕裂现象，如下图： ![dive_into_flutter_vsync_off](/images/dive_into_flutter_vsync_off.jpg 为了解决这个问题，GPU通常有一个机制叫做垂直同步（简写也是V-Sync），当开启垂直同步后，GPU会等待显示器的VSync信号发出后，才进行新的一帧渲染和缓存区更新。这样能解决画面撕裂现象，也增加了画面流畅度，但需要消费更多的计算资源，也会带来部分延迟。 那么目前主流的移动设备是什么情况？从网上查到的资料可以知道，iOS设备会始终使用双缓存，并开启垂直同步。而安卓设备到4.1版本，Google才开始引入这种机制，目前安卓系统是三缓存+垂直同步。 当VSync信号到来后，系统图形服务会通过CADisplayLink(iOS),ViewTreeObserver(Android)等机制通知App，App主线程开始在CPU中计算显示内容，比如视图的创建、布局计算、图片解码、文本绘制等。随后CPU会将计算好的内容提交到GPU去，由GPU进行变化、合成、渲染。随后GPU会把渲染结果提交到帧缓冲区去，等待下一次VSync信号到来时显示到屏幕上。由于垂直同步的机制，如果在一个VSync时间内，CPU或者GPU没有完成内容提交，则那一帧就会被丢弃，等待下一次机会再显示，而这时显示屏会保留之前的内容不变。这就是界面卡顿的原因。 卡顿产生的原因和解决方案参考:iOS保持界面流畅的技巧","link":"/2019/08/09/Dive-Into-Flutter/"},{"title":"Espresso","text":"Android单元测试官方support包Espresso使用 不准备做系统化的Api梳理官方文档里面已经很详细说明了Espresso，这里会累计一些坑或者细节。 intended/intendingintended Asserts that the given matcher matches one and only one intent sent by the application under test. This is an equivalent of verify(mock, times(1)) in Mockito. Verification does not have to occur in the same order as the intents were sent. Intents are recorded from the time that Intents.init is called. 断言只会给matcher匹配一个且只有一个由application发来的intent。类似于Mockito中的verify(mock,times(1))函数,验证过程不需要按照发送intent的相同顺序进行，从Intent.init开始记录。 intending Enables stubbing intent responses. This method is similar to Mockito.when and is particularly useful when the activity launching the intent expects data to be returned (and especially in the case when the destination activity is external). In this case, the test author can call intending(matcher).thenRespond(myResponse) and validate that the launching activity handles the result correctly. Note: the destination activity will not be launched. 打开stubbing Intent响应,类似于Mockito。当拉起Activity，并预计有Intent返回时（特别是当目标Acticity是外部Acticity时），特别有用。在这种情况下，测试者可以调用intending(matcher).thenRespond(myResponse)并验证拉起Activity是否正确处理结果。Note:目标Activity将不会启动。","link":"/2019/08/07/Espresso/"},{"title":"MAC android studio3.1.2无比卡，解决方案","text":"升级Android Studio3.1.2以后，真是java文件写一个字卡三个字，xml文件根本不敢打开。在V2ex找到的解决办法是强制使用独显。会有一定的效果，不过治标不治标，长久的跑还是会卡，并且电量消耗奇快。分享一个下command line 升级Android Studio3.1.2以后，真是java文件写一个字卡三个字，xml文件根本不敢打开。在V2ex找到的解决办法是强制使用独显。会有一定的效果，不过治标不治标，长久的跑还是会卡，并且电量消耗奇快。分享一个下command line sudo pmset -a GPUSwitch 1 0 - 强制使用核显 1 - 强制使用独显（相当于在偏好设置-节能 里去掉自动切换显卡这个选项） 2 - 自动切换显卡 其实时间是最好的老师，公司赶项目，只能忍，今天忙完找了一下，如下解决方案正合适 Help -&gt; Edit Custom VM Options… 修改如下部分 -Xms956m -Xmx3280m -XX:ReservedCodeCacheSize=240m -XX:+UseConcMarkSweepGC -XX:SoftRefLRUPolicyMSPerMB=50 -Dsun.io.useCanonCaches=false -Djava.net.preferIPv4Stack=true -Djna.nosys=true -Djna.boot.library.path= -da Settings -&gt; Appearance -&gt; Show memory indicator然后你就可以在idea右下角看到内存使用情况了 参考:官方提供的开发工具调优策略","link":"/2018/06/01/MAC-android-studio3-1-2%E6%97%A0%E6%AF%94%E5%8D%A1%EF%BC%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"title":"MediaPipe在Android平台上构建AAR","text":"需要在实时音视频中实现虚拟背景，或者在人像上进行贴纸。希望在Google开源的MediaPipe中找到解决方案，本文章介绍MediaPipe在MacOS中的环境搭建，以及编译AAR。后续会补充机械学习相关在Android平台上的使用 环境搭建 获取源码https://github.com/google/mediapipe.git 安装bazel brew install bazel 科学上网 编译脚本 创建相关文件夹、BUILD文件 cd mediapipe/examples/android/src/java/com/google/mediapipe/apps/mkdir buid_aar &amp;&amp; cd buid_aartouch BUILD 编写编译脚本，指定需要编译的模型和计算单元，输出的文件名 load(&quot;//mediapipe/java/com/google/mediapipe:mediapipe_aar.bzl&quot;, &quot;mediapipe_aar&quot;)mediapipe_aar( name = &quot;mediapipe_portrait_segmentation&quot;, calculators = [&quot;//mediapipe/graphs/portrait_segmentation:mobile_calculators&quot;],) 指定需要的cpu架构，执行编译 bazel build --cxxopt='--std=c++14' -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a examples/android/src/java/com/google/mediapipe/apps/buid_aar:mediapipe_portrait_segmentation --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures 生成的aar路径： bazel-bin/mediapipe/examples/android/src/java/com/google/mediapipe/apps/buid_aar/mediapipe_portrait_segmentation.aar 生成Mediapipe二进制图， bazel build -c opt mediapipe/graphs/portrait_segmentation:portrait_segmentation_mobile_gpu_binary_graph","link":"/2021/04/28/MediaPipe%E5%9C%A8Android%E5%B9%B3%E5%8F%B0%E4%B8%8A%E6%9E%84%E5%BB%BAAAR/"},{"title":"Android平台WebRTC开启H265编解码","text":"不像H264，WebRTC已经在内部处理好了相关逻辑，我们只需要稍作修改即可实现H264编解码（硬编只需指定sdp，软编只需打开开关@see 《Android平台WebRTC开启H264软编解码》）。如要实现H265，不仅要增加接口，还需要添加封解RTP包的逻辑，本文基于M86。 开启H265编解码打开支持 增加sdp支持： sdk/android/api/org/webrtc/HardwareVideoEncoderFactory.java @Overridepublic VideoCodecInfo[] getSupportedCodecs() {... for (VideoCodecMimeType type : new VideoCodecMimeType[] { VideoCodecMimeType.VP8, VideoCodecMimeType.VP9, VideoCodecMimeType.H264, VideoCodecMimeType.H265})... return supportedCodecInfos.toArray(new VideoCodecInfo[supportedCodecInfos.size()]);} 增加sps/pps/vps支持： sdk/android/src/java/org/webrtc/HardwareVideoEncoder.javaprotected void deliverEncodedImage() {... final ByteBuffer frameBuffer; if (isKeyFrame &amp;&amp; (codecType == VideoCodecMimeType.H264 || codecType == VideoCodecMimeType.H265)) } 封包支持 增加解包器初始化入口 modules/rtp_rtcp/source/create_video_rtp_depacketizer.cc... #ifndef DISABLE_H265 #include &quot;modules/rtp_rtcp/source/video_rtp_depacketizer_h265.h&quot; #endif... #ifndef DISABLE_H265 case kVideoCodecH265: return std::make_unique&lt;VideoRtpDepacketizerH265&gt;(); #endif... foramt基类增加H265支持 modules/rtp_rtcp/source/rtp_format.cc... #ifndef DISABLE_H265 #include &quot;modules/rtp_rtcp/source/rtp_format_h265.h&quot; #endif... #ifndef DISABLE_H265 #include &quot;modules/video_coding/codecs/h265/include/h265_globals.h&quot; #endif...std::unique_ptr&lt;RtpPacketizer&gt; RtpPacketizer::Create( absl::optional&lt;VideoCodecType&gt; type, rtc::ArrayView&lt;const uint8_t&gt; payload, PayloadSizeLimits limits, // Codec-specific details. const RTPVideoHeader&amp; rtp_video_header) { if (!type) { // Use raw packetizer. return std::make_unique&lt;RtpPacketizerGeneric&gt;(payload, limits); } switch (*type) { case kVideoCodecH264: { const auto&amp; h264 = absl::get&lt;RTPVideoHeaderH264&gt;(rtp_video_header.video_type_header); return std::make_unique&lt;RtpPacketizerH264&gt;(payload, limits, h264.packetization_mode); } #ifndef DISABLE_H265 case kVideoCodecH265: { const auto&amp; h265 = absl::get&lt;RTPVideoHeaderH265&gt;(rtp_video_header.video_type_header); return absl::make_unique&lt;RtpPacketizerH265&gt;( payload, limits, h265.packetization_mode); }#endif... }} 新增H265 format实现类rtp_format_h265： modules/rtp_rtcp/source/rtp_format_h265.cc >folded#include &lt;string.h&gt;#include &quot;absl/types/optional.h&quot;#include &quot;absl/types/variant.h&quot;#include &quot;common_video/h264/h264_common.h&quot;#include &quot;common_video/h265/h265_common.h&quot;#include &quot;common_video/h265/h265_pps_parser.h&quot;#include &quot;common_video/h265/h265_sps_parser.h&quot;#include &quot;common_video/h265/h265_vps_parser.h&quot;#include &quot;modules/include/module_common_types.h&quot;#include &quot;modules/rtp_rtcp/source/byte_io.h&quot;#include &quot;modules/rtp_rtcp/source/rtp_format_h265.h&quot;#include &quot;modules/rtp_rtcp/source/rtp_packet_to_send.h&quot;#include &quot;rtc_base/logging.h&quot;using namespace rtc;namespace webrtc {namespace {enum NaluType { kTrailN = 0, kTrailR = 1, kTsaN = 2, kTsaR = 3, kStsaN = 4, kStsaR = 5, kRadlN = 6, kRadlR = 7, kBlaWLp = 16, kBlaWRadl = 17, kBlaNLp = 18, kIdrWRadl = 19, kIdrNLp = 20, kCra = 21, kVps = 32, kHevcSps = 33, kHevcPps = 34, kHevcAud = 35, kPrefixSei = 39, kSuffixSei = 40, kHevcAp = 48, kHevcFu = 49};/* 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | PayloadHdr (Type=49) | FU header | DONL (cond) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-|*/// Unlike H.264, HEVC NAL header is 2-bytes.static const size_t kHevcNalHeaderSize = 2;// H.265's FU is constructed of 2-byte payload header, and 1-byte FU headerstatic const size_t kHevcFuHeaderSize = 1;static const size_t kHevcLengthFieldSize = 2;enum HevcNalHdrMasks { kHevcFBit = 0x80, kHevcTypeMask = 0x7E, kHevcLayerIDHMask = 0x1, kHevcLayerIDLMask = 0xF8, kHevcTIDMask = 0x7, kHevcTypeMaskN = 0x81, kHevcTypeMaskInFuHeader = 0x3F};// Bit masks for FU headers.enum HevcFuDefs { kHevcSBit = 0x80, kHevcEBit = 0x40, kHevcFuTypeBit = 0x3F };} // namespaceRtpPacketizerH265::RtpPacketizerH265( rtc::ArrayView&lt;const uint8_t&gt; payload, PayloadSizeLimits limits, H265PacketizationMode packetization_mode) : limits_(limits), num_packets_left_(0) { // Guard against uninitialized memory in packetization_mode. RTC_CHECK(packetization_mode == H265PacketizationMode::NonInterleaved || packetization_mode == H265PacketizationMode::SingleNalUnit); for (const auto&amp; nalu : H264::FindNaluIndices(payload.data(), payload.size())) { input_fragments_.push_back( payload.subview(nalu.payload_start_offset, nalu.payload_size)); } if (!GeneratePackets(packetization_mode)) { // If failed to generate all the packets, discard already generated // packets in case the caller would ignore return value and still try to // call NextPacket(). num_packets_left_ = 0; while (!packets_.empty()) { packets_.pop(); } }}RtpPacketizerH265::~RtpPacketizerH265() {}size_t RtpPacketizerH265::NumPackets() const { return num_packets_left_;}bool RtpPacketizerH265::GeneratePackets( H265PacketizationMode packetization_mode) { // For HEVC we follow non-interleaved mode for the packetization, // and don't support single-nalu mode at present. for (size_t i = 0; i &lt; input_fragments_.size();) { int fragment_len = input_fragments_[i].size(); int single_packet_capacity = limits_.max_payload_len; if (input_fragments_.size() == 1) single_packet_capacity -= limits_.single_packet_reduction_len; else if (i == 0) single_packet_capacity -= limits_.first_packet_reduction_len; else if (i + 1 == input_fragments_.size()) { // Pretend that last fragment is larger instead of making last packet // smaller. single_packet_capacity -= limits_.last_packet_reduction_len; } if (fragment_len &gt; single_packet_capacity) { PacketizeFu(i); ++i; } else { PacketizeSingleNalu(i); ++i; } } return true;}bool RtpPacketizerH265::PacketizeFu(size_t fragment_index) { // Fragment payload into packets (FU). // Strip out the original header and leave room for the FU header. rtc::ArrayView&lt;const uint8_t&gt; fragment = input_fragments_[fragment_index]; PayloadSizeLimits limits = limits_; limits.max_payload_len -= kHevcFuHeaderSize + kHevcNalHeaderSize; // Update single/first/last packet reductions unless it is single/first/last // fragment. if (input_fragments_.size() != 1) { // if this fragment is put into a single packet, it might still be the // first or the last packet in the whole sequence of packets. if (fragment_index == input_fragments_.size() - 1) { limits.single_packet_reduction_len = limits_.last_packet_reduction_len; } else if (fragment_index == 0) { limits.single_packet_reduction_len = limits_.first_packet_reduction_len; } else { limits.single_packet_reduction_len = 0; } } if (fragment_index != 0) limits.first_packet_reduction_len = 0; if (fragment_index != input_fragments_.size() - 1) limits.last_packet_reduction_len = 0; // Strip out the original header. size_t payload_left = fragment.size() - kHevcNalHeaderSize; int offset = kHevcNalHeaderSize; std::vector&lt;int&gt; payload_sizes = SplitAboutEqually(payload_left, limits); if (payload_sizes.empty()) return false; for (size_t i = 0; i &lt; payload_sizes.size(); ++i) { int packet_length = payload_sizes[i]; RTC_CHECK_GT(packet_length, 0); uint16_t header = (fragment[0] &lt;&lt; 8) | fragment[1]; packets_.push(PacketUnit(fragment.subview(offset, packet_length), /*first_fragment=*/i == 0, /*last_fragment=*/i == payload_sizes.size() - 1, false, header)); offset += packet_length; payload_left -= packet_length; } num_packets_left_ += payload_sizes.size(); RTC_CHECK_EQ(0, payload_left); return true;}bool RtpPacketizerH265::PacketizeSingleNalu(size_t fragment_index) { // Add a single NALU to the queue, no aggregation. size_t payload_size_left = limits_.max_payload_len; if (input_fragments_.size() == 1) payload_size_left -= limits_.single_packet_reduction_len; else if (fragment_index == 0) payload_size_left -= limits_.first_packet_reduction_len; else if (fragment_index + 1 == input_fragments_.size()) payload_size_left -= limits_.last_packet_reduction_len; rtc::ArrayView&lt;const uint8_t&gt; fragment = input_fragments_[fragment_index]; if (payload_size_left &lt; fragment.size()) { RTC_LOG(LS_ERROR) &lt;&lt; &quot;Failed to fit a fragment to packet in SingleNalu &quot; &quot;packetization mode. Payload size left &quot; &lt;&lt; payload_size_left &lt;&lt; &quot;, fragment length &quot; &lt;&lt; fragment.size() &lt;&lt; &quot;, packet capacity &quot; &lt;&lt; limits_.max_payload_len; return false; } RTC_CHECK_GT(fragment.size(), 0u); packets_.push(PacketUnit(fragment, true /* first */, true /* last */, false /* aggregated */, fragment[0])); ++num_packets_left_; return true;}int RtpPacketizerH265::PacketizeAp(size_t fragment_index) { // Aggregate fragments into one packet (STAP-A). size_t payload_size_left = limits_.max_payload_len; if (input_fragments_.size() == 1) payload_size_left -= limits_.single_packet_reduction_len; else if (fragment_index == 0) payload_size_left -= limits_.first_packet_reduction_len; int aggregated_fragments = 0; size_t fragment_headers_length = 0; rtc::ArrayView&lt;const uint8_t&gt; fragment = input_fragments_[fragment_index]; RTC_CHECK_GE(payload_size_left, fragment.size()); ++num_packets_left_; auto payload_size_needed = [&amp;] { size_t fragment_size = fragment.size() + fragment_headers_length; if (input_fragments_.size() == 1) { // Single fragment, single packet, payload_size_left already adjusted // with limits_.single_packet_reduction_len. return fragment_size; } if (fragment_index == input_fragments_.size() - 1) { // Last fragment, so StrapA might be the last packet. return fragment_size + limits_.last_packet_reduction_len; } return fragment_size; }; while (payload_size_left &gt;= payload_size_needed()) { RTC_CHECK_GT(fragment.size(), 0); packets_.push(PacketUnit(fragment, aggregated_fragments == 0, false, true, fragment[0])); payload_size_left -= fragment.size(); payload_size_left -= fragment_headers_length; fragment_headers_length = kHevcLengthFieldSize; // If we are going to try to aggregate more fragments into this packet // we need to add the STAP-A NALU header and a length field for the first // NALU of this packet. if (aggregated_fragments == 0) fragment_headers_length += kHevcNalHeaderSize + kHevcLengthFieldSize; ++aggregated_fragments; // Next fragment. ++fragment_index; if (fragment_index == input_fragments_.size()) break; fragment = input_fragments_[fragment_index]; } RTC_CHECK_GT(aggregated_fragments, 0); packets_.back().last_fragment = true; return fragment_index;}bool RtpPacketizerH265::NextPacket(RtpPacketToSend* rtp_packet) { RTC_DCHECK(rtp_packet); if (packets_.empty()) { return false; } PacketUnit packet = packets_.front(); if (packet.first_fragment &amp;&amp; packet.last_fragment) { // Single NAL unit packet. size_t bytes_to_send = packet.source_fragment.size(); uint8_t* buffer = rtp_packet-&gt;AllocatePayload(bytes_to_send); memcpy(buffer, packet.source_fragment.data(), bytes_to_send); packets_.pop(); input_fragments_.pop_front(); } else if (packet.aggregated) { bool is_last_packet = num_packets_left_ == 1; NextAggregatePacket(rtp_packet, is_last_packet); } else { NextFragmentPacket(rtp_packet); } rtp_packet-&gt;SetMarker(packets_.empty()); --num_packets_left_; return true;}void RtpPacketizerH265::NextAggregatePacket(RtpPacketToSend* rtp_packet, bool last) { size_t payload_capacity = rtp_packet-&gt;FreeCapacity(); RTC_CHECK_GE(payload_capacity, kHevcNalHeaderSize); uint8_t* buffer = rtp_packet-&gt;AllocatePayload(payload_capacity); RTC_CHECK(buffer); PacketUnit* packet = &amp;packets_.front(); RTC_CHECK(packet-&gt;first_fragment); uint8_t payload_hdr_h = packet-&gt;header &gt;&gt; 8; uint8_t payload_hdr_l = packet-&gt;header &amp; 0xFF; uint8_t layer_id_h = payload_hdr_h &amp; kHevcLayerIDHMask; payload_hdr_h = (payload_hdr_h &amp; kHevcTypeMaskN) | (kHevcAp &lt;&lt; 1) | layer_id_h; buffer[0] = payload_hdr_h; buffer[1] = payload_hdr_l; int index = kHevcNalHeaderSize; bool is_last_fragment = packet-&gt;last_fragment; while (packet-&gt;aggregated) { // Add NAL unit length field. rtc::ArrayView&lt;const uint8_t&gt; fragment = packet-&gt;source_fragment; ByteWriter&lt;uint16_t&gt;::WriteBigEndian(&amp;buffer[index], fragment.size()); index += kHevcLengthFieldSize; // Add NAL unit. memcpy(&amp;buffer[index], fragment.data(), fragment.size()); index += fragment.size(); packets_.pop(); input_fragments_.pop_front(); if (is_last_fragment) break; packet = &amp;packets_.front(); is_last_fragment = packet-&gt;last_fragment; } RTC_CHECK(is_last_fragment); rtp_packet-&gt;SetPayloadSize(index);}void RtpPacketizerH265::NextFragmentPacket(RtpPacketToSend* rtp_packet) { PacketUnit* packet = &amp;packets_.front(); // NAL unit fragmented over multiple packets (FU). // We do not send original NALU header, so it will be replaced by the // PayloadHdr of the first packet. uint8_t payload_hdr_h = packet-&gt;header &gt;&gt; 8; // 1-bit F, 6-bit type, 1-bit layerID highest-bit uint8_t payload_hdr_l = packet-&gt;header &amp; 0xFF; uint8_t layer_id_h = payload_hdr_h &amp; kHevcLayerIDHMask; uint8_t fu_header = 0; // S | E |6 bit type. fu_header |= (packet-&gt;first_fragment ? kHevcSBit : 0); fu_header |= (packet-&gt;last_fragment ? kHevcEBit : 0); uint8_t type = (payload_hdr_h &amp; kHevcTypeMask) &gt;&gt; 1; fu_header |= type; // Now update payload_hdr_h with FU type. payload_hdr_h = (payload_hdr_h &amp; kHevcTypeMaskN) | (kHevcFu &lt;&lt; 1) | layer_id_h; rtc::ArrayView&lt;const uint8_t&gt; fragment = packet-&gt;source_fragment; uint8_t* buffer = rtp_packet-&gt;AllocatePayload( kHevcFuHeaderSize + kHevcNalHeaderSize + fragment.size()); RTC_CHECK(buffer); buffer[0] = payload_hdr_h; buffer[1] = payload_hdr_l; buffer[2] = fu_header; if (packet-&gt;last_fragment) { memcpy(buffer + kHevcFuHeaderSize + kHevcNalHeaderSize, fragment.data(), fragment.size()); } else { memcpy(buffer + kHevcFuHeaderSize + kHevcNalHeaderSize, fragment.data(), fragment.size()); } packets_.pop();}} // namespace webrtc modules/rtp_rtcp/source/rtp_format_h265.h >folded#ifndef WEBRTC_MODULES_RTP_RTCP_SOURCE_RTP_FORMAT_H265_H_#define WEBRTC_MODULES_RTP_RTCP_SOURCE_RTP_FORMAT_H265_H_#include &lt;memory&gt;#include &lt;queue&gt;#include &lt;string&gt;#include &quot;api/array_view.h&quot;#include &quot;modules/include/module_common_types.h&quot;#include &quot;modules/rtp_rtcp/source/rtp_format.h&quot;#include &quot;modules/rtp_rtcp/source/rtp_packet_to_send.h&quot;#include &quot;modules/rtp_rtcp/source/rtp_format.h&quot;#include &quot;modules/video_coding/codecs/h265/include/h265_globals.h&quot;#include &quot;rtc_base/buffer.h&quot;#include &quot;rtc_base/constructor_magic.h&quot;namespace webrtc {class RtpPacketizerH265 : public RtpPacketizer { public: // Initialize with payload from encoder. // The payload_data must be exactly one encoded H.265 frame. RtpPacketizerH265(rtc::ArrayView&lt;const uint8_t&gt; payload, PayloadSizeLimits limits, H265PacketizationMode packetization_mode); ~RtpPacketizerH265() override; size_t NumPackets() const override; // Get the next payload with H.265 payload header. // buffer is a pointer to where the output will be written. // bytes_to_send is an output variable that will contain number of bytes // written to buffer. The parameter last_packet is true for the last packet of // the frame, false otherwise (i.e., call the function again to get the // next packet). // Returns true on success or false if there was no payload to packetize. bool NextPacket(RtpPacketToSend* rtp_packet) override; private: struct Packet { Packet(size_t offset, size_t size, bool first_fragment, bool last_fragment, bool aggregated, uint16_t header) : offset(offset), size(size), first_fragment(first_fragment), last_fragment(last_fragment), aggregated(aggregated), header(header) {} size_t offset; size_t size; bool first_fragment; bool last_fragment; bool aggregated; uint16_t header; // Different from H264 }; struct PacketUnit { PacketUnit(rtc::ArrayView&lt;const uint8_t&gt; source_fragment, bool first_fragment, bool last_fragment, bool aggregated, uint16_t header) : source_fragment(source_fragment), first_fragment(first_fragment), last_fragment(last_fragment), aggregated(aggregated), header(header) {} rtc::ArrayView&lt;const uint8_t&gt; source_fragment; bool first_fragment; bool last_fragment; bool aggregated; uint16_t header; }; typedef std::queue&lt;Packet&gt; PacketQueue; std::deque&lt;rtc::ArrayView&lt;const uint8_t&gt;&gt; input_fragments_; std::queue&lt;PacketUnit&gt; packets_; bool GeneratePackets(H265PacketizationMode packetization_mode); bool PacketizeFu(size_t fragment_index); int PacketizeAp(size_t fragment_index); bool PacketizeSingleNalu(size_t fragment_index); void NextAggregatePacket(RtpPacketToSend* rtp_packet, bool last); void NextFragmentPacket(RtpPacketToSend* rtp_packet); const PayloadSizeLimits limits_; size_t num_packets_left_; RTC_DISALLOW_COPY_AND_ASSIGN(RtpPacketizerH265);};} // namespace webrtc#endif // WEBRTC_MODULES_RTP_RTCP_SOURCE_RTP_FORMAT_H265_H_ 视频渲染支持,vpx通过picture_id,temporal_id,tl0_pic_id标识Nalu间的关系及是否可连续解码，H26x通过seqnum，是否有sps，pps来判断帧间的解码连续性。故，我们把H265的temporalId置为kNoTemporalIdx即可： modules/rtp_rtcp/source/rtp_sender_video.ccuint8_t RTPSenderVideo::GetTemporalId(const RTPVideoHeader&amp; header) { struct TemporalIdGetter { uint8_t operator()(const RTPVideoHeaderVP8&amp; vp8) { return vp8.temporalIdx; } uint8_t operator()(const RTPVideoHeaderVP9&amp; vp9) { return vp9.temporal_idx; } uint8_t operator()(const RTPVideoHeaderH264&amp;) { return kNoTemporalIdx; } #ifndef DISABLE_H265 uint8_t operator()(const RTPVideoHeaderH265&amp;) { return kNoTemporalIdx; } #endif... return absl::visit(TemporalIdGetter(), header.video_type_header);} RTP头增加H265支持构造函数,注意#ifndef与#ifdef的区别： modules/rtp_rtcp/source/rtp_video_header.h#include &quot;modules/video_coding/codecs/h264/include/h264_globals.h&quot; #ifndef DISABLE_H265 #include &quot;modules/video_coding/codecs/h265/include/h265_globals.h&quot; #endif#include &quot;modules/video_coding/codecs/vp8/include/vp8_globals.h&quot;... #ifdef DISABLE_H265using RTPVideoTypeHeader = absl::variant&lt;absl::monostate, RTPVideoHeaderVP8, RTPVideoHeaderVP9, RTPVideoHeaderH264, RTPVideoHeaderLegacyGeneric&gt;; #else using RTPVideoTypeHeader = absl::variant&lt;absl::monostate, RTPVideoHeaderVP8, RTPVideoHeaderVP9, RTPVideoHeaderH264, RTPVideoHeaderH265, RTPVideoHeaderLegacyGeneric&gt;; #endif 新增H265解包逻辑video_rtp_depacketizer_h265： modules/rtp_rtcp/source/video_rtp_depacketizer_h265.h >folded#ifndef MODULES_RTP_RTCP_SOURCE_VIDEO_RTP_DEPACKETIZER_H265_H_#define MODULES_RTP_RTCP_SOURCE_VIDEO_RTP_DEPACKETIZER_H265_H_#include &quot;absl/types/optional.h&quot;#include &quot;modules/rtp_rtcp/source/video_rtp_depacketizer.h&quot;#include &quot;rtc_base/copy_on_write_buffer.h&quot;namespace webrtc {class VideoRtpDepacketizerH265 : public VideoRtpDepacketizer { public: ~VideoRtpDepacketizerH265() override = default; absl::optional&lt;ParsedRtpPayload&gt; Parse( rtc::CopyOnWriteBuffer rtp_payload) override;private: struct ParsedPayload { RTPVideoHeader&amp; video_header() { return video; } const RTPVideoHeader&amp; video_header() const { return video; } RTPVideoHeader video; const uint8_t* payload; size_t payload_length; }; bool Parse(ParsedPayload* parsed_payload, const uint8_t* payload_data, size_t payload_data_length); bool ParseFuNalu(ParsedPayload* parsed_payload, const uint8_t* payload_data); bool ProcessApOrSingleNalu(ParsedPayload* parsed_payload, const uint8_t* payload_data); size_t offset_; size_t length_; std::unique_ptr&lt;rtc::Buffer&gt; modified_buffer_;};} // namespace webrtc#endif // MODULES_RTP_RTCP_SOURCE_VIDEO_RTP_DEPACKETIZER_H265_H_ modules/rtp_rtcp/source/video_rtp_depacketizer_h265.cc >folded#include &quot;modules/rtp_rtcp/source/video_rtp_depacketizer_h265.h&quot;#include &lt;cstddef&gt;#include &lt;cstdint&gt;#include &lt;utility&gt;#include &lt;vector&gt;#include &quot;absl/base/macros.h&quot;#include &quot;absl/types/optional.h&quot;#include &quot;absl/types/variant.h&quot;#include &quot;common_video/h264/h264_common.h&quot;#include &quot;common_video/h265/h265_common.h&quot;#include &quot;common_video/h265/h265_pps_parser.h&quot;#include &quot;common_video/h265/h265_sps_parser.h&quot;#include &quot;common_video/h265/h265_vps_parser.h&quot;#include &quot;modules/rtp_rtcp/source/byte_io.h&quot;#include &quot;modules/rtp_rtcp/source/video_rtp_depacketizer.h&quot;#include &quot;rtc_base/checks.h&quot;#include &quot;rtc_base/copy_on_write_buffer.h&quot;#include &quot;rtc_base/logging.h&quot;namespace webrtc {namespace {/* 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | PayloadHdr (Type=49) | FU header | DONL (cond) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-|*/// Unlike H.264, HEVC NAL header is 2-bytes.static const size_t kHevcNalHeaderSize = 2;// H.265's FU is constructed of 2-byte payload header, and 1-byte FU headerstatic const size_t kHevcFuHeaderSize = 1;static const size_t kHevcLengthFieldSize = 2;static const size_t kHevcApHeaderSize = kHevcNalHeaderSize + kHevcLengthFieldSize;enum HevcNalHdrMasks { kHevcFBit = 0x80, kHevcTypeMask = 0x7E, kHevcLayerIDHMask = 0x1, kHevcLayerIDLMask = 0xF8, kHevcTIDMask = 0x7, kHevcTypeMaskN = 0x81, kHevcTypeMaskInFuHeader = 0x3F};// Bit masks for FU headers.enum HevcFuDefs { kHevcSBit = 0x80, kHevcEBit = 0x40, kHevcFuTypeBit = 0x3F };// TODO(pbos): Avoid parsing this here as well as inside the jitter buffer.bool ParseApStartOffsets(const uint8_t* nalu_ptr, size_t length_remaining, std::vector&lt;size_t&gt;* offsets) { size_t offset = 0; while (length_remaining &gt; 0) { // Buffer doesn't contain room for additional nalu length. if (length_remaining &lt; sizeof(uint16_t)) return false; uint16_t nalu_size = ByteReader&lt;uint16_t&gt;::ReadBigEndian(nalu_ptr); nalu_ptr += sizeof(uint16_t); length_remaining -= sizeof(uint16_t); if (nalu_size &gt; length_remaining) return false; nalu_ptr += nalu_size; length_remaining -= nalu_size; offsets-&gt;push_back(offset + kHevcApHeaderSize); offset += kHevcLengthFieldSize + nalu_size; } return true;}} // namespacebool VideoRtpDepacketizerH265::Parse(ParsedPayload* parsed_payload, const uint8_t* payload_data, size_t payload_data_length) { RTC_CHECK(parsed_payload != nullptr); if (payload_data_length == 0) { RTC_LOG(LS_ERROR) &lt;&lt; &quot;Empty payload.&quot;; return false; } offset_ = 0; length_ = payload_data_length; modified_buffer_.reset(); uint8_t nal_type = (payload_data[0] &amp; kHevcTypeMask) &gt;&gt; 1; parsed_payload-&gt;video_header() .video_type_header.emplace&lt;RTPVideoHeaderH265&gt;(); if (nal_type == H265::NaluType::kFU) { // Fragmented NAL units (FU-A). if (!ParseFuNalu(parsed_payload, payload_data)) return false; } else { // We handle STAP-A and single NALU's the same way here. The jitter buffer // will depacketize the STAP-A into NAL units later. // TODO(sprang): Parse STAP-A offsets here and store in fragmentation vec. if (!ProcessApOrSingleNalu(parsed_payload, payload_data)) return false; } const uint8_t* payload = modified_buffer_ ? modified_buffer_-&gt;data() : payload_data; parsed_payload-&gt;payload = payload + offset_; parsed_payload-&gt;payload_length = length_; return true;}bool VideoRtpDepacketizerH265::ProcessApOrSingleNalu( ParsedPayload* parsed_payload, const uint8_t* payload_data) { parsed_payload-&gt;video_header().width = 0; parsed_payload-&gt;video_header().height = 0; parsed_payload-&gt;video_header().codec = kVideoCodecH265; parsed_payload-&gt;video_header().is_first_packet_in_frame = true; auto&amp; h265_header = absl::get&lt;RTPVideoHeaderH265&gt;( parsed_payload-&gt;video_header().video_type_header); const uint8_t* nalu_start = payload_data + kHevcNalHeaderSize; const size_t nalu_length = length_ - kHevcNalHeaderSize; uint8_t nal_type = (payload_data[0] &amp; kHevcTypeMask) &gt;&gt; 1; std::vector&lt;size_t&gt; nalu_start_offsets; if (nal_type == H265::NaluType::kAP) { // Skip the StapA header (StapA NAL type + length). if (length_ &lt;= kHevcApHeaderSize) { RTC_LOG(LS_ERROR) &lt;&lt; &quot;AP header truncated.&quot;; return false; } if (!ParseApStartOffsets(nalu_start, nalu_length, &amp;nalu_start_offsets)) { RTC_LOG(LS_ERROR) &lt;&lt; &quot;AP packet with incorrect NALU packet lengths.&quot;; return false; } h265_header.packetization_type = kH265AP; // nal_type = (payload_data[kHevcApHeaderSize] &amp; kHevcTypeMask) &gt;&gt; 1; } else { h265_header.packetization_type = kH265SingleNalu; nalu_start_offsets.push_back(0); } h265_header.nalu_type = nal_type; parsed_payload-&gt;video_header().frame_type = VideoFrameType::kVideoFrameDelta; nalu_start_offsets.push_back(length_ + kHevcLengthFieldSize); // End offset. for (size_t i = 0; i &lt; nalu_start_offsets.size() - 1; ++i) { size_t start_offset = nalu_start_offsets[i]; // End offset is actually start offset for next unit, excluding length field // so remove that from this units length. size_t end_offset = nalu_start_offsets[i + 1] - kHevcLengthFieldSize; if (end_offset - start_offset &lt; kHevcNalHeaderSize) { // Same as H.264. RTC_LOG(LS_ERROR) &lt;&lt; &quot;AP packet too short&quot;; return false; } H265NaluInfo nalu; nalu.type = (payload_data[start_offset] &amp; kHevcTypeMask) &gt;&gt; 1; nalu.vps_id = -1; nalu.sps_id = -1; nalu.pps_id = -1; start_offset += kHevcNalHeaderSize; switch (nalu.type) { case H265::NaluType::kVps: { absl::optional&lt;H265VpsParser::VpsState&gt; vps = H265VpsParser::ParseVps( &amp;payload_data[start_offset], end_offset - start_offset); if (vps) { nalu.vps_id = vps-&gt;id; } else { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Failed to parse VPS id from VPS slice.&quot;; } break; } case H265::NaluType::kSps: { // Check if VUI is present in SPS and if it needs to be modified to // avoid excessive decoder latency. // Copy any previous data first (likely just the first header). std::unique_ptr&lt;rtc::Buffer&gt; output_buffer(new rtc::Buffer()); if (start_offset) output_buffer-&gt;AppendData(payload_data, start_offset); absl::optional&lt;H265SpsParser::SpsState&gt; sps = H265SpsParser::ParseSps( &amp;payload_data[start_offset], end_offset - start_offset); if (sps) { parsed_payload-&gt;video_header().width = sps-&gt;width; parsed_payload-&gt;video_header().height = sps-&gt;height; nalu.sps_id = sps-&gt;id; nalu.vps_id = sps-&gt;vps_id; } else { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Failed to parse SPS and VPS id from SPS slice.&quot;; } parsed_payload-&gt;video_header().frame_type = VideoFrameType::kVideoFrameKey; break; } case H265::NaluType::kPps: { uint32_t pps_id; uint32_t sps_id; if (H265PpsParser::ParsePpsIds(&amp;payload_data[start_offset], end_offset - start_offset, &amp;pps_id, &amp;sps_id)) { nalu.pps_id = pps_id; nalu.sps_id = sps_id; } else { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Failed to parse PPS id and SPS id from PPS slice.&quot;; } break; } case H265::NaluType::kIdrWRadl: case H265::NaluType::kIdrNLp: case H265::NaluType::kCra: parsed_payload-&gt;video_header().frame_type = VideoFrameType::kVideoFrameKey; ABSL_FALLTHROUGH_INTENDED; case H265::NaluType::kTrailN: case H265::NaluType::kTrailR: { absl::optional&lt;uint32_t&gt; pps_id = H265PpsParser::ParsePpsIdFromSliceSegmentLayerRbsp( &amp;payload_data[start_offset], end_offset - start_offset, nalu.type); if (pps_id) { nalu.pps_id = *pps_id; } else { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Failed to parse PPS id from slice of type: &quot; &lt;&lt; static_cast&lt;int&gt;(nalu.type); } break; } // Slices below don't contain SPS or PPS ids. case H265::NaluType::kAud: case H265::NaluType::kTsaN: case H265::NaluType::kTsaR: case H265::NaluType::kStsaN: case H265::NaluType::kStsaR: case H265::NaluType::kRadlN: case H265::NaluType::kRadlR: case H265::NaluType::kBlaWLp: case H265::NaluType::kBlaWRadl: case H265::NaluType::kPrefixSei: case H265::NaluType::kSuffixSei: break; case H265::NaluType::kAP: case H265::NaluType::kFU: RTC_LOG(LS_WARNING) &lt;&lt; &quot;Unexpected AP or FU received.&quot;; return false; } if (h265_header.nalus_length == kMaxNalusPerPacket) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Received packet containing more than &quot; &lt;&lt; kMaxNalusPerPacket &lt;&lt; &quot; NAL units. Will not keep track sps and pps ids for all of them.&quot;; } else { h265_header.nalus[h265_header.nalus_length++] = nalu; } } return true;}bool VideoRtpDepacketizerH265::ParseFuNalu( ParsedPayload* parsed_payload, const uint8_t* payload_data) { if (length_ &lt; kHevcFuHeaderSize + kHevcNalHeaderSize) { RTC_LOG(LS_ERROR) &lt;&lt; &quot;FU NAL units truncated.&quot;; return false; } uint8_t f = payload_data[0] &amp; kHevcFBit; uint8_t layer_id_h = payload_data[0] &amp; kHevcLayerIDHMask; uint8_t layer_id_l_unshifted = payload_data[1] &amp; kHevcLayerIDLMask; uint8_t tid = payload_data[1] &amp; kHevcTIDMask; uint8_t original_nal_type = payload_data[2] &amp; kHevcTypeMaskInFuHeader; bool first_fragment = payload_data[2] &amp; kHevcSBit; H265NaluInfo nalu; nalu.type = original_nal_type; nalu.vps_id = -1; nalu.sps_id = -1; nalu.pps_id = -1; if (first_fragment) { offset_ = 1; length_ -= 1; absl::optional&lt;uint32_t&gt; pps_id = H265PpsParser::ParsePpsIdFromSliceSegmentLayerRbsp( payload_data + kHevcNalHeaderSize + kHevcFuHeaderSize, length_ - kHevcFuHeaderSize, nalu.type); if (pps_id) { nalu.pps_id = *pps_id; } else { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Failed to parse PPS from first fragment of FU NAL &quot; &quot;unit with original type: &quot; &lt;&lt; static_cast&lt;int&gt;(nalu.type); } uint8_t* payload = const_cast&lt;uint8_t*&gt;(payload_data + offset_); payload[0] = f | original_nal_type &lt;&lt; 1 | layer_id_h; payload[1] = layer_id_l_unshifted | tid; } else { offset_ = kHevcNalHeaderSize + kHevcFuHeaderSize; length_ -= (kHevcNalHeaderSize + kHevcFuHeaderSize); } if (original_nal_type == H265::NaluType::kIdrWRadl || original_nal_type == H265::NaluType::kIdrNLp || original_nal_type == H265::NaluType::kCra) { parsed_payload-&gt;video_header().frame_type = VideoFrameType::kVideoFrameKey; } else { parsed_payload-&gt;video_header().frame_type = VideoFrameType::kVideoFrameDelta; } parsed_payload-&gt;video_header().width = 0; parsed_payload-&gt;video_header().height = 0; parsed_payload-&gt;video_header().codec = kVideoCodecH265; parsed_payload-&gt;video_header().is_first_packet_in_frame = first_fragment; auto&amp; h265_header = absl::get&lt;RTPVideoHeaderH265&gt;( parsed_payload-&gt;video_header().video_type_header); h265_header.packetization_type = kH265FU; h265_header.nalu_type = original_nal_type; if (first_fragment) { h265_header.nalus[h265_header.nalus_length] = nalu; h265_header.nalus_length = 1; } return true;}absl::optional&lt;VideoRtpDepacketizer::ParsedRtpPayload&gt;VideoRtpDepacketizerH265::Parse(rtc::CopyOnWriteBuffer rtp_payload) { // borrowed from https://webrtc.googlesource.com/src/+/ // 07b17df771af20a6dd98b795592acc62a623c56f // /modules/rtp_rtcp/source/create_video_rtp_depacketizer.cc ParsedPayload parsed_payload; if (!Parse(&amp;parsed_payload, rtp_payload.cdata(), rtp_payload.size())) { return absl::nullopt; } absl::optional&lt;ParsedRtpPayload&gt; result(absl::in_place); result-&gt;video_header = parsed_payload.video; result-&gt;video_payload.SetData(parsed_payload.payload, parsed_payload.payload_length); return result;}} // namespace webrtc 将新增4个文件添加到ninja中参与构建，并通过rtc_use_h265打开H265开关： modules/rtp_rtcp/BUILD.gnif (rtc_enable_bwe_test_logging) { defines = [ &quot;BWE_TEST_LOGGING_COMPILE_TIME_ENABLE=1&quot; ] } else { defines = [ &quot;BWE_TEST_LOGGING_COMPILE_TIME_ENABLE=0&quot; ] }if (rtc_use_h265) { sources += [ &quot;source/rtp_format_h265.cc&quot;, &quot;source/rtp_format_h265.h&quot;, &quot;source/video_rtp_depacketizer_h265.cc&quot;, &quot;source/video_rtp_depacketizer_h265.h&quot;, ] } if (!rtc_use_h265) { defines += [&quot;DISABLE_H265&quot;] } 解包支持 增加H265解包支持 modules/video_coding/include/video_codec_interface.h#include &quot;modules/video_coding/codecs/h264/include/h264_globals.h&quot; #ifndef DISABLE_H265 #include &quot;modules/video_coding/codecs/h265/include/h265_globals.h&quot; #endif... #ifndef DISABLE_H265 struct CodecSpecificInfoH265 { H265PacketizationMode packetization_mode; bool idr_frame; }; #endifunion CodecSpecificInfoUnion { CodecSpecificInfoVP8 VP8; CodecSpecificInfoVP9 VP9; CodecSpecificInfoH264 H264; #ifndef DISABLE_H265 CodecSpecificInfoH265 H265; #endif};static_assert(std::is_pod&lt;CodecSpecificInfoUnion&gt;::value, &quot;&quot;); 设置codec类型 modules/video_coding/encoded_frame.ccvoid VCMEncodedFrame::CopyCodecSpecific(const RTPVideoHeader* header) {... #ifndef DISABLE_H265 case kVideoCodecH265: { _codecSpecificInfo.codecType = kVideoCodecH265; break; } #endif default: { _codecSpecificInfo.codecType = kVideoCodecGeneric; break; }} 新增tracker 解析vps/sps/pps信息，参考h264_sps_pps_tracker modules/video_coding/h265_vps_sps_pps_tracker.cc >folded#include &quot;modules/video_coding/h265_vps_sps_pps_tracker.h&quot;#include &lt;string&gt;#include &lt;utility&gt;#include &quot;common_video/h264/h264_common.h&quot;#include &quot;common_video/h265/h265_common.h&quot;#include &quot;common_video/h265/h265_pps_parser.h&quot;#include &quot;common_video/h265/h265_sps_parser.h&quot;#include &quot;common_video/h265/h265_vps_parser.h&quot;#include &quot;modules/video_coding/codecs/h264/include/h264_globals.h&quot;#include &quot;modules/video_coding/codecs/h265/include/h265_globals.h&quot;#include &quot;modules/video_coding/frame_object.h&quot;#include &quot;modules/video_coding/packet_buffer.h&quot;#include &quot;rtc_base/checks.h&quot;#include &quot;rtc_base/logging.h&quot;namespace webrtc {namespace video_coding {namespace {const uint8_t start_code_h265[] = {0, 0, 0, 1};} // namespaceH265VpsSpsPpsTracker::FixedBitstream H265VpsSpsPpsTracker::CopyAndFixBitstream( rtc::ArrayView&lt;const uint8_t&gt; bitstream, RTPVideoHeader* video_header) { RTC_DCHECK(video_header); RTC_DCHECK(video_header-&gt;codec == kVideoCodecH265); auto&amp; h265_header = absl::get&lt;RTPVideoHeaderH265&gt;(video_header-&gt;video_type_header); bool append_vps_sps_pps = false; auto vps = vps_data_.end(); auto sps = sps_data_.end(); auto pps = pps_data_.end(); for (size_t i = 0; i &lt; h265_header.nalus_length; ++i) { const H265NaluInfo&amp; nalu = h265_header.nalus[i]; switch (nalu.type) { case H265::NaluType::kVps: { vps_data_[nalu.vps_id].size = 0; break; } case H265::NaluType::kSps: { sps_data_[nalu.sps_id].vps_id = nalu.vps_id; sps_data_[nalu.sps_id].width = video_header-&gt;width; sps_data_[nalu.sps_id].height = video_header-&gt;height; break; } case H265::NaluType::kPps: { pps_data_[nalu.pps_id].sps_id = nalu.sps_id; break; } case H265::NaluType::kIdrWRadl: case H265::NaluType::kIdrNLp: case H265::NaluType::kCra: { // If this is the first packet of an IDR, make sure we have the required // SPS/PPS and also calculate how much extra space we need in the buffer // to prepend the SPS/PPS to the bitstream with start codes. if (video_header-&gt;is_first_packet_in_frame) { if (nalu.pps_id == -1) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;No PPS id in IDR nalu.&quot;; return {kRequestKeyframe}; } pps = pps_data_.find(nalu.pps_id); if (pps == pps_data_.end()) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;No PPS with id &lt;&lt; &quot; &lt;&lt; nalu.pps_id &lt;&lt; &quot; received&quot;; return {kRequestKeyframe}; } sps = sps_data_.find(pps-&gt;second.sps_id); if (sps == sps_data_.end()) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;No SPS with id &lt;&lt; &quot; &lt;&lt; pps-&gt;second.sps_id &lt;&lt; &quot; received&quot;; return {kRequestKeyframe}; } vps = vps_data_.find(sps-&gt;second.vps_id); if (vps == vps_data_.end()) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;No VPS with id &lt;&lt; &quot; &lt;&lt; sps-&gt;second.vps_id &lt;&lt; &quot; received&quot;; return {kRequestKeyframe}; } // Since the first packet of every keyframe should have its width and // height set we set it here in the case of it being supplied out of // band. video_header-&gt;width = sps-&gt;second.width; video_header-&gt;height = sps-&gt;second.height; // If the VPS/SPS/PPS was supplied out of band then we will have saved // the actual bitstream in |data|. // This branch is not verified. if (vps-&gt;second.data &amp;&amp; sps-&gt;second.data &amp;&amp; pps-&gt;second.data) { RTC_DCHECK_GT(vps-&gt;second.size, 0); RTC_DCHECK_GT(sps-&gt;second.size, 0); RTC_DCHECK_GT(pps-&gt;second.size, 0); append_vps_sps_pps = true; } } break; } default: break; } } RTC_CHECK(!append_vps_sps_pps || (sps != sps_data_.end() &amp;&amp; pps != pps_data_.end())); // Calculate how much space we need for the rest of the bitstream. size_t required_size = 0; if (append_vps_sps_pps) { required_size += vps-&gt;second.size + sizeof(start_code_h265); required_size += sps-&gt;second.size + sizeof(start_code_h265); required_size += pps-&gt;second.size + sizeof(start_code_h265); } if (h265_header.packetization_type == kH265AP) { const uint8_t* nalu_ptr = bitstream.data() + 1; while (nalu_ptr &lt; bitstream.data() + bitstream.size()) { RTC_DCHECK(video_header-&gt;is_first_packet_in_frame); required_size += sizeof(start_code_h265); // The first two bytes describe the length of a segment. uint16_t segment_length = nalu_ptr[0] &lt;&lt; 8 | nalu_ptr[1]; nalu_ptr += 2; required_size += segment_length; nalu_ptr += segment_length; } } else { if (video_header-&gt;is_first_packet_in_frame) required_size += sizeof(start_code_h265); required_size += bitstream.size(); } // Then we copy to the new buffer. H265VpsSpsPpsTracker::FixedBitstream fixed; fixed.bitstream.EnsureCapacity(required_size); if (append_vps_sps_pps) { // Insert VPS. fixed.bitstream.AppendData(start_code_h265); fixed.bitstream.AppendData(vps-&gt;second.data.get(), vps-&gt;second.size); // Insert SPS. fixed.bitstream.AppendData(start_code_h265); fixed.bitstream.AppendData(sps-&gt;second.data.get(), sps-&gt;second.size); // Insert PPS. fixed.bitstream.AppendData(start_code_h265); fixed.bitstream.AppendData(pps-&gt;second.data.get(), pps-&gt;second.size); // Update codec header to reflect the newly added SPS and PPS. H265NaluInfo vps_info; vps_info.type = H265::NaluType::kVps; vps_info.vps_id = vps-&gt;first; vps_info.sps_id = -1; vps_info.pps_id = -1; H265NaluInfo sps_info; sps_info.type = H265::NaluType::kSps; sps_info.vps_id = vps-&gt;first; sps_info.sps_id = sps-&gt;first; sps_info.pps_id = -1; H265NaluInfo pps_info; pps_info.type = H265::NaluType::kPps; pps_info.vps_id = vps-&gt;first; pps_info.sps_id = sps-&gt;first; pps_info.pps_id = pps-&gt;first; if (h265_header.nalus_length + 3 &lt;= kMaxNalusPerPacket) { h265_header.nalus[h265_header.nalus_length++] = vps_info; h265_header.nalus[h265_header.nalus_length++] = sps_info; h265_header.nalus[h265_header.nalus_length++] = pps_info; } else { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Not enough space in H.265 codec header to insert &quot; &quot;SPS/PPS provided out-of-band.&quot;; } } // Copy the rest of the bitstream and insert start codes. if (h265_header.packetization_type == kH265AP) { const uint8_t* nalu_ptr = bitstream.data() + 1; while (nalu_ptr &lt; bitstream.data() + bitstream.size()) { fixed.bitstream.AppendData(start_code_h265); // The first two bytes describe the length of a segment. uint16_t segment_length = nalu_ptr[0] &lt;&lt; 8 | nalu_ptr[1]; nalu_ptr += 2; size_t copy_end = nalu_ptr - bitstream.data() + segment_length; if (copy_end &gt; bitstream.size()) { return {kDrop}; } fixed.bitstream.AppendData(nalu_ptr, segment_length); nalu_ptr += segment_length; } } else { if (video_header-&gt;is_first_packet_in_frame) { fixed.bitstream.AppendData(start_code_h265); } fixed.bitstream.AppendData(bitstream.data(), bitstream.size()); } fixed.action = kInsert; return fixed;}void H265VpsSpsPpsTracker::InsertVpsSpsPpsNalus( const std::vector&lt;uint8_t&gt;&amp; vps, const std::vector&lt;uint8_t&gt;&amp; sps, const std::vector&lt;uint8_t&gt;&amp; pps) { constexpr size_t kNaluHeaderOffset = 1; if (vps.size() &lt; kNaluHeaderOffset) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;VPS size &quot; &lt;&lt; vps.size() &lt;&lt; &quot; is smaller than &quot; &lt;&lt; kNaluHeaderOffset; return; } if ((vps[0] &amp; 0x7e) &gt;&gt; 1 != H265::NaluType::kSps) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;SPS Nalu header missing&quot;; return; } if (sps.size() &lt; kNaluHeaderOffset) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;SPS size &quot; &lt;&lt; sps.size() &lt;&lt; &quot; is smaller than &quot; &lt;&lt; kNaluHeaderOffset; return; } if ((sps[0] &amp; 0x7e) &gt;&gt; 1 != H265::NaluType::kSps) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;SPS Nalu header missing&quot;; return; } if (pps.size() &lt; kNaluHeaderOffset) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;PPS size &quot; &lt;&lt; pps.size() &lt;&lt; &quot; is smaller than &quot; &lt;&lt; kNaluHeaderOffset; return; } if ((pps[0] &amp; 0x7e) &gt;&gt; 1 != H265::NaluType::kPps) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;SPS Nalu header missing&quot;; return; } absl::optional&lt;H265VpsParser::VpsState&gt; parsed_vps = H265VpsParser::ParseVps( vps.data() + kNaluHeaderOffset, vps.size() - kNaluHeaderOffset); absl::optional&lt;H265SpsParser::SpsState&gt; parsed_sps = H265SpsParser::ParseSps( sps.data() + kNaluHeaderOffset, sps.size() - kNaluHeaderOffset); absl::optional&lt;H265PpsParser::PpsState&gt; parsed_pps = H265PpsParser::ParsePps( pps.data() + kNaluHeaderOffset, pps.size() - kNaluHeaderOffset); if (!parsed_vps) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Failed to parse VPS.&quot;; } if (!parsed_sps) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Failed to parse SPS.&quot;; } if (!parsed_pps) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Failed to parse PPS.&quot;; } if (!parsed_vps || !parsed_pps || !parsed_sps) { return; } VpsInfo vps_info; vps_info.size = vps.size(); uint8_t* vps_data = new uint8_t[vps_info.size]; memcpy(vps_data, vps.data(), vps_info.size); vps_info.data.reset(vps_data); vps_data_[parsed_vps-&gt;id] = std::move(vps_info); SpsInfo sps_info; sps_info.size = sps.size(); sps_info.width = parsed_sps-&gt;width; sps_info.height = parsed_sps-&gt;height; sps_info.vps_id = parsed_sps-&gt;vps_id; uint8_t* sps_data = new uint8_t[sps_info.size]; memcpy(sps_data, sps.data(), sps_info.size); sps_info.data.reset(sps_data); sps_data_[parsed_sps-&gt;id] = std::move(sps_info); PpsInfo pps_info; pps_info.size = pps.size(); pps_info.sps_id = parsed_pps-&gt;sps_id; uint8_t* pps_data = new uint8_t[pps_info.size]; memcpy(pps_data, pps.data(), pps_info.size); pps_info.data.reset(pps_data); pps_data_[parsed_pps-&gt;id] = std::move(pps_info); RTC_LOG(LS_INFO) &lt;&lt; &quot;Inserted SPS id &quot; &lt;&lt; parsed_sps-&gt;id &lt;&lt; &quot; and PPS id &quot; &lt;&lt; parsed_pps-&gt;id &lt;&lt; &quot; (referencing SPS &quot; &lt;&lt; parsed_pps-&gt;sps_id &lt;&lt; &quot;)&quot;;}} // namespace video_coding} // namespace webrtc modules/video_coding/h265_vps_sps_pps_tracker.h >folded#ifndef MODULES_VIDEO_CODING_H265_VPS_SPS_PPS_TRACKER_H_#define MODULES_VIDEO_CODING_H265_VPS_SPS_PPS_TRACKER_H_#include &lt;cstdint&gt;#include &lt;map&gt;#include &lt;memory&gt;#include &lt;vector&gt;#include &quot;api/array_view.h&quot;#include &quot;modules/rtp_rtcp/source/rtp_video_header.h&quot;#include &quot;rtc_base/copy_on_write_buffer.h&quot;namespace webrtc {namespace video_coding {class H265VpsSpsPpsTracker { public: enum PacketAction { kInsert, kDrop, kRequestKeyframe }; struct FixedBitstream { PacketAction action; rtc::CopyOnWriteBuffer bitstream; }; // Returns fixed bitstream and modifies |video_header|. FixedBitstream CopyAndFixBitstream(rtc::ArrayView&lt;const uint8_t&gt; bitstream, RTPVideoHeader* video_header); void InsertVpsSpsPpsNalus(const std::vector&lt;uint8_t&gt;&amp; vps, const std::vector&lt;uint8_t&gt;&amp; sps, const std::vector&lt;uint8_t&gt;&amp; pps); private: struct VpsInfo { size_t size = 0; std::unique_ptr&lt;uint8_t[]&gt; data; }; struct PpsInfo { int sps_id = -1; size_t size = 0; std::unique_ptr&lt;uint8_t[]&gt; data; }; struct SpsInfo { int vps_id = -1; size_t size = 0; int width = -1; int height = -1; std::unique_ptr&lt;uint8_t[]&gt; data; }; std::map&lt;uint32_t, VpsInfo&gt; vps_data_; std::map&lt;uint32_t, PpsInfo&gt; pps_data_; std::map&lt;uint32_t, SpsInfo&gt; sps_data_;};} // namespace video_coding} // namespace webrtc#endif // MODULES_VIDEO_CODING_H264_SPS_PPS_TRACKER_H_ 在网络抖动缓存信息中新增枚举： modules/video_coding/jitter_buffer_common.henum { kH264StartCodeLengthBytes = 4 }; #ifndef DISABLE_H265 enum { kH265StartCodeLengthBytes = 4 }; #endif PacketBufferRTP包缓冲区增加H265支持： modules/video_coding/packet_buffer.cc#include &quot;common_video/h264/h264_common.h&quot;#ifndef DISABLE_H265#include &quot;common_video/h265/h265_common.h&quot;#endif#include &quot;modules/rtp_rtcp/source/rtp_header_extensions.h&quot;#include &quot;modules/rtp_rtcp/source/rtp_packet_received.h&quot;#include &quot;modules/rtp_rtcp/source/rtp_video_header.h&quot;#include &quot;modules/video_coding/codecs/h264/include/h264_globals.h&quot;#ifndef DISABLE_H265#include &quot;modules/video_coding/codecs/h265/include/h265_globals.h&quot;#endif#include &quot;rtc_base/checks.h&quot;...std::vector&lt;std::unique_ptr&lt;PacketBuffer::Packet&gt;&gt; PacketBuffer::FindFrames( uint16_t seq_num) { std::vector&lt;std::unique_ptr&lt;PacketBuffer::Packet&gt;&gt; found_frames; for (size_t i = 0; i &lt; buffer_.size() &amp;&amp; PotentialNewFrame(seq_num); ++i) {... bool is_h264_keyframe = false; bool is_h265 = false;#ifndef DISABLE_H265 is_h265 = buffer_[start_index]-&gt;codec() == kVideoCodecH265; bool has_h265_sps = false; bool has_h265_pps = false; bool has_h265_idr = false; bool is_h265_keyframe = false;#endif int idr_width = -1; int idr_height = -1; while (true) { ++tested_packets; if (!is_h264 &amp;&amp; !is_h265 &amp;&amp; buffer_[start_index]-&gt;is_first_packet_in_frame()) break; if (is_h264) { ... }#ifndef DISABLE_H265 if (is_h265 &amp;&amp; !is_h265_keyframe) { const auto* h265_header = absl::get_if&lt;RTPVideoHeaderH265&gt;( &amp;buffer_[start_index]-&gt;video_header.video_type_header); if (!h265_header || h265_header-&gt;nalus_length &gt;= kMaxNalusPerPacket) return found_frames; for (size_t j = 0; j &lt; h265_header-&gt;nalus_length; ++j) { if (h265_header-&gt;nalus[j].type == H265::NaluType::kSps) { has_h265_sps = true; } else if (h265_header-&gt;nalus[j].type == H265::NaluType::kPps) { has_h265_pps = true; } else if (h265_header-&gt;nalus[j].type == H265::NaluType::kIdrWRadl || h265_header-&gt;nalus[j].type == H265::NaluType::kIdrNLp || h265_header-&gt;nalus[j].type == H265::NaluType::kCra) { has_h265_idr = true; } } if ((has_h265_sps &amp;&amp; has_h265_pps) || has_h265_idr) { is_h265_keyframe = true; // Store the resolution of key frame which is the packet with // smallest index and valid resolution; typically its IDR or SPS // packet; there may be packet preceeding this packet, IDR's // resolution will be applied to them. if (buffer_[start_index]-&gt;width() &gt; 0 &amp;&amp; buffer_[start_index]-&gt;height() &gt; 0) { idr_width = buffer_[start_index]-&gt;width(); idr_height = buffer_[start_index]-&gt;height(); } } }#endif... if (is_h264) { ... }#ifndef DISABLE_H265 if (is_h265) { // Warn if this is an unsafe frame. if (has_h265_idr &amp;&amp; (!has_h265_sps || !has_h265_pps)) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Received H.265-IDR frame &quot; &lt;&lt; &quot;(SPS: &quot; &lt;&lt; has_h265_sps &lt;&lt; &quot;, PPS: &quot; &lt;&lt; has_h265_pps &lt;&lt; &quot;). Treating as delta frame since &quot; &lt;&lt; &quot;WebRTC-SpsPpsIdrIsH265Keyframe is always enabled.&quot;; } // Now that we have decided whether to treat this frame as a key frame // or delta frame in the frame buffer, we update the field that // determines if the RtpFrameObject is a key frame or delta frame. const size_t first_packet_index = start_seq_num % buffer_.size(); if (is_h265_keyframe) { buffer_[first_packet_index]-&gt;video_header.frame_type = VideoFrameType::kVideoFrameKey; if (idr_width &gt; 0 &amp;&amp; idr_height &gt; 0) { // IDR frame was finalized and we have the correct resolution for // IDR; update first packet to have same resolution as IDR. buffer_[first_packet_index]-&gt;video_header.width = idr_width; buffer_[first_packet_index]-&gt;video_header.height = idr_height; } } else { buffer_[first_packet_index]-&gt;video_header.frame_type = VideoFrameType::kVideoFrameDelta; } // If this is not a key frame, make sure there are no gaps in the // packet sequence numbers up until this point. if (!is_h265_keyframe &amp;&amp; missing_packets_.upper_bound(start_seq_num) != missing_packets_.begin()) { return found_frames; } }#endif... } return found_frames;} packet insetStartCode增加H265支持： modules/video_coding/packet.cc completeNALU(kNaluIncomplete),#ifndef DISABLE_H265 insertStartCode((videoHeader.codec == kVideoCodecH264 || videoHeader.codec == kVideoCodecH265) &amp;&amp; videoHeader.is_first_packet_in_frame),#else insertStartCode(videoHeader.codec == kVideoCodecH264 &amp;&amp; videoHeader.is_first_packet_in_frame),#endif video_header(videoHeader), 支持H265包拼接,新增函数GetH265NaluInfos： modules/video_coding/session_info.cc+ #ifndef DISABLE_H265+ std::vector&lt;H265NaluInfo&gt; VCMSessionInfo::GetH265NaluInfos() const {+ if (packets_.empty() || packets_.front().video_header.codec != kVideoCodecH265)+ return std::vector&lt;H265NaluInfo&gt;();+ std::vector&lt;H265NaluInfo&gt; nalu_infos;+ for (const VCMPacket&amp; packet : packets_) {+ const auto&amp; h265 =+ absl::get&lt;RTPVideoHeaderH265&gt;(packet.video_header.video_type_header);+ for (size_t i = 0; i &lt; h265.nalus_length; ++i) {+ nalu_infos.push_back(h265.nalus[i]);+ }+ }+ return nalu_infos;+ }+ #endif...size_t VCMSessionInfo::InsertBuffer(uint8_t* frame_buffer, PacketIterator packet_it) { ... const size_t kH264NALHeaderLengthInBytes = 1;+ #ifndef DISABLE_H265+ const size_t kH265NALHeaderLengthInBytes = 2;+ const auto* h265 =+ absl::get_if&lt;RTPVideoHeaderH265&gt;(&amp;packet.video_header.video_type_header);+ #endif ... return packet.sizeBytes;+ #ifndef DISABLE_H265+ } else if (h265 &amp;&amp; h265-&gt;packetization_type == kH265AP) {+ // Similar to H264, for H265 aggregation packets, we rely on jitter buffer+ // to remove the two length bytes between each NAL unit, and potentially add+ // start codes.+ size_t required_length = 0;+ const uint8_t* nalu_ptr =+ packet_buffer + kH265NALHeaderLengthInBytes; // skip payloadhdr+ while (nalu_ptr &lt; packet_buffer + packet.sizeBytes) {+ size_t length = BufferToUWord16(nalu_ptr);+ required_length +=+ length + (packet.insertStartCode ? kH265StartCodeLengthBytes : 0);+ nalu_ptr += kLengthFieldLength + length;+ }+ ShiftSubsequentPackets(packet_it, required_length);+ nalu_ptr = packet_buffer + kH265NALHeaderLengthInBytes;+ uint8_t* frame_buffer_ptr = frame_buffer + offset;+ while (nalu_ptr &lt; packet_buffer + packet.sizeBytes) {+ size_t length = BufferToUWord16(nalu_ptr);+ nalu_ptr += kLengthFieldLength;+ // since H265 shares the same start code as H264, use the same Insert+ // function to handle start code.+ frame_buffer_ptr += Insert(nalu_ptr, length, packet.insertStartCode,+ const_cast&lt;uint8_t*&gt;(frame_buffer_ptr));+ nalu_ptr += length;+ }+ packet.sizeBytes = required_length;+ return packet.sizeBytes;+ #endif } ShiftSubsequentPackets( packet_it, packet.sizeBytes + (packet.insertStartCode ? kH264StartCodeLengthBytes : 0)); packet.sizeBytes = Insert(packet_buffer, packet.sizeBytes, packet.insertStartCode, const_cast&lt;uint8_t*&gt;(packet.dataPtr)); return packet.sizeBytes;}...int VCMSessionInfo::InsertPacket(const VCMPacket&amp; packet, uint8_t* frame_buffer, const FrameData&amp; frame_data) {...+ #ifndef DISABLE_H265+ } else if (packet.codec() == kVideoCodecH265) {+ frame_type_ = packet.video_header.frame_type;+ if (packet.is_first_packet_in_frame() &amp;&amp;+ (first_packet_seq_num_ == -1 ||+ IsNewerSequenceNumber(first_packet_seq_num_, packet.seqNum))) {+ first_packet_seq_num_ = packet.seqNum;+ }+ if (packet.markerBit &amp;&amp;+ (last_packet_seq_num_ == -1 ||+ IsNewerSequenceNumber(packet.seqNum, last_packet_seq_num_))) {+ last_packet_seq_num_ = packet.seqNum;+ }+ #endif... return static_cast&lt;int&gt;(returnLength);} modules/video_coding/session_info.h std::vector&lt;NaluInfo&gt; GetNaluInfos() const;#ifndef DISABLE_H265 std::vector&lt;H265NaluInfo&gt; GetH265NaluInfos() const;#endif 将新增2个文件添加到ninja中参与构建： if (rtc_use_h265) { sources += [ &quot;h265_vps_sps_pps_tracker.cc&quot;, &quot;h265_vps_sps_pps_tracker.h&quot;, ] } RTP相关支持 配置最小码率枚举，避免编译不通过： absl::optional&lt;DataRate&gt; GetExperimentalMinVideoBitrate(VideoCodecType type) { ...#ifndef DISABLE_H265 case kVideoCodecH265:#endif case kVideoCodecGeneric: ...} 使rtp载荷增加H265支持： video/rtp_video_stream_receiver.ccvoid RtpVideoStreamReceiver::OnReceivedPayloadData( rtc::CopyOnWriteBuffer codec_payload, const RtpPacketReceived&amp; rtp_packet, const RTPVideoHeader&amp; video) {...#ifndef DISABLE_H265 } else if (packet-&gt;codec() == kVideoCodecH265) { // Only when we start to receive packets will we know what payload type // that will be used. When we know the payload type insert the correct // sps/pps into the tracker. if (packet-&gt;payload_type != last_payload_type_) { last_payload_type_ = packet-&gt;payload_type; InsertSpsPpsIntoTracker(packet-&gt;payload_type); } video_coding::H265VpsSpsPpsTracker::FixedBitstream fixed = h265_tracker_.CopyAndFixBitstream( rtc::MakeArrayView(codec_payload.cdata(), codec_payload.size()), &amp;packet-&gt;video_header); switch (fixed.action) { case video_coding::H265VpsSpsPpsTracker::kRequestKeyframe: rtcp_feedback_buffer_.RequestKeyFrame(); rtcp_feedback_buffer_.SendBufferedRtcpFeedback(); ABSL_FALLTHROUGH_INTENDED; case video_coding::H265VpsSpsPpsTracker::kDrop: return; case video_coding::H265VpsSpsPpsTracker::kInsert: packet-&gt;video_payload = std::move(fixed.bitstream); break; }#endif } else { packet-&gt;video_payload = std::move(codec_payload); } } video/rtp_video_stream_receiver.h#ifndef DISABLE_H265#include &quot;modules/video_coding/h265_vps_sps_pps_tracker.h&quot;#endif... std::map&lt;uint8_t, std::unique_ptr&lt;VideoRtpDepacketizer&gt;&gt; payload_type_map_;#ifndef DISABLE_H265 video_coding::H265VpsSpsPpsTracker h265_tracker_;#endif video/rtp_video_stream_receiver2.ccvoid RtpVideoStreamReceiver2::OnReceivedPayloadData( rtc::CopyOnWriteBuffer codec_payload, const RtpPacketReceived&amp; rtp_packet, const RTPVideoHeader&amp; video) {...#ifndef DISABLE_H265 } else if (packet-&gt;codec() == kVideoCodecH265) { // Only when we start to receive packets will we know what payload type // that will be used. When we know the payload type insert the correct // sps/pps into the tracker. if (packet-&gt;payload_type != last_payload_type_) { last_payload_type_ = packet-&gt;payload_type; InsertSpsPpsIntoTracker(packet-&gt;payload_type); } video_coding::H265VpsSpsPpsTracker::FixedBitstream fixed = h265_tracker_.CopyAndFixBitstream( rtc::MakeArrayView(codec_payload.cdata(), codec_payload.size()), &amp;packet-&gt;video_header); switch (fixed.action) { case video_coding::H265VpsSpsPpsTracker::kRequestKeyframe: rtcp_feedback_buffer_.RequestKeyFrame(); rtcp_feedback_buffer_.SendBufferedRtcpFeedback(); ABSL_FALLTHROUGH_INTENDED; case video_coding::H265VpsSpsPpsTracker::kDrop: return; case video_coding::H265VpsSpsPpsTracker::kInsert: packet-&gt;video_payload = std::move(fixed.bitstream); break; }#endif } else { packet-&gt;video_payload = std::move(codec_payload); } video/rtp_video_stream_receiver2.h#ifndef DISABLE_H265#include &quot;modules/video_coding/h265_vps_sps_pps_tracker.h&quot;#endif... std::map&lt;uint8_t, std::unique_ptr&lt;VideoRtpDepacketizer&gt;&gt; payload_type_map_ RTC_GUARDED_BY(worker_task_checker_);#ifndef DISABLE_H265 video_coding::H265VpsSpsPpsTracker h265_tracker_;#endif 统计支持： video/send_statistics_proxy.ccenum HistogramCodecType { kVideoUnknown = 0, kVideoVp8 = 1, kVideoVp9 = 2, kVideoH264 = 3,#ifndef DISABLE_H265 kVideoH265 = 4,#endif kVideoMax = 64,};HistogramCodecType PayloadNameToHistogramCodecType( const std::string&amp; payload_name) { VideoCodecType codecType = PayloadStringToCodecType(payload_name); switch (codecType) { case kVideoCodecVP8: return kVideoVp8; case kVideoCodecVP9: return kVideoVp9; case kVideoCodecH264: return kVideoH264;#ifndef DISABLE_H265 case kVideoCodecH265: return kVideoH265;#endif default: return kVideoUnknown; }} 接收流解码器初始化支持： video/video_receive_stream.ccVideoCodec CreateDecoderVideoCodec(const VideoReceiveStream::Decoder&amp; decoder) { VideoCodec codec; memset(&amp;codec, 0, sizeof(codec)); codec.codecType = PayloadStringToCodecType(decoder.video_format.name);... return associated_codec;#ifndef DISABLE_H265 } else if (codec.codecType == kVideoCodecH265) { *(codec.H265()) = VideoEncoder::GetDefaultH265Settings();#endif }... return codec;} 接收流编码器支持： video/video_stream_encoder.ccbool RequiresEncoderReset(const VideoCodec&amp; prev_send_codec, const VideoCodec&amp; new_send_codec, bool was_encode_called_since_last_initialization) {... case kVideoCodecH264: if (new_send_codec.H264() != prev_send_codec.H264()) { return true; } break;#ifndef DISABLE_H265 case kVideoCodecH265: if (new_send_codec.H265() != prev_send_codec.H265()) { return true; } break;#endif} 编解码器支持 视频数据中增加H265类型： api/video/encoded_image.h#include &quot;api/video/video_codec_type.h&quot; api/video/video_codec_type.h#ifndef DISABLE_H265enum VideoCodecType { // Java_cpp_enum.py does not allow ifdef in enum class, // so we have to create two version of VideoCodecType here kVideoCodecGeneric = 0, kVideoCodecVP8, kVideoCodecVP9, kVideoCodecAV1, kVideoCodecH264, kVideoCodecH265, kVideoCodecMultiplex,};#elseenum VideoCodecType { // There are various memset(..., 0, ...) calls in the code that rely on // kVideoCodecGeneric being zero. kVideoCodecGeneric = 0, kVideoCodecVP8, kVideoCodecVP9, kVideoCodecAV1, kVideoCodecH264, kVideoCodecMultiplex,};#endif codec基本信息支持： api/video_codecs/video_codec.ccconstexpr char kPayloadNameH264[] = &quot;H264&quot;;#ifndef DISABLE_H265constexpr char kPayloadNameH265[] = &quot;H265&quot;;#endif...#ifndef DISABLE_H265bool VideoCodecH265::operator==(const VideoCodecH265&amp; other) const { return (frameDroppingOn == other.frameDroppingOn &amp;&amp; keyFrameInterval == other.keyFrameInterval &amp;&amp; vpsLen == other.vpsLen &amp;&amp; spsLen == other.spsLen &amp;&amp; ppsLen == other.ppsLen &amp;&amp; (spsLen == 0 || memcmp(spsData, other.spsData, spsLen) == 0) &amp;&amp; (ppsLen == 0 || memcmp(ppsData, other.ppsData, ppsLen) == 0));}#endif...const VideoCodecH264&amp; VideoCodec::H264() const { RTC_DCHECK_EQ(codecType, kVideoCodecH264); return codec_specific_.H264;}#ifndef DISABLE_H265VideoCodecH265* VideoCodec::H265() { RTC_DCHECK_EQ(codecType, kVideoCodecH265); return &amp;codec_specific_.H265;}const VideoCodecH265&amp; VideoCodec::H265() const { RTC_DCHECK_EQ(codecType, kVideoCodecH265); return codec_specific_.H265;}#endifconst char* CodecTypeToPayloadString(VideoCodecType type) { ... case kVideoCodecH264: return kPayloadNameH264;#ifndef DISABLE_H265 case kVideoCodecH265: return kPayloadNameH265;#endif case kVideoCodecMultiplex: return kPayloadNameMultiplex; case kVideoCodecGeneric: return kPayloadNameGeneric;}VideoCodecType PayloadStringToCodecType(const std::string&amp; name) { ... if (absl::EqualsIgnoreCase(name, kPayloadNameH264)) return kVideoCodecH264;#ifndef DISABLE_H265 if (absl::EqualsIgnoreCase(name, kPayloadNameH265)) return kVideoCodecH265;#endif if (absl::EqualsIgnoreCase(name, kPayloadNameMultiplex)) return kVideoCodecMultiplex; return kVideoCodecGeneric;} api/video_codecs/video_codec.h#ifndef DISABLE_H265struct VideoCodecH265 { bool operator==(const VideoCodecH265&amp; other) const; bool operator!=(const VideoCodecH265&amp; other) const { return !(*this == other); } bool frameDroppingOn; int keyFrameInterval; const uint8_t* vpsData; size_t vpsLen; const uint8_t* spsData; size_t spsLen; const uint8_t* ppsData; size_t ppsLen;};#endif...union VideoCodecUnion { VideoCodecVP8 VP8; VideoCodecVP9 VP9; VideoCodecH264 H264;#ifndef DISABLE_H265 VideoCodecH265 H265;#endif};...class RTC_EXPORT VideoCodec { public: ...#ifndef DISABLE_H265 VideoCodecH265* H265(); const VideoCodecH265&amp; H265() const;#endif private: // TODO(hta): Consider replacing the union with a pointer type. // This will allow removing the VideoCodec* types from this file. VideoCodecUnion codec_specific_;}; 解码器降级支持： api/video_codecs/video_decoder_software_fallback_wrapper.ccvoid VideoDecoderSoftwareFallbackWrapper::UpdateFallbackDecoderHistograms() { switch (codec_settings_.codecType) { ... case kVideoCodecH264: RTC_HISTOGRAM_COUNTS_100000(kFallbackHistogramsUmaPrefix + &quot;H264&quot;, hw_decoded_frames_since_last_fallback_); break;#ifndef DISABLE_H265 case kVideoCodecH265: RTC_HISTOGRAM_COUNTS_100000(kFallbackHistogramsUmaPrefix + &quot;H265&quot;, hw_decoded_frames_since_last_fallback_); break;#endif case kVideoCodecMultiplex: RTC_HISTOGRAM_COUNTS_100000(kFallbackHistogramsUmaPrefix + &quot;Multiplex&quot;, hw_decoded_frames_since_last_fallback_); break; }} 编码器配置： api/video_codecs/video_encoder_config.ccvoid VideoEncoderConfig::EncoderSpecificSettings::FillEncoderSpecificSettings( VideoCodec* codec) const { if (codec-&gt;codecType == kVideoCodecH264) { FillVideoCodecH264(codec-&gt;H264()); } else if (codec-&gt;codecType == kVideoCodecVP8) { FillVideoCodecVp8(codec-&gt;VP8()); } else if (codec-&gt;codecType == kVideoCodecVP9) { FillVideoCodecVp9(codec-&gt;VP9());#ifndef DISABLE_H265 } else if (codec-&gt;codecType == kVideoCodecH265) { FillVideoCodecH265(codec-&gt;H265());#endif } else { RTC_NOTREACHED() &lt;&lt; &quot;Encoder specifics set/used for unknown codec type.&quot;; }}#ifndef DISABLE_H265void VideoEncoderConfig::EncoderSpecificSettings::FillVideoCodecH265( VideoCodecH265* h265_settings) const { RTC_NOTREACHED();}#endif#ifndef DISABLE_H265VideoEncoderConfig::H265EncoderSpecificSettings::H265EncoderSpecificSettings( const VideoCodecH265&amp; specifics) : specifics_(specifics) {}void VideoEncoderConfig::H265EncoderSpecificSettings::FillVideoCodecH265( VideoCodecH265* h265_settings) const { *h265_settings = specifics_;}#endif api/video_codecs/video_encoder_config.h class EncoderSpecificSettings : public rtc::RefCountInterface { public: ... virtual void FillVideoCodecH264(VideoCodecH264* h264_settings) const;#ifndef DISABLE_H265 virtual void FillVideoCodecH265(VideoCodecH265* h265_settings) const;#endif private: ~EncoderSpecificSettings() override {} friend class VideoEncoderConfig; };#ifndef DISABLE_H265 class H265EncoderSpecificSettings : public EncoderSpecificSettings { public: explicit H265EncoderSpecificSettings(const VideoCodecH265&amp; specifics); void FillVideoCodecH265(VideoCodecH265* h265_settings) const override; private: VideoCodecH265 specifics_; };#endif api/video_codecs/video_encoder.cc#ifndef DISABLE_H265VideoCodecH265 VideoEncoder::GetDefaultH265Settings() { VideoCodecH265 h265_settings; memset(&amp;h265_settings, 0, sizeof(h265_settings)); // h265_settings.profile = kProfileBase; h265_settings.frameDroppingOn = true; h265_settings.keyFrameInterval = 3000; h265_settings.spsData = nullptr; h265_settings.spsLen = 0; h265_settings.ppsData = nullptr; h265_settings.ppsLen = 0; return h265_settings;}#endif api/video_codecs/video_encoder.h static VideoCodecH264 GetDefaultH264Settings();#ifndef DISABLE_H265 static VideoCodecH265 GetDefaultH265Settings();#endif 载荷配置： call/rtp_payload_params.ccvoid PopulateRtpWithCodecSpecifics(const CodecSpecificInfo&amp; info, absl::optional&lt;int&gt; spatial_index, RTPVideoHeader* rtp) { switch (info.codecType) { ... case kVideoCodecH264: { auto&amp; h264_header = rtp-&gt;video_type_header.emplace&lt;RTPVideoHeaderH264&gt;(); h264_header.packetization_mode = info.codecSpecific.H264.packetization_mode; rtp-&gt;simulcastIdx = spatial_index.value_or(0); return; }#ifndef DISABLE_H265 case kVideoCodecH265: { auto h265_header = rtp-&gt;video_type_header.emplace&lt;RTPVideoHeaderH265&gt;(); h265_header.packetization_mode = info.codecSpecific.H265.packetization_mode; } return;#endif... }}void RtpPayloadParams::SetGeneric(const CodecSpecificInfo* codec_specific_info, int64_t frame_id, bool is_keyframe, RTPVideoHeader* rtp_video_header) {... switch (rtp_video_header-&gt;codec) { ... case VideoCodecType::kVideoCodecH264: if (codec_specific_info) { H264ToGeneric(codec_specific_info-&gt;codecSpecific.H264, frame_id, is_keyframe, rtp_video_header); } return;#ifndef DISABLE_H265 case VideoCodecType::kVideoCodecH265:#endif case VideoCodecType::kVideoCodecMultiplex: return; }} rtp信息解析配置 在common_video中新增h265文件夹： common_video/h265/h265_bitstream_parser.cc >folded#include &quot;common_video/h265/h265_bitstream_parser.h&quot;#include &lt;stdlib.h&gt;#include &lt;cstdint&gt;#include &lt;vector&gt;#include &quot;common_video/h265/h265_common.h&quot;#include &quot;rtc_base/bit_buffer.h&quot;#include &quot;rtc_base/logging.h&quot;namespace {const int kMaxAbsQpDeltaValue = 51;const int kMinQpValue = 0;const int kMaxQpValue = 51;} // namespacenamespace webrtc {#define RETURN_ON_FAIL(x, res) \\ if (!(x)) { \\ RTC_LOG_F(LS_ERROR) &lt;&lt; &quot;FAILED: &quot; #x; \\ return res; \\ }#define RETURN_INV_ON_FAIL(x) RETURN_ON_FAIL(x, kInvalidStream)H265BitstreamParser::H265BitstreamParser() {}H265BitstreamParser::~H265BitstreamParser() {}H265BitstreamParser::Result H265BitstreamParser::ParseNonParameterSetNalu( const uint8_t* source, size_t source_length, uint8_t nalu_type) { if (!sps_ || !pps_) return kInvalidStream; last_slice_qp_delta_ = absl::nullopt; const std::vector&lt;uint8_t&gt; slice_rbsp = H265::ParseRbsp(source, source_length); if (slice_rbsp.size() &lt; H265::kNaluTypeSize) return kInvalidStream; rtc::BitBuffer slice_reader(slice_rbsp.data() + H265::kNaluTypeSize, slice_rbsp.size() - H265::kNaluTypeSize); // Check to see if this is an IDR slice, which has an extra field to parse // out. //bool is_idr = (source[0] &amp; 0x0F) == H265::NaluType::kIdr; //uint8_t nal_ref_idc = (source[0] &amp; 0x60) &gt;&gt; 5; uint32_t golomb_tmp; uint32_t bits_tmp; // first_slice_segment_in_pic_flag: u(1) uint32_t first_slice_segment_in_pic_flag = 0; RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;first_slice_segment_in_pic_flag, 1)); if (H265::NaluType::kBlaWLp &lt;= nalu_type &amp;&amp; nalu_type &lt;= H265::NaluType::kRsvIrapVcl23) { // no_output_of_prior_pics_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, 1)); } // slice_pic_parameter_set_id: ue(v) RETURN_INV_ON_FAIL(slice_reader.ReadExponentialGolomb(&amp;golomb_tmp)); uint32_t dependent_slice_segment_flag = 0; if (first_slice_segment_in_pic_flag == 0) { if (pps_-&gt;dependent_slice_segments_enabled_flag) { // dependent_slice_segment_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;dependent_slice_segment_flag, 1)); } // slice_segment_address: u(v) int32_t log2_ctb_size_y = sps_-&gt;log2_min_luma_coding_block_size_minus3 + 3 + sps_-&gt;log2_diff_max_min_luma_coding_block_size; uint32_t ctb_size_y = 1 &lt;&lt; log2_ctb_size_y; uint32_t pic_width_in_ctbs_y = sps_-&gt;pic_width_in_luma_samples / ctb_size_y; if(sps_-&gt;pic_width_in_luma_samples % ctb_size_y) pic_width_in_ctbs_y++; uint32_t pic_height_in_ctbs_y = sps_-&gt;pic_height_in_luma_samples / ctb_size_y; if(sps_-&gt;pic_height_in_luma_samples % ctb_size_y) pic_height_in_ctbs_y++; uint32_t slice_segment_address_bits = H265::Log2(pic_height_in_ctbs_y * pic_width_in_ctbs_y); RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, slice_segment_address_bits)); } if (dependent_slice_segment_flag == 0) { for (uint32_t i = 0; i &lt; pps_-&gt;num_extra_slice_header_bits; i++) { // slice_reserved_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, 1)); } // slice_type: ue(v) uint32_t slice_type = 0; RETURN_INV_ON_FAIL(slice_reader.ReadExponentialGolomb(&amp;slice_type)); if (pps_-&gt;output_flag_present_flag) { // pic_output_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, 1)); } if (sps_-&gt;separate_colour_plane_flag) { // colour_plane_id: u(2) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, 2)); } uint32_t num_long_term_sps = 0; uint32_t num_long_term_pics = 0; std::vector&lt;uint32_t&gt; lt_idx_sps; std::vector&lt;uint32_t&gt; used_by_curr_pic_lt_flag; uint32_t short_term_ref_pic_set_sps_flag = 0; uint32_t short_term_ref_pic_set_idx = 0; H265SpsParser::ShortTermRefPicSet short_term_ref_pic_set; uint32_t slice_temporal_mvp_enabled_flag = 0; if (nalu_type != H265::NaluType::kIdrWRadl &amp;&amp; nalu_type != H265::NaluType::kIdrNLp) { // slice_pic_order_cnt_lsb: u(v) uint32_t slice_pic_order_cnt_lsb_bits = sps_-&gt;log2_max_pic_order_cnt_lsb_minus4 + 4; RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, slice_pic_order_cnt_lsb_bits)); // short_term_ref_pic_set_sps_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;short_term_ref_pic_set_sps_flag, 1)); if (!short_term_ref_pic_set_sps_flag) { absl::optional&lt;H265SpsParser::ShortTermRefPicSet&gt; ref_pic_set = H265SpsParser::ParseShortTermRefPicSet(sps_-&gt;num_short_term_ref_pic_sets, sps_-&gt;num_short_term_ref_pic_sets, sps_-&gt;short_term_ref_pic_set, *sps_, &amp;slice_reader); if (ref_pic_set) { short_term_ref_pic_set = *ref_pic_set; } else { return kInvalidStream; } } else if (sps_-&gt;num_short_term_ref_pic_sets &gt; 1) { // short_term_ref_pic_set_idx: u(v) uint32_t short_term_ref_pic_set_idx_bits = H265::Log2(sps_-&gt;num_short_term_ref_pic_sets); if ((uint32_t)(1 &lt;&lt; short_term_ref_pic_set_idx_bits) &lt; sps_-&gt;num_short_term_ref_pic_sets) { short_term_ref_pic_set_idx_bits++; } if (short_term_ref_pic_set_idx_bits &gt; 0) { RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;short_term_ref_pic_set_idx, short_term_ref_pic_set_idx_bits)); } } if (sps_-&gt;long_term_ref_pics_present_flag) { if (sps_-&gt;num_long_term_ref_pics_sps &gt; 0) { // num_long_term_sps: ue(v) RETURN_INV_ON_FAIL(slice_reader.ReadExponentialGolomb(&amp;num_long_term_sps)); } // num_long_term_sps: ue(v) RETURN_INV_ON_FAIL(slice_reader.ReadExponentialGolomb(&amp;num_long_term_pics)); lt_idx_sps.resize(num_long_term_sps + num_long_term_pics, 0); used_by_curr_pic_lt_flag.resize(num_long_term_sps + num_long_term_pics, 0); for (uint32_t i = 0; i &lt; num_long_term_sps + num_long_term_pics; i++) { if (i &lt; num_long_term_sps) { if (sps_-&gt;num_long_term_ref_pics_sps &gt; 1) { // lt_idx_sps: u(v) uint32_t lt_idx_sps_bits = H265::Log2(sps_-&gt;num_long_term_ref_pics_sps); RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;lt_idx_sps[i], lt_idx_sps_bits)); } } else { // poc_lsb_lt: u(v) uint32_t poc_lsb_lt_bits = sps_-&gt;log2_max_pic_order_cnt_lsb_minus4 + 4; RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, poc_lsb_lt_bits)); // used_by_curr_pic_lt_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;used_by_curr_pic_lt_flag[i], 1)); } // delta_poc_msb_present_flag: u(1) uint32_t delta_poc_msb_present_flag = 0; RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;delta_poc_msb_present_flag, 1)); if (delta_poc_msb_present_flag) { // delta_poc_msb_cycle_lt: ue(v) RETURN_INV_ON_FAIL(slice_reader.ReadExponentialGolomb(&amp;golomb_tmp)); } } } if (sps_-&gt;sps_temporal_mvp_enabled_flag) { // slice_temporal_mvp_enabled_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;slice_temporal_mvp_enabled_flag, 1)); } } if (sps_-&gt;sample_adaptive_offset_enabled_flag) { // slice_sao_luma_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, 1)); uint32_t chroma_array_type = sps_-&gt;separate_colour_plane_flag == 0 ? sps_-&gt;chroma_format_idc : 0; if (chroma_array_type != 0) { // slice_sao_chroma_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, 1)); } } if (slice_type == H265::SliceType::kP || slice_type == H265::SliceType::kB) { // num_ref_idx_active_override_flag: u(1) uint32_t num_ref_idx_active_override_flag = 0; RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;num_ref_idx_active_override_flag, 1)); uint32_t num_ref_idx_l0_active_minus1 = pps_-&gt;num_ref_idx_l0_default_active_minus1; uint32_t num_ref_idx_l1_active_minus1 = pps_-&gt;num_ref_idx_l1_default_active_minus1; if (num_ref_idx_active_override_flag) { // num_ref_idx_l0_active_minus1: ue(v) RETURN_INV_ON_FAIL(slice_reader.ReadExponentialGolomb(&amp;num_ref_idx_l0_active_minus1)); if (slice_type == H265::SliceType::kB) { // num_ref_idx_l1_active_minus1: ue(v) RETURN_INV_ON_FAIL(slice_reader.ReadExponentialGolomb(&amp;num_ref_idx_l1_active_minus1)); } } uint32_t num_pic_total_curr = CalcNumPocTotalCurr( num_long_term_sps, num_long_term_pics, lt_idx_sps, used_by_curr_pic_lt_flag, short_term_ref_pic_set_sps_flag, short_term_ref_pic_set_idx, short_term_ref_pic_set); if (pps_-&gt;lists_modification_present_flag &amp;&amp; num_pic_total_curr &gt; 1) { // ref_pic_lists_modification() uint32_t list_entry_bits = H265::Log2(num_pic_total_curr); if ((uint32_t)(1 &lt;&lt; list_entry_bits) &lt; num_pic_total_curr) { list_entry_bits++; } // ref_pic_list_modification_flag_l0: u(1) uint32_t ref_pic_list_modification_flag_l0 = 0; RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;ref_pic_list_modification_flag_l0, 1)); if (ref_pic_list_modification_flag_l0) { for (uint32_t i = 0; i &lt; num_ref_idx_l0_active_minus1; i++) { // list_entry_l0: u(v) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, list_entry_bits)); } } if (slice_type == H265::SliceType::kB) { // ref_pic_list_modification_flag_l1: u(1) uint32_t ref_pic_list_modification_flag_l1 = 0; RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;ref_pic_list_modification_flag_l1, 1)); if (ref_pic_list_modification_flag_l1) { for (uint32_t i = 0; i &lt; num_ref_idx_l1_active_minus1; i++) { // list_entry_l1: u(v) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, list_entry_bits)); } } } } if (slice_type == H265::SliceType::kB) { // mvd_l1_zero_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, 1)); } if (pps_-&gt;cabac_init_present_flag) { // cabac_init_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;bits_tmp, 1)); } if (slice_temporal_mvp_enabled_flag) { uint32_t collocated_from_l0_flag = 0; if (slice_type == H265::SliceType::kB) { // collocated_from_l0_flag: u(1) RETURN_INV_ON_FAIL(slice_reader.ReadBits(&amp;collocated_from_l0_flag, 1)); } if ((collocated_from_l0_flag &amp;&amp; num_ref_idx_l0_active_minus1 &gt; 0) || (!collocated_from_l0_flag &amp;&amp; num_ref_idx_l1_active_minus1 &gt; 0)) { // collocated_ref_idx: ue(v) RETURN_INV_ON_FAIL(slice_reader.ReadExponentialGolomb(&amp;golomb_tmp)); } } if ((pps_-&gt;weighted_pred_flag &amp;&amp; slice_type == H265::SliceType::kP) || (pps_-&gt;weighted_bipred_flag &amp;&amp; slice_type == H265::SliceType::kB)) { // pred_weight_table() // TODO(piasy): Do we need support for pred_weight_table()? RTC_LOG(LS_ERROR) &lt;&lt; &quot;Streams with pred_weight_table unsupported.&quot;; return kUnsupportedStream; } // five_minus_max_num_merge_cand: ue(v) RETURN_INV_ON_FAIL(slice_reader.ReadExponentialGolomb(&amp;golomb_tmp)); // TODO(piasy): motion_vector_resolution_control_idc? } } // slice_qp_delta: se(v) int32_t last_slice_qp_delta; RETURN_INV_ON_FAIL( slice_reader.ReadSignedExponentialGolomb(&amp;last_slice_qp_delta)); if (abs(last_slice_qp_delta) &gt; kMaxAbsQpDeltaValue) { // Something has gone wrong, and the parsed value is invalid. RTC_LOG(LS_WARNING) &lt;&lt; &quot;Parsed QP value out of range.&quot;; return kInvalidStream; } last_slice_qp_delta_ = last_slice_qp_delta; return kOk;}uint32_t H265BitstreamParser::CalcNumPocTotalCurr( uint32_t num_long_term_sps, uint32_t num_long_term_pics, const std::vector&lt;uint32_t&gt; lt_idx_sps, const std::vector&lt;uint32_t&gt; used_by_curr_pic_lt_flag, uint32_t short_term_ref_pic_set_sps_flag, uint32_t short_term_ref_pic_set_idx, const H265SpsParser::ShortTermRefPicSet&amp; short_term_ref_pic_set) { uint32_t num_poc_total_curr = 0; uint32_t curr_sps_idx; bool used_by_curr_pic_lt[16]; uint32_t num_long_term = num_long_term_sps + num_long_term_pics; for (uint32_t i = 0; i &lt; num_long_term; i++) { if (i &lt; num_long_term_sps) { used_by_curr_pic_lt[i] = sps_-&gt;used_by_curr_pic_lt_sps_flag[lt_idx_sps[i]]; } else { used_by_curr_pic_lt[i] = used_by_curr_pic_lt_flag[i]; } } if (short_term_ref_pic_set_sps_flag) { curr_sps_idx = short_term_ref_pic_set_idx; } else { curr_sps_idx = sps_-&gt;num_short_term_ref_pic_sets; } if (sps_-&gt;short_term_ref_pic_set.size() &lt;= curr_sps_idx) { if (curr_sps_idx != 0 || short_term_ref_pic_set_sps_flag) { return 0; } } const H265SpsParser::ShortTermRefPicSet* ref_pic_set; if (curr_sps_idx &lt; sps_-&gt;short_term_ref_pic_set.size()) { ref_pic_set = &amp;(sps_-&gt;short_term_ref_pic_set[curr_sps_idx]); } else { ref_pic_set = &amp;short_term_ref_pic_set; } for (uint32_t i = 0; i &lt; ref_pic_set-&gt;num_negative_pics; i++) { if (ref_pic_set-&gt;used_by_curr_pic_s0_flag[i]) { num_poc_total_curr++; } } for (uint32_t i = 0; i &lt; ref_pic_set-&gt;num_positive_pics; i++) { if (ref_pic_set-&gt;used_by_curr_pic_s1_flag[i]) { num_poc_total_curr++; } } for (uint32_t i = 0; i &lt; num_long_term_sps + num_long_term_pics; i++) { if (used_by_curr_pic_lt[i]) { num_poc_total_curr++; } } return num_poc_total_curr;}void H265BitstreamParser::ParseSlice(const uint8_t* slice, size_t length) { H265::NaluType nalu_type = H265::ParseNaluType(slice[0]); if (nalu_type == H265::NaluType::kSps) { sps_ = H265SpsParser::ParseSps(slice + H265::kNaluTypeSize, length - H265::kNaluTypeSize); if (!sps_) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Unable to parse SPS from H265 bitstream.&quot;; } } else if (nalu_type == H265::NaluType::kPps) { pps_ = H265PpsParser::ParsePps(slice + H265::kNaluTypeSize, length - H265::kNaluTypeSize); if (!pps_) { RTC_LOG(LS_WARNING) &lt;&lt; &quot;Unable to parse PPS from H265 bitstream.&quot;; } } else if (nalu_type &lt;= H265::NaluType::kRsvIrapVcl23) { Result res = ParseNonParameterSetNalu(slice, length, nalu_type); if (res != kOk) { RTC_LOG(LS_INFO) &lt;&lt; &quot;Failed to parse bitstream. Error: &quot; &lt;&lt; res; } }}void H265BitstreamParser::ParseBitstream(const uint8_t* bitstream, size_t length) { std::vector&lt;H265::NaluIndex&gt; nalu_indices = H265::FindNaluIndices(bitstream, length); for (const H265::NaluIndex&amp; index : nalu_indices) ParseSlice(&amp;bitstream[index.payload_start_offset], index.payload_size);}bool H265BitstreamParser::GetLastSliceQp(int* qp) const { if (!last_slice_qp_delta_ || !pps_) { return false; } const int parsed_qp = 26 + pps_-&gt;pic_init_qp_minus26 + *last_slice_qp_delta_; if (parsed_qp &lt; kMinQpValue || parsed_qp &gt; kMaxQpValue) { RTC_LOG(LS_ERROR) &lt;&lt; &quot;Parsed invalid QP from bitstream.&quot;; return false; } *qp = parsed_qp; return true;}void H265BitstreamParser::ParseBitstream( rtc::ArrayView&lt;const uint8_t&gt; bitstream) { ParseBitstream(bitstream.data(), bitstream.size());}absl::optional&lt;int&gt; H265BitstreamParser::GetLastSliceQp() const { int qp; bool success = GetLastSliceQp(&amp;qp); return success ? absl::optional&lt;int&gt;(qp) : absl::nullopt;}} // namespace webrtc common_video/h265/h265_bitstream_parser.h >folded#ifndef COMMON_VIDEO_H265_H265_BITSTREAM_PARSER_H_#define COMMON_VIDEO_H265_H265_BITSTREAM_PARSER_H_#include &lt;stddef.h&gt;#include &lt;stdint.h&gt;#include &quot;absl/types/optional.h&quot;#include &quot;api/video_codecs/bitstream_parser.h&quot;#include &quot;common_video/h265/h265_pps_parser.h&quot;#include &quot;common_video/h265/h265_sps_parser.h&quot;namespace webrtc {// Stateful H265 bitstream parser (due to SPS/PPS). Used to parse out QP values// from the bitstream.// TODO(pbos): Unify with RTP SPS parsing and only use one H265 parser.// TODO(pbos): If/when this gets used on the receiver side CHECKs must be// removed and gracefully abort as we have no control over receive-side// bitstreams.class H265BitstreamParser : public BitstreamParser { public: H265BitstreamParser(); ~H265BitstreamParser() override; // These are here for backwards-compatability for the time being. void ParseBitstream(const uint8_t* bitstream, size_t length); bool GetLastSliceQp(int* qp) const; // New interface. void ParseBitstream(rtc::ArrayView&lt;const uint8_t&gt; bitstream) override; absl::optional&lt;int&gt; GetLastSliceQp() const override; protected: enum Result { kOk, kInvalidStream, kUnsupportedStream, }; void ParseSlice(const uint8_t* slice, size_t length); Result ParseNonParameterSetNalu(const uint8_t* source, size_t source_length, uint8_t nalu_type); uint32_t CalcNumPocTotalCurr(uint32_t num_long_term_sps, uint32_t num_long_term_pics, const std::vector&lt;uint32_t&gt; lt_idx_sps, const std::vector&lt;uint32_t&gt; used_by_curr_pic_lt_flag, uint32_t short_term_ref_pic_set_sps_flag, uint32_t short_term_ref_pic_set_idx, const H265SpsParser::ShortTermRefPicSet&amp; short_term_ref_pic_set); // SPS/PPS state, updated when parsing new SPS/PPS, used to parse slices. absl::optional&lt;H265SpsParser::SpsState&gt; sps_; absl::optional&lt;H265PpsParser::PpsState&gt; pps_; // Last parsed slice QP. absl::optional&lt;int32_t&gt; last_slice_qp_delta_;};} // namespace webrtc#endif // COMMON_VIDEO_H265_H265_BITSTREAM_PARSER_H_ common_video/h265/h265_common.cc >folded#include &quot;common_video/h265/h265_common.h&quot;#include &quot;common_video/h264/h264_common.h&quot;namespace webrtc {namespace H265 {const uint8_t kNaluTypeMask = 0x7E;std::vector&lt;NaluIndex&gt; FindNaluIndices(const uint8_t* buffer, size_t buffer_size) { std::vector&lt;H264::NaluIndex&gt; indices = H264::FindNaluIndices(buffer, buffer_size); std::vector&lt;NaluIndex&gt; results; for (auto&amp; index : indices) { results.push_back({index.start_offset, index.payload_start_offset, index.payload_size}); } return results;}NaluType ParseNaluType(uint8_t data) { return static_cast&lt;NaluType&gt;((data &amp; kNaluTypeMask) &gt;&gt; 1);}std::vector&lt;uint8_t&gt; ParseRbsp(const uint8_t* data, size_t length) { return H264::ParseRbsp(data, length);}void WriteRbsp(const uint8_t* bytes, size_t length, rtc::Buffer* destination) { H264::WriteRbsp(bytes, length, destination);}uint32_t Log2(uint32_t value) { uint32_t result = 0; // If value is not a power of two an additional bit is required // to account for the ceil() of log2() below. if ((value &amp; (value - 1)) != 0) { ++result; } while (value &gt; 0) { value &gt;&gt;= 1; ++result; } return result;}} // namespace H265} // namespace webrtc common_video/h265/h265_common.h >folded#ifndef COMMON_VIDEO_H265_H265_COMMON_H_#define COMMON_VIDEO_H265_H265_COMMON_H_#include &lt;memory&gt;#include &lt;vector&gt;#include &quot;rtc_base/buffer.h&quot;namespace webrtc {namespace H265 {// The size of a full NALU start sequence {0 0 0 1}, used for the first NALU// of an access unit, and for SPS and PPS blocks.const size_t kNaluLongStartSequenceSize = 4;// The size of a shortened NALU start sequence {0 0 1}, that may be used if// not the first NALU of an access unit or an SPS or PPS block.const size_t kNaluShortStartSequenceSize = 3;// The size of the NALU type byte (2).const size_t kNaluTypeSize = 2;enum NaluType : uint8_t { kTrailN = 0, kTrailR = 1, kTsaN = 2, kTsaR = 3, kStsaN = 4, kStsaR = 5, kRadlN = 6, kRadlR = 7, kBlaWLp = 16, kBlaWRadl = 17, kBlaNLp = 18, kIdrWRadl = 19, kIdrNLp = 20, kCra = 21, kRsvIrapVcl23 = 23, kVps = 32, kSps = 33, kPps = 34, kAud = 35, kPrefixSei = 39, kSuffixSei = 40, kAP = 48, kFU = 49};enum SliceType : uint8_t { kB = 0, kP = 1, kI = 2 };struct NaluIndex { // Start index of NALU, including start sequence. size_t start_offset; // Start index of NALU payload, typically type header. size_t payload_start_offset; // Length of NALU payload, in bytes, counting from payload_start_offset. size_t payload_size;};// Returns a vector of the NALU indices in the given buffer.std::vector&lt;NaluIndex&gt; FindNaluIndices(const uint8_t* buffer, size_t buffer_size);// Get the NAL type from the header byte immediately following start sequence.NaluType ParseNaluType(uint8_t data);// Methods for parsing and writing RBSP. See section 7.4.2 of the H265 spec.//// The following sequences are illegal, and need to be escaped when encoding:// 00 00 00 -&gt; 00 00 03 00// 00 00 01 -&gt; 00 00 03 01// 00 00 02 -&gt; 00 00 03 02// And things in the source that look like the emulation byte pattern (00 00 03)// need to have an extra emulation byte added, so it's removed when decoding:// 00 00 03 -&gt; 00 00 03 03//// Decoding is simply a matter of finding any 00 00 03 sequence and removing// the 03 emulation byte.// Parse the given data and remove any emulation byte escaping.std::vector&lt;uint8_t&gt; ParseRbsp(const uint8_t* data, size_t length);// Write the given data to the destination buffer, inserting and emulation// bytes in order to escape any data the could be interpreted as a start// sequence.void WriteRbsp(const uint8_t* bytes, size_t length, rtc::Buffer* destination);uint32_t Log2(uint32_t value);} // namespace H265} // namespace webrtc#endif // COMMON_VIDEO_H265_H265_COMMON_H_ common_video/h265/h265_pps_parser.cc >folded#include &quot;common_video/h265/h265_pps_parser.h&quot;#include &lt;memory&gt;#include &lt;vector&gt;#include &quot;common_video/h265/h265_common.h&quot;#include &quot;common_video/h265/h265_sps_parser.h&quot;#include &quot;rtc_base/bit_buffer.h&quot;#include &quot;rtc_base/logging.h&quot;#define RETURN_EMPTY_ON_FAIL(x) \\ if (!(x)) { \\ return absl::nullopt; \\ }namespace {const int kMaxPicInitQpDeltaValue = 25;const int kMinPicInitQpDeltaValue = -26;} // namespacenamespace webrtc {// General note: this is based off the 06/2019 version of the H.265 standard.// You can find it on this page:// http://www.itu.int/rec/T-REC-H.265absl::optional&lt;H265PpsParser::PpsState&gt; H265PpsParser::ParsePps( const uint8_t* data, size_t length) { // First, parse out rbsp, which is basically the source buffer minus emulation // bytes (the last byte of a 0x00 0x00 0x03 sequence). RBSP is defined in // section 7.3.1.1 of the H.265 standard. std::vector&lt;uint8_t&gt; unpacked_buffer = H265::ParseRbsp(data, length); rtc::BitBuffer bit_buffer(unpacked_buffer.data(), unpacked_buffer.size()); return ParseInternal(&amp;bit_buffer);}bool H265PpsParser::ParsePpsIds(const uint8_t* data, size_t length, uint32_t* pps_id, uint32_t* sps_id) { RTC_DCHECK(pps_id); RTC_DCHECK(sps_id); // First, parse out rbsp, which is basically the source buffer minus emulation // bytes (the last byte of a 0x00 0x00 0x03 sequence). RBSP is defined in // section 7.3.1.1 of the H.265 standard. std::vector&lt;uint8_t&gt; unpacked_buffer = H265::ParseRbsp(data, length); rtc::BitBuffer bit_buffer(unpacked_buffer.data(), unpacked_buffer.size()); return ParsePpsIdsInternal(&amp;bit_buffer, pps_id, sps_id);}absl::optional&lt;uint32_t&gt; H265PpsParser::ParsePpsIdFromSliceSegmentLayerRbsp( const uint8_t* data, size_t length, uint8_t nalu_type) { rtc::BitBuffer slice_reader(data, length); // first_slice_segment_in_pic_flag: u(1) uint32_t first_slice_segment_in_pic_flag = 0; RETURN_EMPTY_ON_FAIL( slice_reader.ReadBits(&amp;first_slice_segment_in_pic_flag, 1)); if (nalu_type &gt;= H265::NaluType::kBlaWLp &amp;&amp; nalu_type &lt;= H265::NaluType::kRsvIrapVcl23) { // no_output_of_prior_pics_flag: u(1) RETURN_EMPTY_ON_FAIL(slice_reader.ConsumeBits(1)); } // slice_pic_parameter_set_id: ue(v) uint32_t slice_pic_parameter_set_id = 0; if (!slice_reader.ReadExponentialGolomb(&amp;slice_pic_parameter_set_id)) return absl::nullopt; return slice_pic_parameter_set_id;}absl::optional&lt;H265PpsParser::PpsState&gt; H265PpsParser::ParseInternal( rtc::BitBuffer* bit_buffer) { PpsState pps; RETURN_EMPTY_ON_FAIL(ParsePpsIdsInternal(bit_buffer, &amp;pps.id, &amp;pps.sps_id)); uint32_t bits_tmp; uint32_t golomb_ignored; int32_t signed_golomb_ignored; // dependent_slice_segments_enabled_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;pps.dependent_slice_segments_enabled_flag, 1)); // output_flag_present_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;pps.output_flag_present_flag, 1)); // num_extra_slice_header_bits: u(3) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;pps.num_extra_slice_header_bits, 3)); // sign_data_hiding_enabled_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;bits_tmp, 1)); // cabac_init_present_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;pps.cabac_init_present_flag, 1)); // num_ref_idx_l0_default_active_minus1: ue(v) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadExponentialGolomb(&amp;pps.num_ref_idx_l0_default_active_minus1)); // num_ref_idx_l1_default_active_minus1: ue(v) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadExponentialGolomb(&amp;pps.num_ref_idx_l1_default_active_minus1)); // init_qp_minus26: se(v) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadSignedExponentialGolomb(&amp;pps.pic_init_qp_minus26)); // Sanity-check parsed value if (pps.pic_init_qp_minus26 &gt; kMaxPicInitQpDeltaValue || pps.pic_init_qp_minus26 &lt; kMinPicInitQpDeltaValue) { RETURN_EMPTY_ON_FAIL(false); } // constrained_intra_pred_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;bits_tmp, 1)); // transform_skip_enabled_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;bits_tmp, 1)); // cu_qp_delta_enabled_flag: u(1) uint32_t cu_qp_delta_enabled_flag = 0; RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;cu_qp_delta_enabled_flag, 1)); if (cu_qp_delta_enabled_flag) { // diff_cu_qp_delta_depth: ue(v) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); } // pps_cb_qp_offset: se(v) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadSignedExponentialGolomb(&amp;signed_golomb_ignored)); // pps_cr_qp_offset: se(v) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadSignedExponentialGolomb(&amp;signed_golomb_ignored)); // pps_slice_chroma_qp_offsets_present_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;bits_tmp, 1)); // weighted_pred_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;pps.weighted_pred_flag, 1)); // weighted_bipred_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;pps.weighted_bipred_flag, 1)); // transquant_bypass_enabled_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;bits_tmp, 1)); // tiles_enabled_flag: u(1) uint32_t tiles_enabled_flag = 0; RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;tiles_enabled_flag, 1)); // entropy_coding_sync_enabled_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;bits_tmp, 1)); if (tiles_enabled_flag) { // num_tile_columns_minus1: ue(v) uint32_t num_tile_columns_minus1 = 0; RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadExponentialGolomb(&amp;num_tile_columns_minus1)); // num_tile_rows_minus1: ue(v) uint32_t num_tile_rows_minus1 = 0; RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadExponentialGolomb(&amp;num_tile_rows_minus1)); // uniform_spacing_flag: u(1) uint32_t uniform_spacing_flag = 0; RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;uniform_spacing_flag, 1)); if (!uniform_spacing_flag) { for (uint32_t i = 0; i &lt; num_tile_columns_minus1; i++) { // column_width_minus1: ue(v) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); } for (uint32_t i = 0; i &lt; num_tile_rows_minus1; i++) { // row_height_minus1: ue(v) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); } // loop_filter_across_tiles_enabled_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;bits_tmp, 1)); } } // pps_loop_filter_across_slices_enabled_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;bits_tmp, 1)); // deblocking_filter_control_present_flag: u(1) uint32_t deblocking_filter_control_present_flag = 0; RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;deblocking_filter_control_present_flag, 1)); if (deblocking_filter_control_present_flag) { // deblocking_filter_override_enabled_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;bits_tmp, 1)); // pps_deblocking_filter_disabled_flag: u(1) uint32_t pps_deblocking_filter_disabled_flag = 0; RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;pps_deblocking_filter_disabled_flag, 1)); if (!pps_deblocking_filter_disabled_flag) { // pps_beta_offset_div2: se(v) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadSignedExponentialGolomb(&amp;signed_golomb_ignored)); // pps_tc_offset_div2: se(v) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadSignedExponentialGolomb(&amp;signed_golomb_ignored)); } } // pps_scaling_list_data_present_flag: u(1) uint32_t pps_scaling_list_data_present_flag = 0; RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;pps_scaling_list_data_present_flag, 1)); if (pps_scaling_list_data_present_flag) { // scaling_list_data() if (!H265SpsParser::ParseScalingListData(bit_buffer)) { return absl::nullopt; } } // lists_modification_present_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;pps.lists_modification_present_flag, 1)); // log2_parallel_merge_level_minus2: ue(v) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); // slice_segment_header_extension_present_flag: u(1) RETURN_EMPTY_ON_FAIL(bit_buffer-&gt;ReadBits(&amp;bits_tmp, 1)); return pps;}bool H265PpsParser::ParsePpsIdsInternal(rtc::BitBuffer* bit_buffer, uint32_t* pps_id, uint32_t* sps_id) { // pic_parameter_set_id: ue(v) if (!bit_buffer-&gt;ReadExponentialGolomb(pps_id)) return false; // seq_parameter_set_id: ue(v) if (!bit_buffer-&gt;ReadExponentialGolomb(sps_id)) return false; return true;}} // namespace webrtc common_video/h265/h265_pps_parser.h >folded#ifndef COMMON_VIDEO_H265_PPS_PARSER_H_#define COMMON_VIDEO_H265_PPS_PARSER_H_#include &quot;absl/types/optional.h&quot;namespace rtc {class BitBuffer;}namespace webrtc {// A class for parsing out picture parameter set (PPS) data from a H265 NALU.class H265PpsParser { public: // The parsed state of the PPS. Only some select values are stored. // Add more as they are actually needed. struct PpsState { PpsState() = default; uint32_t dependent_slice_segments_enabled_flag = 0; uint32_t cabac_init_present_flag = 0; uint32_t output_flag_present_flag = 0; uint32_t num_extra_slice_header_bits = 0; uint32_t num_ref_idx_l0_default_active_minus1 = 0; uint32_t num_ref_idx_l1_default_active_minus1 = 0; int32_t pic_init_qp_minus26 = 0; uint32_t weighted_pred_flag = 0; uint32_t weighted_bipred_flag = 0; uint32_t lists_modification_present_flag = 0; uint32_t id = 0; uint32_t sps_id = 0; }; // Unpack RBSP and parse PPS state from the supplied buffer. static absl::optional&lt;PpsState&gt; ParsePps(const uint8_t* data, size_t length); static bool ParsePpsIds(const uint8_t* data, size_t length, uint32_t* pps_id, uint32_t* sps_id); static absl::optional&lt;uint32_t&gt; ParsePpsIdFromSliceSegmentLayerRbsp( const uint8_t* data, size_t length, uint8_t nalu_type); protected: // Parse the PPS state, for a bit buffer where RBSP decoding has already been // performed. static absl::optional&lt;PpsState&gt; ParseInternal(rtc::BitBuffer* bit_buffer); static bool ParsePpsIdsInternal(rtc::BitBuffer* bit_buffer, uint32_t* pps_id, uint32_t* sps_id);};} // namespace webrtc#endif // COMMON_VIDEO_H265_PPS_PARSER_H_ common_video/h265/h265_sps_parser.cc >folded#include &lt;memory&gt;#include &lt;vector&gt;#include &quot;common_video/h265/h265_common.h&quot;#include &quot;common_video/h265/h265_sps_parser.h&quot;#include &quot;rtc_base/bit_buffer.h&quot;#include &quot;rtc_base/logging.h&quot;namespace {typedef absl::optional&lt;webrtc::H265SpsParser::SpsState&gt; OptionalSps;typedef absl::optional&lt;webrtc::H265SpsParser::ShortTermRefPicSet&gt; OptionalShortTermRefPicSet;#define RETURN_EMPTY_ON_FAIL(x) \\ if (!(x)) { \\ return OptionalSps(); \\ }#define RETURN_FALSE_ON_FAIL(x) \\ if (!(x)) { \\ return false; \\ }#define RETURN_EMPTY2_ON_FAIL(x) \\ if (!(x)) { \\ return OptionalShortTermRefPicSet(); \\ }} // namespacenamespace webrtc {H265SpsParser::SpsState::SpsState() = default;H265SpsParser::ShortTermRefPicSet::ShortTermRefPicSet() = default;// General note: this is based off the 06/2019 version of the H.265 standard.// You can find it on this page:// http://www.itu.int/rec/T-REC-H.265// Unpack RBSP and parse SPS state from the supplied buffer.absl::optional&lt;H265SpsParser::SpsState&gt; H265SpsParser::ParseSps( const uint8_t* data, size_t length) { std::vector&lt;uint8_t&gt; unpacked_buffer = H265::ParseRbsp(data, length); rtc::BitBuffer bit_buffer(unpacked_buffer.data(), unpacked_buffer.size()); return ParseSpsInternal(&amp;bit_buffer);}bool H265SpsParser::ParseScalingListData(rtc::BitBuffer* buffer) { uint32_t scaling_list_pred_mode_flag[4][6]; uint32_t scaling_list_pred_matrix_id_delta[4][6]; int32_t scaling_list_dc_coef_minus8[4][6]; int32_t scaling_list[4][6][64]; for (int size_id = 0; size_id &lt; 4; size_id++) { for (int matrix_id = 0; matrix_id &lt; 6; matrix_id += (size_id == 3) ? 3 : 1) { // scaling_list_pred_mode_flag: u(1) RETURN_FALSE_ON_FAIL(buffer-&gt;ReadBits(&amp;scaling_list_pred_mode_flag[size_id][matrix_id], 1)); if (!scaling_list_pred_mode_flag[size_id][matrix_id]) { // scaling_list_pred_matrix_id_delta: ue(v) RETURN_FALSE_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;scaling_list_pred_matrix_id_delta[size_id][matrix_id])); } else { int32_t next_coef = 8; uint32_t coef_num = std::min(64, 1 &lt;&lt; (4 + (size_id &lt;&lt; 1))); if (size_id &gt; 1) { // scaling_list_dc_coef_minus8: se(v) RETURN_FALSE_ON_FAIL(buffer-&gt;ReadSignedExponentialGolomb(&amp;scaling_list_dc_coef_minus8[size_id - 2][matrix_id])); next_coef = scaling_list_dc_coef_minus8[size_id - 2][matrix_id]; } for (uint32_t i = 0; i &lt; coef_num; i++) { // scaling_list_delta_coef: se(v) int32_t scaling_list_delta_coef = 0; RETURN_FALSE_ON_FAIL(buffer-&gt;ReadSignedExponentialGolomb(&amp;scaling_list_delta_coef)); next_coef = (next_coef + scaling_list_delta_coef + 256) % 256; scaling_list[size_id][matrix_id][i] = next_coef; } } } } return true;}absl::optional&lt;H265SpsParser::ShortTermRefPicSet&gt; H265SpsParser::ParseShortTermRefPicSet( uint32_t st_rps_idx, uint32_t num_short_term_ref_pic_sets, const std::vector&lt;H265SpsParser::ShortTermRefPicSet&gt;&amp; short_term_ref_pic_set, H265SpsParser::SpsState&amp; sps, rtc::BitBuffer* buffer) { H265SpsParser::ShortTermRefPicSet ref_pic_set; uint32_t inter_ref_pic_set_prediction_flag = 0; if (st_rps_idx != 0) { // inter_ref_pic_set_prediction_flag: u(1) RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadBits(&amp;inter_ref_pic_set_prediction_flag, 1)); } if (inter_ref_pic_set_prediction_flag) { uint32_t delta_idx_minus1 = 0; if (st_rps_idx == num_short_term_ref_pic_sets) { // delta_idx_minus1: ue(v) RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;delta_idx_minus1)); } // delta_rps_sign: u(1) uint32_t delta_rps_sign = 0; RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadBits(&amp;delta_rps_sign, 1)); // abs_delta_rps_minus1: ue(v) uint32_t abs_delta_rps_minus1 = 0; RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;abs_delta_rps_minus1)); uint32_t ref_rps_idx = st_rps_idx - (delta_idx_minus1 + 1); uint32_t num_delta_pocs = 0; if (short_term_ref_pic_set[ref_rps_idx].inter_ref_pic_set_prediction_flag) { auto&amp; used_by_curr_pic_flag = short_term_ref_pic_set[ref_rps_idx].used_by_curr_pic_flag; auto&amp; use_delta_flag = short_term_ref_pic_set[ref_rps_idx].use_delta_flag; if (used_by_curr_pic_flag.size() != use_delta_flag.size()) { return OptionalShortTermRefPicSet(); } for (uint32_t i = 0; i &lt; used_by_curr_pic_flag.size(); i++) { if (used_by_curr_pic_flag[i] || use_delta_flag[i]) { num_delta_pocs++; } } } else { num_delta_pocs = short_term_ref_pic_set[ref_rps_idx].num_negative_pics + short_term_ref_pic_set[ref_rps_idx].num_positive_pics; } ref_pic_set.used_by_curr_pic_flag.resize(num_delta_pocs + 1, 0); ref_pic_set.use_delta_flag.resize(num_delta_pocs + 1, 1); for (uint32_t j = 0; j &lt;= num_delta_pocs; j++) { // used_by_curr_pic_flag: u(1) RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadBits(&amp;ref_pic_set.used_by_curr_pic_flag[j], 1)); if (!ref_pic_set.used_by_curr_pic_flag[j]) { // use_delta_flag: u(1) RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadBits(&amp;ref_pic_set.use_delta_flag[j], 1)); } } } else { // num_negative_pics: ue(v) RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;ref_pic_set.num_negative_pics)); // num_positive_pics: ue(v) RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;ref_pic_set.num_positive_pics)); ref_pic_set.delta_poc_s0_minus1.resize(ref_pic_set.num_negative_pics, 0); ref_pic_set.used_by_curr_pic_s0_flag.resize(ref_pic_set.num_negative_pics, 0); for (uint32_t i = 0; i &lt; ref_pic_set.num_negative_pics; i++) { // delta_poc_s0_minus1: ue(v) RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;ref_pic_set.delta_poc_s0_minus1[i])); // used_by_curr_pic_s0_flag: u(1) RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadBits(&amp;ref_pic_set.used_by_curr_pic_s0_flag[i], 1)); } ref_pic_set.delta_poc_s1_minus1.resize(ref_pic_set.num_positive_pics, 0); ref_pic_set.used_by_curr_pic_s1_flag.resize(ref_pic_set.num_positive_pics, 0); for (uint32_t i = 0; i &lt; ref_pic_set.num_positive_pics; i++) { // delta_poc_s1_minus1: ue(v) RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;ref_pic_set.delta_poc_s1_minus1[i])); // used_by_curr_pic_s1_flag: u(1) RETURN_EMPTY2_ON_FAIL(buffer-&gt;ReadBits(&amp;ref_pic_set.used_by_curr_pic_s1_flag[i], 1)); } } return OptionalShortTermRefPicSet(ref_pic_set);}absl::optional&lt;H265SpsParser::SpsState&gt; H265SpsParser::ParseSpsInternal( rtc::BitBuffer* buffer) { // Now, we need to use a bit buffer to parse through the actual HEVC SPS // format. See Section 7.3.2.2.1 (&quot;General sequence parameter set data // syntax&quot;) of the H.265 standard for a complete description. // Since we only care about resolution, we ignore the majority of fields, but // we still have to actively parse through a lot of the data, since many of // the fields have variable size. // We're particularly interested in: // chroma_format_idc -&gt; affects crop units // pic_{width,height}_* -&gt; resolution of the frame in macroblocks (16x16). // frame_crop_*_offset -&gt; crop information SpsState sps; // The golomb values we have to read, not just consume. uint32_t golomb_ignored; // sps_video_parameter_set_id: u(4) uint32_t sps_video_parameter_set_id = 0; RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;sps_video_parameter_set_id, 4)); // sps_max_sub_layers_minus1: u(3) uint32_t sps_max_sub_layers_minus1 = 0; RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;sps_max_sub_layers_minus1, 3)); sps.sps_max_sub_layers_minus1 = sps_max_sub_layers_minus1; sps.sps_max_dec_pic_buffering_minus1.resize(sps_max_sub_layers_minus1 + 1, 0); // sps_temporal_id_nesting_flag: u(1) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(1)); // profile_tier_level(1, sps_max_sub_layers_minus1). We are acutally not // using them, so read/skip over it. // general_profile_space+general_tier_flag+general_prfile_idc: u(8) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBytes(1)); // general_profile_compatabilitiy_flag[32] RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBytes(4)); // general_progressive_source_flag + interlaced_source_flag+ // non-packed_constraint flag + frame_only_constraint_flag: u(4) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(4)); // general_profile_idc decided flags or reserved. u(43) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(43)); // general_inbld_flag or reserved 0: u(1) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(1)); // general_level_idc: u(8) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBytes(1)); // if max_sub_layers_minus1 &gt;=1, read the sublayer profile information std::vector&lt;uint32_t&gt; sub_layer_profile_present_flags; std::vector&lt;uint32_t&gt; sub_layer_level_present_flags; uint32_t sub_layer_profile_present = 0; uint32_t sub_layer_level_present = 0; for (uint32_t i = 0; i &lt; sps_max_sub_layers_minus1; i++) { // sublayer_profile_present_flag and sublayer_level_presnet_flag: u(2) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;sub_layer_profile_present, 1)); RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;sub_layer_level_present, 1)); sub_layer_profile_present_flags.push_back(sub_layer_profile_present); sub_layer_level_present_flags.push_back(sub_layer_level_present); } if (sps_max_sub_layers_minus1 &gt; 0) { for (uint32_t j = sps_max_sub_layers_minus1; j &lt; 8; j++) { // reserved 2 bits: u(2) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(2)); } } for (uint32_t k = 0; k &lt; sps_max_sub_layers_minus1; k++) { if (sub_layer_profile_present_flags[k]) { // // sub_layer profile_space/tier_flag/profile_idc. ignored. u(8) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBytes(1)); // profile_compatability_flag: u(32) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBytes(4)); // sub_layer progressive_source_flag/interlaced_source_flag/ // non_packed_constraint_flag/frame_only_constraint_flag: u(4) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(4)); // following 43-bits are profile_idc specific. We simply read/skip it. // u(43) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(43)); // 1-bit profile_idc specific inbld flag. We simply read/skip it. u(1) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(1)); } if (sub_layer_level_present_flags[k]) { // sub_layer_level_idc: u(8) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBytes(1)); } } // sps_seq_parameter_set_id: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;sps.id)); // chrome_format_idc: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;sps.chroma_format_idc)); if (sps.chroma_format_idc == 3) { // seperate_colour_plane_flag: u(1) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;sps.separate_colour_plane_flag, 1)); } uint32_t pic_width_in_luma_samples = 0; uint32_t pic_height_in_luma_samples = 0; // pic_width_in_luma_samples: ue(v) RETURN_EMPTY_ON_FAIL( buffer-&gt;ReadExponentialGolomb(&amp;pic_width_in_luma_samples)); // pic_height_in_luma_samples: ue(v) RETURN_EMPTY_ON_FAIL( buffer-&gt;ReadExponentialGolomb(&amp;pic_height_in_luma_samples)); // conformance_window_flag: u(1) uint32_t conformance_window_flag = 0; RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;conformance_window_flag, 1)); uint32_t conf_win_left_offset = 0; uint32_t conf_win_right_offset = 0; uint32_t conf_win_top_offset = 0; uint32_t conf_win_bottom_offset = 0; if (conformance_window_flag) { // conf_win_left_offset: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;conf_win_left_offset)); // conf_win_right_offset: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;conf_win_right_offset)); // conf_win_top_offset: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;conf_win_top_offset)); // conf_win_bottom_offset: ue(v) RETURN_EMPTY_ON_FAIL( buffer-&gt;ReadExponentialGolomb(&amp;conf_win_bottom_offset)); } // bit_depth_luma_minus8: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); // bit_depth_chroma_minus8: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); // log2_max_pic_order_cnt_lsb_minus4: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;sps.log2_max_pic_order_cnt_lsb_minus4)); uint32_t sps_sub_layer_ordering_info_present_flag = 0; // sps_sub_layer_ordering_info_present_flag: u(1) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;sps_sub_layer_ordering_info_present_flag, 1)); for (uint32_t i = (sps_sub_layer_ordering_info_present_flag != 0) ? 0 : sps_max_sub_layers_minus1; i &lt;= sps_max_sub_layers_minus1; i++) { // sps_max_dec_pic_buffering_minus1: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;sps.sps_max_dec_pic_buffering_minus1[i])); // sps_max_num_reorder_pics: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); // sps_max_latency_increase_plus1: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); } // log2_min_luma_coding_block_size_minus3: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;sps.log2_min_luma_coding_block_size_minus3)); // log2_diff_max_min_luma_coding_block_size: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;sps.log2_diff_max_min_luma_coding_block_size)); // log2_min_luma_transform_block_size_minus2: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); // log2_diff_max_min_luma_transform_block_size: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); // max_transform_hierarchy_depth_inter: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); // max_transform_hierarchy_depth_intra: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); // scaling_list_enabled_flag: u(1) uint32_t scaling_list_enabled_flag = 0; RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;scaling_list_enabled_flag, 1)); if (scaling_list_enabled_flag) { // sps_scaling_list_data_present_flag: u(1) uint32_t sps_scaling_list_data_present_flag = 0; RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;sps_scaling_list_data_present_flag, 1)); if (sps_scaling_list_data_present_flag) { // scaling_list_data() if (!ParseScalingListData(buffer)) { return OptionalSps(); } } } // amp_enabled_flag: u(1) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(1)); // sample_adaptive_offset_enabled_flag: u(1) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;sps.sample_adaptive_offset_enabled_flag, 1)); // pcm_enabled_flag: u(1) uint32_t pcm_enabled_flag = 0; RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;pcm_enabled_flag, 1)); if (pcm_enabled_flag) { // pcm_sample_bit_depth_luma_minus1: u(4) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(4)); // pcm_sample_bit_depth_chroma_minus1: u(4) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(4)); // log2_min_pcm_luma_coding_block_size_minus3: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); // log2_diff_max_min_pcm_luma_coding_block_size: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;golomb_ignored)); // pcm_loop_filter_disabled_flag: u(1) RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(1)); } // num_short_term_ref_pic_sets: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;sps.num_short_term_ref_pic_sets)); sps.short_term_ref_pic_set.resize(sps.num_short_term_ref_pic_sets); for (uint32_t st_rps_idx = 0; st_rps_idx &lt; sps.num_short_term_ref_pic_sets; st_rps_idx++) { // st_ref_pic_set() OptionalShortTermRefPicSet ref_pic_set = ParseShortTermRefPicSet( st_rps_idx, sps.num_short_term_ref_pic_sets, sps.short_term_ref_pic_set, sps, buffer); if (ref_pic_set) { sps.short_term_ref_pic_set[st_rps_idx] = *ref_pic_set; } else { return OptionalSps(); } } // long_term_ref_pics_present_flag: u(1) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;sps.long_term_ref_pics_present_flag, 1)); if (sps.long_term_ref_pics_present_flag) { // num_long_term_ref_pics_sps: ue(v) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadExponentialGolomb(&amp;sps.num_long_term_ref_pics_sps)); sps.used_by_curr_pic_lt_sps_flag.resize(sps.num_long_term_ref_pics_sps, 0); for (uint32_t i = 0; i &lt; sps.num_long_term_ref_pics_sps; i++) { // lt_ref_pic_poc_lsb_sps: u(v) uint32_t lt_ref_pic_poc_lsb_sps_bits = sps.log2_max_pic_order_cnt_lsb_minus4 + 4; RETURN_EMPTY_ON_FAIL(buffer-&gt;ConsumeBits(lt_ref_pic_poc_lsb_sps_bits)); // used_by_curr_pic_lt_sps_flag: u(1) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;sps.used_by_curr_pic_lt_sps_flag[i], 1)); } } // sps_temporal_mvp_enabled_flag: u(1) RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;sps.sps_temporal_mvp_enabled_flag, 1)); // Far enough! We don't use the rest of the SPS. sps.vps_id = sps_video_parameter_set_id; sps.pic_width_in_luma_samples = pic_width_in_luma_samples; sps.pic_height_in_luma_samples = pic_height_in_luma_samples; // Start with the resolution determined by the pic_width/pic_height fields. sps.width = pic_width_in_luma_samples; sps.height = pic_height_in_luma_samples; if (conformance_window_flag) { int sub_width_c = ((1 == sps.chroma_format_idc) || (2 == sps.chroma_format_idc)) &amp;&amp; (0 == sps.separate_colour_plane_flag) ? 2 : 1; int sub_height_c = (1 == sps.chroma_format_idc) &amp;&amp; (0 == sps.separate_colour_plane_flag) ? 2 : 1; // the offset includes the pixel within conformance window. so don't need to // +1 as per spec sps.width -= sub_width_c * (conf_win_right_offset + conf_win_left_offset); sps.height -= sub_height_c * (conf_win_top_offset + conf_win_bottom_offset); } return OptionalSps(sps);}} // namespace webrtc common_video/h265/h265_sps_parser.h >folded#ifndef COMMON_VIDEO_H265_H265_SPS_PARSER_H_#define COMMON_VIDEO_H265_H265_SPS_PARSER_H_#include &lt;vector&gt;#include &quot;absl/types/optional.h&quot;namespace rtc {class BitBuffer;}namespace webrtc {// A class for parsing out sequence parameter set (SPS) data from an H265 NALU.class H265SpsParser { public: struct ShortTermRefPicSet { ShortTermRefPicSet(); uint32_t inter_ref_pic_set_prediction_flag = 0; std::vector&lt;uint32_t&gt; used_by_curr_pic_flag; std::vector&lt;uint32_t&gt; use_delta_flag; uint32_t num_negative_pics = 0; uint32_t num_positive_pics = 0; std::vector&lt;uint32_t&gt; delta_poc_s0_minus1; std::vector&lt;uint32_t&gt; used_by_curr_pic_s0_flag; std::vector&lt;uint32_t&gt; delta_poc_s1_minus1; std::vector&lt;uint32_t&gt; used_by_curr_pic_s1_flag; }; // The parsed state of the SPS. Only some select values are stored. // Add more as they are actually needed. struct SpsState { SpsState(); uint32_t sps_max_sub_layers_minus1; uint32_t chroma_format_idc = 0; uint32_t separate_colour_plane_flag = 0; uint32_t pic_width_in_luma_samples = 0; uint32_t pic_height_in_luma_samples = 0; uint32_t log2_max_pic_order_cnt_lsb_minus4 = 0; std::vector&lt;uint32_t&gt; sps_max_dec_pic_buffering_minus1; uint32_t log2_min_luma_coding_block_size_minus3 = 0; uint32_t log2_diff_max_min_luma_coding_block_size = 0; uint32_t sample_adaptive_offset_enabled_flag = 0; uint32_t num_short_term_ref_pic_sets = 0; std::vector&lt;H265SpsParser::ShortTermRefPicSet&gt; short_term_ref_pic_set; uint32_t long_term_ref_pics_present_flag = 0; uint32_t num_long_term_ref_pics_sps = 0; std::vector&lt;uint32_t&gt; used_by_curr_pic_lt_sps_flag; uint32_t sps_temporal_mvp_enabled_flag = 0; uint32_t width = 0; uint32_t height = 0; uint32_t id = 0; uint32_t vps_id = 0; }; // Unpack RBSP and parse SPS state from the supplied buffer. static absl::optional&lt;SpsState&gt; ParseSps(const uint8_t* data, size_t length); static bool ParseScalingListData(rtc::BitBuffer* buffer); static absl::optional&lt;ShortTermRefPicSet&gt; ParseShortTermRefPicSet( uint32_t st_rps_idx, uint32_t num_short_term_ref_pic_sets, const std::vector&lt;ShortTermRefPicSet&gt;&amp; ref_pic_sets, SpsState&amp; sps, rtc::BitBuffer* buffer); protected: // Parse the SPS state, for a bit buffer where RBSP decoding has already been // performed. static absl::optional&lt;SpsState&gt; ParseSpsInternal(rtc::BitBuffer* buffer);};} // namespace webrtc#endif // COMMON_VIDEO_H265_H265_SPS_PARSER_H_ common_video/h265/h265_vps_parser.cc >folded#include &lt;memory&gt;#include &lt;vector&gt;#include &quot;common_video/h265/h265_common.h&quot;#include &quot;common_video/h265/h265_vps_parser.h&quot;#include &quot;rtc_base/bit_buffer.h&quot;#include &quot;rtc_base/logging.h&quot;namespace {typedef absl::optional&lt;webrtc::H265VpsParser::VpsState&gt; OptionalVps;#define RETURN_EMPTY_ON_FAIL(x) \\ if (!(x)) { \\ return OptionalVps(); \\ }} // namespacenamespace webrtc {H265VpsParser::VpsState::VpsState() = default;// General note: this is based off the 06/2019 version of the H.265 standard.// You can find it on this page:// http://www.itu.int/rec/T-REC-H.265// Unpack RBSP and parse SPS state from the supplied buffer.absl::optional&lt;H265VpsParser::VpsState&gt; H265VpsParser::ParseVps( const uint8_t* data, size_t length) { std::vector&lt;uint8_t&gt; unpacked_buffer = H265::ParseRbsp(data, length); rtc::BitBuffer bit_buffer(unpacked_buffer.data(), unpacked_buffer.size()); return ParseInternal(&amp;bit_buffer);}absl::optional&lt;H265VpsParser::VpsState&gt; H265VpsParser::ParseInternal( rtc::BitBuffer* buffer) { // Now, we need to use a bit buffer to parse through the actual HEVC VPS // format. See Section 7.3.2.1 (&quot;Video parameter set RBSP syntax&quot;) of the // H.265 standard for a complete description. VpsState vps; // vps_video_parameter_set_id: u(4) vps.id = 0; RETURN_EMPTY_ON_FAIL(buffer-&gt;ReadBits(&amp;vps.id, 4)); return OptionalVps(vps);}} // namespace webrtc common_video/h265/h265_vps_parser.h >folded#ifndef COMMON_VIDEO_H265_H265_VPS_PARSER_H_#define COMMON_VIDEO_H265_H265_VPS_PARSER_H_#include &quot;absl/types/optional.h&quot;namespace rtc {class BitBuffer;}namespace webrtc {// A class for parsing out sequence parameter set (VPS) data from an H265 NALU.class H265VpsParser { public: // The parsed state of the VPS. Only some select values are stored. // Add more as they are actually needed. struct VpsState { VpsState(); uint32_t id = 0; }; // Unpack RBSP and parse VPS state from the supplied buffer. static absl::optional&lt;VpsState&gt; ParseVps(const uint8_t* data, size_t length); protected: // Parse the VPS state, for a bit buffer where RBSP decoding has already been // performed. static absl::optional&lt;VpsState&gt; ParseInternal(rtc::BitBuffer* bit_buffer);};} // namespace webrtc#endif // COMMON_VIDEO_H265_H265_VPS_PARSER_H_ 将新增文件添加到ninja中参与构建： if (rtc_use_h265) { sources += [ &quot;h265/h265_bitstream_parser.cc&quot;, &quot;h265/h265_bitstream_parser.h&quot;, &quot;h265/h265_common.cc&quot;, &quot;h265/h265_common.h&quot;, &quot;h265/h265_pps_parser.cc&quot;, &quot;h265/h265_pps_parser.h&quot;, &quot;h265/h265_sps_parser.cc&quot;, &quot;h265/h265_sps_parser.h&quot;, &quot;h265/h265_vps_parser.cc&quot;, &quot;h265/h265_vps_parser.h&quot;, ]} 部分配置修改 log配置： logging/rtc_event_log/encoder/rtc_event_log_encoder_new_format.ccrtclog2::FrameDecodedEvents::Codec ConvertToProtoFormat(VideoCodecType codec) { switch (codec) { ... case VideoCodecType::kVideoCodecH264: return rtclog2::FrameDecodedEvents::CODEC_H264;#ifndef DISABLE_H265 case VideoCodecType::kVideoCodecH265: return rtclog2::FrameDecodedEvents::CODEC_H265;#endif case VideoCodecType::kVideoCodecMultiplex: // This codec type is afaik not used. return rtclog2::FrameDecodedEvents::CODEC_UNKNOWN; } RTC_NOTREACHED(); return rtclog2::FrameDecodedEvents::CODEC_UNKNOWN;} logging/rtc_event_log/rtc_event_log2.protomessage FrameDecodedEvents { enum Codec { CODEC_UNKNOWN = 0; CODEC_GENERIC = 1; CODEC_VP8 = 2; CODEC_VP9 = 3; CODEC_AV1 = 4; CODEC_H264 = 5; CODEC_H265 = 6; } ...} 相关常量 media/base/media_constants.ccconst char kHEVCCodecName[] = &quot;H265X&quot;;#ifndef DISABLE_H265const char kH265CodecName[] = &quot;H265&quot;;#endif// RFC 6184 RTP Payload Format for H.264 videoconst char kH264FmtpProfileLevelId[] = &quot;profile-level-id&quot;;const char kH264FmtpLevelAsymmetryAllowed[] = &quot;level-asymmetry-allowed&quot;;const char kH264FmtpPacketizationMode[] = &quot;packetization-mode&quot;;const char kH264FmtpSpropParameterSets[] = &quot;sprop-parameter-sets&quot;;const char kH264FmtpSpsPpsIdrInKeyframe[] = &quot;sps-pps-idr-in-keyframe&quot;;const char kH264ProfileLevelConstrainedBaseline[] = &quot;42e01f&quot;;const char kH264ProfileLevelConstrainedHigh[] = &quot;640c1f&quot;;#ifndef DISABLE_H265// RFC 7798 RTP Payload Format for H.265 videoconst char kH265FmtpProfileSpace[] = &quot;profile-space&quot;;const char kH265FmtpProfileId[] = &quot;profile-id&quot;;const char kH265FmtpTierFlag[] = &quot;tier-flag&quot;;const char kH265FmtpLevelId[] = &quot;level-id&quot;;#endif media/base/media_constants.hRTC_EXPORT extern const char kHEVCCodecName[];#ifndef DISABLE_H265RTC_EXPORT extern const char kH265CodecName[];#endif// RFC 6184 RTP Payload Format for H.264 videoRTC_EXPORT extern const char kH264FmtpProfileLevelId[];RTC_EXPORT extern const char kH264FmtpLevelAsymmetryAllowed[];RTC_EXPORT extern const char kH264FmtpPacketizationMode[];extern const char kH264FmtpSpropParameterSets[];extern const char kH264FmtpSpsPpsIdrInKeyframe[];extern const char kH264ProfileLevelConstrainedBaseline[];extern const char kH264ProfileLevelConstrainedHigh[];#ifndef DISABLE_H265// RFC 7798 RTP Payload Format for H.265 videoRTC_EXPORT extern const char kH265FmtpProfileSpace[];RTC_EXPORT extern const char kH265FmtpProfileId[];RTC_EXPORT extern const char kH265FmtpTierFlag[];RTC_EXPORT extern const char kH265FmtpLevelId[];#endifextern const int kDefaultVideoMaxFramerate; 最后打开H265开关BUILD.gnconfig(&quot;common_inherited_config&quot;) {... if (!rtc_use_h265) { defines += [ &quot;DISABLE_H265&quot; ] }} build_overrides/build.gniif (is_win || is_ios || is_android) { rtc_use_h265 = true} else { rtc_use_h265 = false} end漫长的修改后，WebRTC将支持H265，笔者在测试H265与H264的区别时，发现JitterBufferCache 减少140ms，果然是更复杂的算法、更高的压缩率。 参考： owt-deps-webrtc","link":"/2022/09/29/Android%E5%B9%B3%E5%8F%B0WebRTC%E5%BC%80%E5%90%AFH265%E7%BC%96%E8%A7%A3%E7%A0%81/"},{"title":"Mixin","text":"对Dart中Mixin的理解 Tip当然多继承会有很多不足之处，例如，结构复杂化，有限顺序模糊，功能冲突等问题，举一个列子： 一个物体的本质只能有一个，一个动物只能是狗或者只能是猫，如果你想创造一个会玩毛线球会玩激光的狗，那么只需创造一个描述这类行为的接口，然后在自己的类里面实现”玩耍”接口，具体实现这些玩的行为，最终你同样会得到一个既像狗又像猫的动物。如果你想让这个动物叫起来像猫而不是狗，那么重写即可，子类里重新定义“叫”这个行为即可。但无论如何，这样得到的类是绝对不会有多重继承的冲突的。 对于Mixin并没有一个准确的概念，有人理解为Mix in混入。它类似于多继承，但通常混入Mixin的类和Mixin类本身斌那个不是is-a的关系。实质上Mixin是通过语言特性，来更简洁地实现组合模式。因此，Mixin可以灵活地添加某些功能。传统的接口概念中，并不包含实现部分，而Mixin包含实现。 基本概念 Mixins in Strongtalk建议了解一下Mixin学术文献资料，因为它定义了重要概念和注解。 注：Smalltalk被公认历史上第二个面向对象的程序设计语言和第一个真正的集成开发环境（IDE）。并且它对其它众多的程序设计语言的产生起到了极大的推动作用，主要有：Objective-c,Actor,Java和Ruby等，90年代的许多软件开发思想得利于Smalltalk，例如Design Patterns,Extreme Programming(XP)和Refactoring等。Strongtalk的最独特之处是支持渐进式的类型注解，这种思想在Dart,PHP,Python 3和TypeScript等语言中都有体现。Gilad Bracha是Dart开发团队的一员，在20世纪90年代，Gilad同Urs Hölzle和Lars Bak等人一起创建了语言Smalltalk的一个高性能版本即Strongtalk。但随着Java的流行，Sun停止了Strongtalk的投入，并将团队成员重新分配来优化Java的性能，而Strongtalk演变成了官方JVM即Hotspot. – 维基百科 作为一个支持类和继承的语言，类隐式地定义了Mixin。Mixin隐式地通过类主体（Class Body）进行定义，并建立子类和父类之间的变量增量（Delta）。而Class类实际上则是一个Mixin应用，即是通过隐式定义的Mixin应用于父类的结果。 Mixin Application混合应用类似于Function Application函数应用。在数学中，混合类M可以视作从父类到子类新增的一个功能，将M注入超类S，并且返回一个S的子类。在研究文献中，这通常写作 M |&gt; S。 基于函数应用的概念，可以定义复合函数（即函数组合）。该概念贯穿于混合组合中。我们定义混合M1和M2的组合，写作M1*M2,如(M1 * M2) |&gt; S = M1 |&gt; (M2 |&gt; S)。 函数非常有用，因为他们可以应用于不同的参数。同样，Class隐式定义的Mixin，通常仅在类声明给出的父类中应用一次。为允许Mixin应用于不同的父类，我们要么声明Mixin不依赖于特定的父类，要么脱离于Class隐式的Mixin，然后重用外部的原始定义。 语法和语义 Mixin通过正常的类声明被隐式定义。原则上，每个类都定义了一个Mixin，并可以从类中提取出来。然而，Mixin只能从未定义构造函数的类中提取。由于沿着继承链传递构造函数参数的需要，该约束能避免出现新的连锁问题。 举一个🌰: abstract class Collection&lt;E&gt; { Collection&lt;E&gt; newInstance(); void forEach(void f(E element)) { //body } void add(E element) { //body } //...}abstract class DOMElementList&lt;E&gt; = DOMList with Collection&lt;E&gt;;abstract class DOMElementSet&lt;E&gt; = DOMSet with Collection&lt;E&gt;; Collection是一个标准类，并用来声明一个Mixin。另外，DOMElementList和DOMElementSet也是混合应用。这两个Mixin在类声明的时候，通过特殊的形式来定义。在类声明中包含一个名字，并用with语句来声明他们与父类中的混合应用相同。**Collection是抽象类，因为它并没有实现类中年定义的抽象方法newInstance()**。 上述中，事实上DOMElementList混合Collection mixin |&gt; DOMList，而DOMElementSet则是Collection mixin |&gt; DOMSet。 这样做的好处是，Collection中的代码可以在类的多个继承层次中被共享。因此，无论DOMList还是DOMSet，都不需要重复、复制Collection中的代码，并且任何变化都会使Collection传递到这两个继承结构中，大大简化了维护代码。上面的代码介绍了Mixin应用的一种方式：混合应用指定应用的Mixin和父类，以及混合应用的名称。 另外一种情况，混合应用出现在类声明的with语句中，以逗号来分隔标识符列表的时候。此时，所有的标识符代表Class。在这种情况下，多混合在extends语句中可以构成及应用于父类名称，生成一个匿名的父类。再以同样的🌰： class DOMElementList&lt;E&gt; extends DOMList with Collection&lt;E&gt; { DOMElementList&lt;E&gt; newInstance() =&gt; new DOMElementList&lt;E&gt;();}class DOMElementList&lt;E&gt; extends DOMSet with Collection&lt;E&gt; { DOMElementSet&lt;E&gt; newInstance() =&gt; new DOMElementSet&lt;E&gt;();} 这里，DOMElementList并不是应用Collection mixin |&gt; DOMList。相反，它是一个父类为应用的新定义的类（注意extends关键字），DOMElementSet同样如此。注意，在每一种情况下，抽象函数newInstance()必须单独实现，以便能够直接被实例化。 想象一下，如果DOMList有一个带参数的构造函数： class DOMElementList&lt;E&gt; extends DOMList with Collection&lt;E&gt; { DOMElementList&lt;E&gt; newInstance() =&gt; new DOMElementList&lt;E&gt;(0); DOMElementList(size):super(size);} 构造函数可以为各个字段以及泛型参数设置值。每个Mixin都有一个自定义的构造函数被单独调用，父类也是如此。因为Mixin的构造函数不能够被声明，所以调用函数可以省略。在底层的实现中，调用总是放在初始列表之前。 第二种是以方便实用的语法糖形式，将多个Mixin混入类中，而不需要引入多个中间声明。举一个🌰： class Person { String name; Person(this.name);}class Maestro extends Person with Musical, Aggressive, Demented { Maestro(name):super(name);} 这里，父类是一个混合应用：Demented mixin |&gt; Aggressive mixin |&gt; Musical mixin |&gt; Person 假设Person有带参数的构造函数，则Musical mixin |&gt; Person将继承Person的构造函数。以此类推，一直到Maestro实际的父类Person，它由一系列的Mixin应用组成。 细节描述 Privacy私有 一个混合应用很可能是咋外部最初声明Class的库中被声明，这对访问混合应用的成员没有任何影响。根据混合应用的语义可知，访问成员取决于库最初声明的位置。这与普通的继承一样，是由底层语法（C++）的继承语义决定的。 Statics静态 是否可以通过混合应用使用最初Class的静态值？同样，由继承的语义进行分析，在Dart中静态成员不会被继承。 Types类型 混合应用的实例是什么类型？通常，它是父类的子类型，以及通过Mixin名称表示的子类型。换句话说，它与最初Class的类型相同。 最初的Class有它自身的父类。为确保特定的混合应用与最初进行混入的Class兼容，需要使用with语句。例如，如果通过with语句定义了Class A，并应用了一个混合M，M源自Class K，那么A必须支持K定义的接口。","link":"/2019/07/25/Mixin/"},{"title":"Task :app:compileDebugJavaWithJavac Failed处理","text":"今天遇到了Task :app:compileDebugJavaWithJavac FAILED问题，网上答案很杂，解决以后总结一下。 该问题基本由于gradle编译导致，可以用 $ gradlew compileDebug --stacktrace 来跟踪gradle编译时的stacktrace信息，如果出现-bash: ./gradlew: Permission denied错误，可以输入 $ chmod +x gradlew 来解决权限问题，不过gradlew comileDebug 并不能看到有价值的信息，根据提示，输入 $ gradlew compileDebugSources --stacktrace -info 解决问题，原来是某个.java文件没push导致。","link":"/2018/05/09/Task-app-compileDebugJavaWithJavac-Failed%E5%A4%84%E7%90%86/"},{"title":"WindowManager窗口管理","text":"WindowManager相关属性汇总 WindowManager窗口管理WindowManager是android的窗口管理器，所有显示窗口都可以通过它来控制 WindowManager.addView(view,layoutParams) 添加一个view到窗口中 WindowManager.updateView(view,layoutParams) 更新窗口中的view属性 WindowManager.removeView(view) 删除窗口中的View WindowManager.LayoutParams获取布局参数 WindowManager.LayoutParams params = getWindow().getAttributes(); 创建布局参数 WindowManager.LayoutParams params = new WindowManager.LayoutParams(); WindowManager.LayoutParams.type用于确定窗口在屏幕上的显示层次 FIRST_APPLICATION_WINDOW 普通应用的第一个窗口 TYPE_BASE_APPLICATION 作为所有应用基础的窗口, 其他应用窗口都在其上 TYPE_APPLICATION 普通应用窗口. token必须为Activity的token, 指明该窗口属于谁 TYPE_APPLICATION_STARTING 应用启动时显示的窗口. 用于系统在应用能够显示之前显示一些东西 LAST_APPLICATION_WINDOW 应用最后一种窗口类型 FIRST_SUB_WINDOW 子窗口 TYPE_APPLICATION_PANEL 在应用窗口之上的面板窗口, 出现在所依附的窗口之上 TYPE_APPLICATION_MEDIA 显示媒体(如视频)的窗口, 在他们依附的窗口之下显示 TYPE_APPLICATION_SUB_PANEL 应用窗口之上的子面板窗口, 显示在所依附的窗口和其他面板之上 TYPE_APPLICATION_ATTACHED_DIALOG 类似TYPE_APPLICATION_PANEL, 但会作为顶层窗口, 而不是容器的子窗口 TYPE_APPLICATION_MEDIA_OVERLAY 隐藏在媒体窗口上显示覆盖层的窗口,显示在TYPE_APPLICATION_MEDIA和应用窗口之间 TYPE_APPLICATION_ABOVE_SUB_PANEL 一个子面板窗口, 在应用窗口和子面板窗口之上 LAST_SUB_WINDOW 最后一个子窗口 FIRST_SYSTEM_WINDOW 第一个系统窗口 TYPE_STATUS_BAR 状态栏,只能有一个状态栏窗口. 放置在屏幕上方, 其他所有窗口都在其之下 TYPE_SEARCH_BAR 搜索条,只能有一个搜索条窗口, 放置在屏幕顶层 TYPE_PHONE 电话窗口,这是非应用窗口, 用于来电的界面,该窗口通常置于所有应用之上, 但在状态栏下 TYPE_SYSTEM_ALERT 系统窗口, 例如低电量警告弹窗, 在应用窗口之上 TYPE_KEYGUARD 锁屏窗口 TYPE_TOAST 透明通知. 不会拦截触摸事件, 可以向下透传 TYPE_SYSTEM_OVERLAY 系统覆盖窗口, 在所有东西之上. 该窗口必须禁止获取输入焦点, 否则会变成锁屏 TYPE_PRIORITY_PHONE 优先级电话, 即使锁屏也会显示,该窗口必须禁止获取输入焦点, 否则会变成锁屏 TYPE_SYSTEM_DIALOG 状态栏拉出的面板 TYPE_KEYGUARD_DIALOG 锁屏 TYPE_SYSTEM_ERROR 系统错误窗口, 在所有内容之上 TYPE_INPUT_METHOD 输入法窗口, 在普通UI之上，可以缩放 TYPE_INPUT_METHOD_DIALOG 输入法对话框窗口, 在当前输入法窗口之上 TYPE_WALLPAPER 壁纸窗口, 在任意窗口之下, 壁纸之上 TYPE_STATUS_BAR_PANEL 状态栏拉出的面板 TYPE_SECURE_SYSTEM_OVERLAY 安全的系统覆盖窗口, 在所有内容之上，必须禁止获取输入焦点, 否则会变成锁屏，同TYPE_SYSTEM_OVERLAY类似, 区别是只允许系统创建这种覆盖层,，应用无法创建 TYPE_DRAG 拖拽窗口，最多有一个, 在所有窗口之上 TYPE_STATUS_BAR_SUB_PANEL 状态栏拉出的面板, 在状态栏之下 TYPE_POINTER 鼠标指针 TYPE_NAVIGATION_BAR 导航条 TYPE_VOLUME_OVERLAY 调整音量时显示的音量窗口 TYPE_BOOT_PROGRESS 隐藏，启动进度对话框, 在全局任何事物之上 TYPE_INPUT_CONSUMER 消费输入事件的窗口 TYPE_DREAM 隐藏，屏保窗口, 在锁屏之上 TYPE_NAVIGATION_BAR_PANEL 导航条面板 TYPE_DISPLAY_OVERLAY 显示覆盖窗口, 用于模拟第二个显示设备 TYPE_MAGNIFICATION_OVERLAY 放大覆盖窗口，用于突出放大的部分 TYPE_KEYGUARD_SCRIM 隐藏，锁屏scrim窗口, 当锁屏需要重启时显示 TYPE_PRIVATE_PRESENTATION Presentation窗口 TYPE_VOICE_INTERACTION 隐藏，语音互动窗口 TYPE_ACCESSIBILITY_OVERLAY 辅助功能覆盖层 TYPE_VOICE_INTERACTION_STARTING 隐藏，语音互动开始窗口 TYPE_DOCK_DIVIDER 隐藏，托盘窗口, 仅系统进程拥有 TYPE_QS_DIALOG 类似TYPE_APPLICATION_ATTACHED_DIALOG, 但用于快速设置 TYPE_SCREENSHOT 隐藏，同TYPE_DREAM类似, 但用于截屏 WindowManager.LayoutParams.flags用于确定窗口的行为 FLAG_ALLOW_LOCK_WHILE_SCREEN_ON 当窗口对用户可见时, 允许锁屏.可以单独使用, 也可以和FLAG_KEEP_SCREEN_ON和FLAG_SHOW_WHEN_LOCKED结合使用 FLAG_DIM_BEHIND 该窗口以下的内容都会变暗,可以使用dimAmount来控制变暗的程度 FLAG_BLUR_BEHIND 失效了, 不再支持,该窗口以下的内容都会模糊 FLAG_NOT_FOCUSABLE 该窗口不可获取按键输入焦点, 因此用户无法向其发送按键或按钮事件. 这些事件会被窗口以下的控件获取.,该flag同时会启动FLAG_NOT_TOUCH_MODAL,无论你是否显式的设置,设置该flag同时暗示着,该窗口不再需要同输入法交互,因此该窗口和输入法窗口会以Z轴方式叠放一般该窗口会覆盖在输入法窗口之上),可以使用FLAG_ALT_FOCUSABLE_IM来修改这个行为 FLAG_NOT_TOUCHABLE 该窗口不可接收任何触摸事件 FLAG_NOT_TOUCH_MODAL 允许任何在该窗口之外的触摸事件传递到该窗口以下的控件, 即使该窗口是focusable的(即没有设置FLAG_NOT_FOCUSABLE).否则该窗口会消费所有的触摸事件, 无论触摸是否在窗口之内 FLAG_TOUCHABLE_WHEN_WAKING 已经过时, 现在没有任何效果,如果设备处于睡眠中, 此时第一次点击屏幕的事件将会被该窗口接收,通常第一次触摸事件会被系统消费,因为用户无法看见他们点击的是什么 FLAG_KEEP_SCREEN_ON 当窗口对于用户可见时, 保持设备屏幕常亮 FLAG_LAYOUT_IN_SCREEN 将窗口放置在整个屏幕中,忽略状态栏等周边装饰边框.,窗口内容必须定位正确,才能获取到装饰边框信息 FLAG_LAYOUT_NO_LIMITS 允许窗口扩展到屏幕之外 FLAG_FULLSCREEN 当该窗口显示时, 隐藏所有屏幕装饰(如状态栏),允许窗口使用整个屏幕,当带有该flag的窗口是顶层窗口时, 状态栏会被隐藏,全屏窗口会忽略SOFT_INPUT_ADJUST_RESIZE对于softInputMode的值,窗口会一直保持全屏, 且不能缩放可以通过theme属性来控制, 如Theme_Black_NoTitleBar_Fullscreen等 FLAG_FORCE_NOT_FULLSCREEN 覆盖FLAG_FULLSCREEN, 并强制显示屏幕装饰(如状态栏) FLAG_DITHER 过时, 不再使用,开启图像抖动 FLAG_SECURE 将窗口内容作为安全内容, 阻止窗口出现在截屏, 或是被不安全的显示器显示 FLAG_SCALED 可以根据布局参数进行拉伸 FLAG_IGNORE_CHEEK_PRESSES 用于在用户将屏幕贴近脸部时, 防止误按 FLAG_LAYOUT_INSET_DECOR 仅同FLAG_LAYOUT_IN_SCREEN一起使用.窗口可能出现在装饰下面(如状态栏下面),使用这个flag后, 窗口会确保不会被装饰物覆盖 FLAG_ALT_FOCUSABLE_IM 反转FLAG_NOT_FOCUSABLE的交互状态.即, 如果同时设置了本flag和FLAG_NOT_FOCUSABLE,则窗口表现为需要同输入法交互,同时会被至于输入法之下,如果设置了本flag而没有设置FLAG_NOT_FOCUSABLE, 则窗口表现为不需要同输入法交互, 同时会被至于输入法之上 FLAG_WATCH_OUTSIDE_TOUCH 如果设置了FLAG_NOT_TOUCH_MODAL,那么可以同时设置此flag来接收窗口之外发生的MotionEvent.ACTION_OUTSIDE事件.注意, 你不会接收到完整的down/move/up手势,只会接收到按下位置的ACTION_OUTSIDE事件 FLAG_SHOW_WHEN_LOCKED 当锁屏时, 允许窗口显示,窗口优先于锁屏,可以同FLAG_KEEP_SCREEN_ON一起使用,来保持屏幕常亮并在显示锁屏之前显示该窗口,可以同FLAG_DISMISS_KEYGUARD一起使用,来取消非安全的锁屏,该flag只能应用于最顶层的全屏窗口 FLAG_SHOW_WALLPAPER 要求系统壁纸显示在窗口之下,窗口必须是透明的, 才可以看到壁纸,该flag只保证壁纸存在,可以通过theme属性来设置,如Theme_Wallpaper_NoTitleBar等 FLAG_TURN_SCREEN_ON 当窗口被添加或从不可见到可见状态时, 会点亮屏幕 FLAG_DISMISS_KEYGUARD 禁用锁屏,除非是非安全锁屏,与FLAG_SHOW_WHEN_LOCKED正相反,如果锁屏当前是激活的,并且是安全锁屏(需要解锁的),那么用户仍需要进行解锁才能看到窗口,除非设置了FLAG_SHOW_WHEN_LOCKED FLAG_SPLIT_TOUCH 窗口会接收窗口之外的多点触摸事件 FLAG_HARDWARE_ACCELERATED 对窗口启用硬件加速 FLAG_LAYOUT_IN_OVERSCAN 允许窗口扩展到overscan区域 FLAG_TRANSLUCENT_STATUS 要求状态栏透明 FLAG_TRANSLUCENT_NAVIGATION 要求导航栏透明 FLAG_LOCAL_FOCUS_MODE 允许独立于window manager来控制焦点事件,通常该模式的窗口不能从window manager获取触摸/按键事件, 但能够通过Window#injectInputEvent(InputEvent)来获取本地注入事件 FLAG_SLIPPERY 隐藏,允许触摸从一个窗口划出到另一个窗口,该flag仅对当前窗口生效,触摸可以划出, 但无法再划入 FLAG_LAYOUT_ATTACHED_IN_DECOR 当布局依附于窗口时, 所依附的窗口可能会覆盖在屏幕装饰之上, 比如导航栏.设置此flag后,windowmanager将在decor窗口内对所依附的窗口进行布局,这样便不会覆盖在屏幕装饰上 FLAG_DRAWS_SYSTEM_BAR_BACKGROUNDS 指示该窗口用于绘制系统状态栏的背景,如果设置此flag, 系统状态栏会变为透明背景, 窗口中响应的区域会被Window#getStatusBarColor()和Window#getNavigationBarColor()的颜色所填充 WindowManager.LayoutParams.softInputMode用于确定窗口和输入法之间的关系 SOFT_INPUT_MASK_STATE 指定输入法的覆盖层, 确定输入法区域是否可见的代码 SOFT_INPUT_STATE_UNSPECIFIED 输入法的可见状态为: 未指定状态 SOFT_INPUT_STATE_UNCHANGED 输入法的可见状态为: 不改变输入法当前状态 SOFT_INPUT_STATE_HIDDEN 输入法的可见状态为: 用户进入窗口时, 隐藏所有输入法 SOFT_INPUT_STATE_ALWAYS_HIDDEN 输入法的可见状态为: 窗口获取焦点时, 隐藏所有输入法 SOFT_INPUT_STATE_VISIBLE 输入法的可见状态为: 用户进入窗口时, 显示输入法 SOFT_INPUT_STATE_ALWAYS_VISIBLE 输入法的可见状态为: 当窗口获取输入焦点时, 显示输入法 SOFT_INPUT_MASK_ADJUST 指定窗口是否应该根据输入法进行调整的代码 SOFT_INPUT_ADJUST_UNSPECIFIED 窗口调整设置为: 未指定, 系统会尝试进行选择 SOFT_INPUT_ADJUST_RESIZE 窗口调整设置为: 当显示输入法时, 允许窗口被缩放, 使得窗口的内容不会被输入法覆盖,不能同SOFT_INPUT_ADJUST_PAN一起使用,如果窗口布局属性包含FLAG_FULLSCREEN, 该选项会被忽略,窗口不会缩放, 而是保持全屏 SOFT_INPUT_ADJUST_PAN 窗口调整设置为: 当显示输入法时,移动窗口使得输入焦点可见,而不会缩放窗口不能同SOFT_INPUT_ADJUST_RESIZE一起使用 SOFT_INPUT_ADJUST_NOTHING 窗口调整设置为: 当显示输入法时, 既不缩放, 也不移动 SOFT_INPUT_IS_FORWARD_NAVIGATION 用户导航到此窗口时的配置代码. 通常有系统配置, 除非你需要自定义. 当窗口显示后,该配置会清除 ActivityInfo.screenOrientation用于确定窗口的方向 SCREEN_ORIENTATION_UNSPECIFIED 不指定屏幕方向, 跟随系统 SCREEN_ORIENTATION_LANDSCAPE 默认的横向(听筒在左, 按键在右) SCREEN_ORIENTATION_PORTRAIT 默认的竖向(听筒在上, 按键在下, 不包括听筒在下, 按键在上) SCREEN_ORIENTATION_REVERSE_LANDSCAPE 与默认相反的横向(听筒在右, 按键在左) SCREEN_ORIENTATION_REVERSE_PORTRAIT 与默认相反的竖向(实际和SCREEN_ORIENTATION_PORTRAIT一样) SCREEN_ORIENTATION_SENSOR 重力传感器感知的方向(除听筒在下, 按键在上的3个方向) SCREEN_ORIENTATION_NOSENSOR 不使用传感器方向 SCREEN_ORIENTATION_SENSOR_LANDSCAPE 重力方向的横向(听筒在左, 按键在右 / 听筒在右, 按键在左) SCREEN_ORIENTATION_SENSOR_PORTRAIT 重力方向的竖向(听筒在上, 按键在下, 不包括听筒在下, 按键在上) SCREEN_ORIENTATION_FULL_SENSOR 重力方向(听筒在上, 按键在下 / 听筒在下, 按键在上 / 听筒在左, 按键在右 /听筒在右,按键在左) SCREEN_ORIENTATION_USER 用户设置的方向 SCREEN_ORIENTATION_USER_LANDSCAPE 用户设置的横向 SCREEN_ORIENTATION_USER_PORTRAIT 用户设置的竖向 SCREEN_ORIENTATION_FULL_USER 用户设置的4个方向(听筒在上, 按键在下 / 听筒在下, 按键在上 / 听筒在左, 按键在右 /听筒在右, 按键在左) SCREEN_ORIENTATION_BEHIND 当前界面下的Activity的方向 SCREEN_ORIENTATION_LOCKED 锁定当前方向 获取当前方向 int mCurrentOrientation = getResources().getConfiguration().orientation; WindowManager.LayoutParams.rotationAnimation用于确定屏幕旋转时的动画. 是否有效取决于手机 ROTATION_ANIMATION_JUMPCUT 立刻切换, 没有动画 ROTATION_ANIMATION_CROSSFADE 有淡入淡出效果 ROTATION_ANIMATION_ROTATE 旋转动画 ROTATION_ANIMATION_CHANGED (不知道) params.dimAmount窗口下层变暗程度, 默认1.0f不变暗0.0f~1.0f buttonBrightness设置按键的亮度. 有些手机没有键盘灯的无效0.0f~1.0f screenBrightness屏幕亮度,默认是负数, 表示跟随系统亮度.0.0f~1.0f表示最暗到最亮 systemUiVisibility设置系统界面的可见性. 是否有效和手机系统有关 View.INVISIBLE 系统UI不可见 View.SYSTEM_UI_FLAG_LOW_PROFILE 低调模式, 状态栏和图标会变暗 常见应用场景悬浮窗悬浮窗主要是设置type, 有多种type都可以实现悬浮窗效果(PHONE, SYSTEM_ALERT…) WindowManager mWindowManager = (WindowManager) getApplicationContext().getSystemService(Context.WINDOW_SERVICE);// 创建一个新的布局WindowManager.LayoutParams param = new WindowManager.LayoutParams();// 设置窗口属性param.type = WindowManager.LayoutParams.TYPE_SYSTEM_ALERT; // 设置为系统警告窗, 可以悬浮在其他应用之上param.format = PixelFormat.TRANSLUCENT; // 支持透明param.flags = WindowManager.LayoutParams.FLAG_LAYOUT_IN_SCREEN // 可在全屏幕布局, 不受状态栏影响 | WindowManager.LayoutParams.FLAG_NOT_FOCUSABLE; // 最初不可获取焦点, 这样不影响底层应用接收触摸事件param.alpha = 0.9f; // 悬浮窗的透明度param.gravity = Gravity.LEFT | Gravity.TOP; // 悬浮窗的重力效果param.width = dp2px(140); // 悬浮窗宽度param.height = WindowManager.LayoutParams.WRAP_CONTENT; // 悬浮窗高度// 以下将悬浮穿定位在屏幕中央int screenWidth = mWindowManager.getDefaultDisplay().getWidth();int screenHeight = mWindowManager.getDefaultDisplay().getHeight();param.x = (screenWidth - param.width) / 2;param.y = (screenHeight - param.height) / 2;// 创建悬浮窗viewmFloatView = View.inflate(this, R.layout.view_float_window, null);ButterKnife.bind(this, mFloatView);// 添加到屏幕mWindowManager.addView(mFloatView, param); 悬浮窗中需要EditText进行输入悬浮窗如果需要弹出输入法进行输入，就需要获取焦点 当悬浮窗中有EditText需要使用输入法时, 使用FLAG_NOT_TOUCH_MODAL, 这样悬浮窗中的EditText就可以弹出输入法. 此时下层应用可以操作, 但不能弹出输入法, 不能使用返回键. 如需要恢复下层应用的完全操作, 可以等输入完毕后, 找合适时机再把悬浮窗改回FLAG_NOT_FOCUSABLE, 以便恢复背景window的控制 悬浮窗不影响下层应用的操作比如在悬浮窗中没有EditText时, 使用FLAG_NOT_FOCUSABLE禁止获取焦点, 这样背景window就可以操作, 可以输入, 可以使用按键 悬浮窗和输入法之间的叠放效果 当输入法和悬浮窗输入窗口重叠时, 有两种处理方式, 都是通过param.softInputMode来实现:整体移动悬浮窗, 使输入的地方出现 WindowManager.LayoutParams.SOFT_INPUT_ADJUST_PAN 缩小悬浮窗中可以缩小的部分, 使输入的地方出现 WindowManager.LayoutParams.SOFT_INPUT_ADJUST_RESIZE 悬浮窗的拖动效果为悬浮窗设置onTouchListener, 通过事件判断是点击还是移动,使用mWindowManager.updateViewLayout(mFloatView, param)来更新窗口位置 floatView.setOnTouchListener(new View.OnTouchListener() { // 记录上次移动的位置 private float lastX = 0; private float lastY = 0; // 是否是移动事件 boolean isMoved = false; @Override public boolean onTouch(View v, MotionEvent event) { switch (event.getAction()) { case MotionEvent.ACTION_DOWN: isMoved = false; // 记录按下位置 lastX = event.getRawX(); lastY = event.getRawY(); break; case MotionEvent.ACTION_MOVE: isMoved = true; // 记录移动后的位置 float moveX = event.getRawX(); float moveY = event.getRawY(); // 获取当前窗口的布局属性, 添加偏移量, 并更新界面, 实现移动 WindowManager.LayoutParams param = (WindowManager.LayoutParams) mFloatView.getLayoutParams(); param.x += (int) (moveX - lastX); param.y += (int) (moveY - lastY); mWindowManager.updateViewLayout(mFloatView, param); lastX = moveX; lastY = moveY; case MotionEvent.ACTION_CANCEL: isMoved = true; break; } // 如果是移动事件, 则消费掉; 如果不是, 则由其他处理, 比如点击 return isMoved; }}); Activity设置屏幕旋转, 全屏等android:configChanges=&quot;orientation|screenSize&quot;keyboardHiddenorientationscreenSizeorientation|screenSize: 改变屏幕方向时并不重启ActivitysetRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_LANDSCAPE)getWindow().addFlags(WindowManager.LayoutParams.FLAG_FULLSCREEN);getWindow().setAttributes(windowParams); 弹窗位于软键盘之下，其他视图之上getWindow().addFlags( WindowManager.LayoutParams.FLAG_ALT_FOCUSABLE_IM|WindowManager.LayoutParams.FLAG_NOT_FOCUSABLE); FLAG_ALT_FOCUSABLE_IM|FLAG_NOT_FOCUSABLE可以实现UI，不过弹窗上的edittext不会接受到软键盘上的输入事件，FLAG_NOT_FOCUSABLE已经拒绝接受了。那可以在dialog onCreate的时候完成以上UI FLAG设置，在show的时候清除flag即可接受到输入事件 getWindow().clearFlags(WindowManager.LayoutParams.FLAG_ALT_FOCUSABLE_IM|WindowManager.LayoutParams.FLAG_NOT_FOCUSABLE);getWindow().setSoftInputMode(WindowManager.LayoutParams.SOFT_INPUT_STATE_VISIBLE); Dialog全屏显示 Phonewindow中对windowIsFloating进行判断，activity默认主题为false，generateLayout方法中对该参数进行判断设置LayoutParams为MATCH_PARENT或WRAP_CONTENNT。 windowBackground主要是默认背景的问题，默认采用了有padding的InsetDrawable,设置了一些边距，导致上面的状态栏，底部的导航栏，左右都有一定的边距。 windowNoTitle去除toolbar等顶部UI。&lt;style name=&quot;Dialog.FullScreen&quot; parent=&quot;Theme.AppCompat.Dialog&quot;&gt; &lt;item name=&quot;android:windowNoTitle&quot;&gt;true&lt;/item&gt; &lt;item name=&quot;android:windowBackground&quot;&gt;@color/transparent&lt;/item&gt; &lt;item name=&quot;android:windowIsFloating&quot;&gt;false&lt;/item&gt;&lt;/style&gt;","link":"/2018/10/23/WindowManager%E7%AA%97%E5%8F%A3%E7%AE%A1%E7%90%86/"},{"title":"MAC adb无线连接Android真机","text":"一直懒得搞，因为用不上，最近配置了双屏，瞬间usb插口不够用，于是有了下文。 一直懒得搞，因为用不上，最近配置了双屏，瞬间usb插口不够用了，于是就有了下文。 准备工作 手机和电脑需要再同一个局域网 电脑上已经安装adb工具。 开始打开手机端口让手机在指定的端口可以接收到TCP/IP连接 确保手机开启usb调试 用usb线把手机和电脑连接起来 执行命令：’adb tcpip 5555’ 执行成功后拔掉usb线，端口可以不用5555，这个官方默认使用的。 找到手机的IP地址一般在 设置-我得设备-全部参数-状态信息-IP地址 通过IP地址连接手机执行命令： &gt; adb connect 192.168.26.58:5555 若端口号是5555，则可以省略，直接： &gt; adb connect 192.168.26.58 如果没有连接成功如果确定网络与端口没有问题，可以尝试重启adb服务： &gt; adb kill-server 连接成功如果连接成功，可以执行以下命令查看当前连接的设备列表： &gt; adb devices","link":"/2018/05/29/MAC-adb%E6%97%A0%E7%BA%BF%E8%BF%9E%E6%8E%A5Android%E7%9C%9F%E6%9C%BA/"},{"title":"cocoapods私有库处理","text":"抽离工具类并提交cocoaPods私有库 最近在提取项目中的部分工具类，有用到cocoaPods私有库，所以把它记录下来。 私有库使用流程为私有库添加repo$ pod repo add REPO_NAME git@github:artsy/Specs.git 创建私有库$ pod repo add REPO_NAME SOURCE_URL 检查repo安装成功$ cd ~/.cocoapods/repos/REPO_NAME $ pod repo lint . 把准备好的代码库spec加入到私有库中该命令会把此版本的.podSpec拷贝到私有库中并push到远程git代码库 $ pod repo push REPO_NAME SPEC_NAME.podspec .podSpec文件引用其他私有库情况项目中需要引用到私有库的library,这个时候需要用到 sources s.dependency 'THLTencentOpenAPI' $ pod lib lint --sources=https://xx/ThirdSDK.git $ pod repo push ThirdSDK --sources=https://xx/ThirdSDK.git 包含静态库情况添加 --use-libraries 忽略掉相关检查 部分问题处理 The repo `MyRepo` at `../.cocoapods/repos/ThirdSDK` is not clean $ pod repo update ThirdSDK","link":"/2018/03/29/cocoapods%E7%A7%81%E6%9C%89%E5%BA%93%E5%A4%84%E7%90%86/"},{"title":"android studio支持系统签名","text":"Android源码环境编译调试 如果想让apk拥有系统权限，需要再AndroidManifest.xml中添加共享系统进程属性： android:sharedUserId=&quot;android.uid.system&quot; android:sharedUserId=&quot;android.uid.shared&quot; android:sharedUserId=&quot;android.media&quot; 这时候apk的签名就需要是系统签名(platform、shared或media)才能正常使用。 常用系统签名方式Android源码环境下签名这种方式比较麻烦，你需要有编译过的源码环境，并按如下步骤： 1、拷贝App源码到Android源码的packages/apps/目录下，且App源码是普通(Eclipse)格式的2、配置Android.mk，在其中添加 LOCAL_CERTIFICATE := platform 或 shared 或 media 3、使用mm编译App，生成的apk即系统签名 手动重新签名这种方式比在源码环境下签名简单，App可以在Eclipse或Android Studio下编译，然后给apk重新签名即可。但这种方式在频繁调试的时候比较痛苦，即使写成脚本，也需要重复一样的操作。 相关文件platform.x509.pem、platform.pk8、signapk.jar 文件位置platform.x509.pem、platform.pk8: ../build/target/product/security signapk.jar: ../out/host/linux-x86/framework signapk源码路径: ../build/tools/signapk 签名命令java -jar signapk.jar platform.x509.pem platform.pk8 old.apk new.apk 步骤1、将相关文件及源apk文件置于同一路径下2、检查源apk包，去掉META-INF/CERT.SF 和 META-INF/CERT.RSA 文件3、执行签名命令即可 Android Studio系统签名让Android Studio集成系统签名，需要用到一个工具keytool-importkeypair，详见下文。 keytool-importkeypairkeytool-importkeypair – A shell script to import key/certificate pairs into an existing Java keystore 相关文件platform.x509.pem、platform.pk8、keytool-importkeypair、demo.jks、signature.sh我的做法是在App根目录新建Signature文件夹专门存放签名相关文件。1.生成签名文件xx.jks2.编写签名脚本signature.sh，内容如下: #!/bin/sh 转换系统签名命令./keytool-importkeypair -k demo.jks -p 123456 -pk8 platform.pk8 -cert platform.x509.pem -alias demo # demo.jks : 签名文件 # 123456 : 签名文件密码 # platform.pk8、platform.x509.pem : 系统签名文件 # demo : 签名文件别名 为脚本文件添加可执行权限： $ sudo chmod a+x signature.sh 执行脚本： $ ./signature.sh 3.配置builde.gradle signingConfigs { release { storeFile file(&quot;../signature/demo.jks&quot;) storePassword '123456' keyAlias 'demo' keyPassword '123456' } debug { storeFile file(&quot;../signature/demo.jks&quot;) storePassword '123456' keyAlias 'demo' keyPassword '123456' } } 这样debug或release apk就带有系统签名了。","link":"/2018/05/21/android-studio%E6%94%AF%E6%8C%81%E7%B3%BB%E7%BB%9F%E7%AD%BE%E5%90%8D/"},{"title":"kotlin inline","text":"使用高阶函数会带来一些运行时的效率损失：每一个函数都是一个对象，并且会捕获一个闭包。即那些在函数体内会访问到的变量。内存分配和虚拟调用会引入运行时间开销。 前言Kotlin在集合API中大量使用了Lambda，这使得我们在对集合进行操作的时候优雅了许多。但是这种方式的代价就是，在Kotlin中使用Lambda表达式会带来一些额外的开销，而内联函数应运而生，用来解决优化Kotlin支持Lambda表达式之后所带来的开销。 优化Lambda开销Kotlin默认面向JDK6，而JDK8才引入Lambda表达式支持。在Kotlin中每申明一个Lambda表达式，就会在字节码中产生一个匿名类，该匿名类包含了一个invoke方法，作为Lambda的调用方法，每次调用都会创建一个新的对象，必须采用某种方法来优化Lambda带来的额外开销，也就是内联函数。 invokedynamic与Kotlin这种在编译期通过硬编码生成Lambda转换类的机制不同，Java在SE 7之后通过invokedynamic技术实现了在运行期才产生相应的翻译代码。在invokedynamic被首次调用的时候，就会触发产生一个匿名类来替换中间码invokedynamic，后续的调用会直接采用这个匿名类的代码，这样做的好处主要体现在： 由于具体的转换实现是在运行时产生的，在字节码中能看到的只有一个固定的invokedynamic，所以需要静态生成的类的个数及字节码大小都显著减少。 与编译时写死在字节码中的策略不同，利用invokedynamic可以把实际的翻译策略隐藏在JDK库的实现，这极大提高了灵活性，在确保向后兼容性的同时，后期可以继续对翻译策略不断优化升级。 JVM天然支持了针对该方式的Lambda表达式的翻译和优化，这也意味着开发者在书写Lambda表达式的同时，可以完全不用关心这个问题，这极大的提高了开发体验（体验很重要）。 内联函数invokedynamic固然不错，但Kotlin不支持它的理由也很充分。我们有足够的理由相信，其最大的原因是Kotlin在一开始就需要兼容Android最主流的Java版本SE 6，这导致无法通过invokedynamic来解决Android平台的Lambda开销问题。因此作为另外一种主流的解决方案，Kotlin拥抱了内联函数，在C++、C#等语言中也支持这种特性。简单来说，我们可以用inline关键字来修饰函数，这些函数就成了内联函数。它们的函数体在编译期被嵌入每一个被调用的地方，以减少额外生成的匿名类数，以及函数执行的时间开销。 inline语法我们看一下内联函数如何操作 fun main(args:Array&lt;String&gt;) { foo { println(&quot;dive into kotlin...&quot;) }}fun foo(block: () -&gt; Unit) { block()} 申明一个高阶函数foo，接收一个类型为()-&gt;Unit的Lambda，并在main函数中调用它。以下是通过字节码反编译得到的Java代码: public static final void foo(@NotNull Function0 block) { Intrinsics.checkParameterIsNotNull(block, &quot;block&quot;); block.invoke(); } public static final void main(@NotNull String[] args) { Intrinsics.checkParameterIsNotNull(args, &quot;args&quot;); foo((Function0)null.INSTANCE); } 调用foo就会产生一个Function0类型的block类，然后通过invoke方法来执行，这会增加额外的生成类和调用开销，现在我们给foo函数加上inline修饰符，如下: inline fun foo(block:() -&gt; Unit) { block()} public static final void main(@NotNull String[] args) { Intrinsics.checkParameterIsNotNull(args, &quot;args&quot;); int $i$f$foo = false; int var2 = false; String var3 = &quot;xxxx&quot;; boolean var4 = false; System.out.print(var3);} 果然，foo函数体代码及被调用的Lambda代码都粘贴到了相应调用的位置，如果这是一个工程中的公共的方法，或者被嵌套在一个循环调用的逻辑体中，通过inline语法糖，我们可以彻底消除这种额外调用，从而节约开销。 一些情况下应该避免使用inline JVM对普通的函数已经能够根据实际情况智能地判断是否进行内联优化，所以我们并不需要对其使用Kotlin的inline语法，那只会让字节码变得更加复杂 尽量避免对具有大量函数体的函数进行内联，这样会导致过多的字节码数量 一旦一个函数被定义为内联函数，便不能获取闭包类的私有成员，除非被声明为internal 避免参数被内联noinline现实中情况往往十分复杂，可能多个参数时，我们只想对部分Lambda参数内联，其他不内联，这个时候就需要noinline关键字 inline fun foo (block1 () -&gt; Unit,noinline block2:()-&gt;Unit) { block1() block2()} 非局部返回Kotlin中的内联函数除了优化Lambda开销之外，还带来了其他方面的特效，典型的就是非局部返回和具体化参数类型。 使用 fun main(args: Array&lt;String&gt;) { foo() } fun localReturn() { return } fun foo() { println(&quot;before local return&quot;) localReturn() println(&quot;after local return&quot;) return }//运行结果//before local return//after local return localReturn执行后，其函数体中的return只会在该函数的局部生效，所以localReturn()之后的println函数依旧生效。换成Lambda表达式的版本: fun main(args: Array&lt;String&gt;) { foo {return} } fun foo (returning: () -&gt; Unit) { println(&quot;before local return&quot;) returning() println(&quot;after local return&quot;) return }//运行结果// Error:(2,11) Kotlin: 'return' is not allowed here 编译报错，正常情况下Lambda表达式不允许存在return关键字，这时候内联函数就派上用场了，foo函数进行内联后： //运行结果//before local return 内联函数foo的函数体及参数Lambda会直接替代具体的调用，所以实际产生的代码中，return相当于直接暴露在main函数中，所以returning()之后的代码自然不会被执行。这个就是所谓的非局部返回。 使用标签实现Lambda非局部返回另外一种等效的方式，是通过标签利用@符号来实现Lambda非局部返回，我们可以在不申明inline的情况下，实现同样的效果： fun main(args: Array&lt;String&gt;) { foo (return@foo) } fun foo(returning: () -&gt; Unit) { println(&quot;before local return&quot;) returning() println(&quot;after local return&quot;) return }//运行结果//before local return crossinline我们内联函数所接收的Lambda参数常常来自于上下文其他地方，为了避免带有return的Lambda参数产生破坏，我们可以是crossinline来修饰该参数，从而杜绝该问题的发生。 fun main(args:Array&lt;String&gt;) { foo {return}//return会有下滑波浪线，IDEA会报错}inline fun foo(crossinline returning:()-&gt;Unit) { println(&quot;before&quot;) returning() println(&quot;after&quot;) return} 具体化参数类型除了非局部返回之外，内联函数还可以帮助Kotlin实现具体化参数类型，Kotlin与Java一样，由于运行时的类型擦拭，我们并不能直接获取一个参数的类型。然而，由于内联函数会直接在字节码中生成相应的函数体实现，这种情况下我们反而可以获得参数的具体类型，可以用reified修饰符来实现这一效果 fun main(args:Array&lt;String&gt;){ getType&lt;Int&gt;() } inline fun &lt;reified T&gt;getType() { print(T::class) }//运行结果//class Kotlin.Int 这一特性在Android中格外有用。比如，当我们要调用startActivity时，通常需要把具体的目标视图类作为参数传递，而在Kotlin中: inline fun &lt;reified T : Activity&gt; Activity.startActivity() { startActivity(Intent(this,T::class.java))}//usagestartActivity&lt;LoginActivity&gt;() END","link":"/2019/11/18/kotlin-inline/"},{"title":"wake me up","text":"Feeling my way through the darkness Guided by a beating heart I can't tell where the journey will end But i know where to start Wake Me up -AviciiFeeling my way through the darkness Guided by a beating heart I can’t tell where the journey will end But i know where to start They tell me I’m too young to understand They say I’m caught up in a dream Well life will pass me by if i don’t open up my eyes Well that’s fine by me So wake me up when it’s all over When I’m wiser and I’m older All this time I was finding myself And I did’t know I was lost I tried carrying the weight of the world But I only hava two hands All this time I was finding myself And I didn’t know I was lost I tried carrying the weight of the world But I only have two hands I hope I get the chance to travel the world But I don’t have any plans I wish that I could stay forever this young Not afraid to close my eyes Life’s a game made for everyone And love is the prize So wake me up when it’s all over When I’m wiser and I’m older All this time I was finding myself And I didn’t know I was lost So wake me up when it’s all over When I’m wiser and I’m older All this time I was finding myself And I didn’t know I was lost So wake me up when it’s all over When I’m wiser and I’m older All this time I was finding myself And I didn’t know I was lost I didn’t know I was lost I didn’t know I was lost I didn’t know I was lost I didn’t know I was lost I didn’t know I was lost So wake me up when it’s all over When I’m wiser and I’m older All this time I was finding myself And I didn’t know I was lost","link":"/2019/03/07/wake-me-up/"},{"title":"Filament实现简易Animoji","text":"Animoji是首先由苹果提出的增强现实表情包，利用摄像头捕捉到的面部特征点，及麦克风记录的声音，最终生成的3D动画表情。我们尝试在Android上实现类似的效果，于是有了这篇文章记录技术重点。 特征点捕捉模型选用Face mesh（如果对眼球转动有捕捉要求可以使用Iris），谷歌给出了其捕捉的468个特征点坐标图，我们在找到需要的特征点后，通过减法得到表情值大小，从而计算出加权顶点坐标位置对应的模型效果。 在神经网络推理框架的选择上，ncnn相对于MediaPipe有更高的性能优势，但是其camera相关功能使用camera2NDK，部分机型底层并不支持以至于出现黑屏现象，所以考虑到demo的实现效率，我们选用mediapipe，若想把此功能运用在商业产品上，可以考虑使用ncnn并替换到camera部分实现。 iris在face mesh基础上增加了Iris地标模型，如下图3d表情眼珠的区别： iris vs face_mesh 3D引擎渲染工具我们选择Filament作为3D模型渲染库，一是其用于Android设备上的Sceneform库中实现ARCore，二是其优秀的3D渲染效果及低耗能。当然我们因为不可抗拒因素无法直接使用ARCore，对Filament的学习势在必行。 Filament不仅有自己的模型格式，且支持glb/gltf，并且提供api操作节点，在初步了解光照、天空盒等概念后，开始编写demo。 从摄像头获取面部模型Mediapipe已经把camera数据转换成了textureFrame，进而触发filament渲染模型。filament demo提供的Choreographer触发方式在此是行不通的，垂直同步时间戳已经是最小刷新间隔，同时使用会出现严重的卡顿问题。 从特征点到3D模型那么如何把模型捕捉到的特征点转化为3D表情呢？这里以张嘴为例，设计给出的GLB文件已经定义好嘴部动作的权重，如”weights” { &quot;extras&quot;: { &quot;targetNames&quot;: [ &quot;xx&quot; ] }, &quot;name&quot;: &quot;mouth&quot;, &quot;primitives&quot;: [ { &quot;attributes&quot;: { &quot;POSITION&quot;: 0, &quot;NORMAL&quot;: 1 }, &quot;indices&quot;: 2, &quot;material&quot;: 0, &quot;targets&quot;: [ { &quot;POSITION&quot;: 3, &quot;NORMAL&quot;: 4 } ] } ], &quot;weights&quot;: [ 1 ]}, 可以看到”weights”权重默认为1，我们通过获取特征点坐标17、0处y轴坐标值，转换ndc坐标后，做减法得出嘴巴张开度，从何获取其权重 val landmarkList = faceMeshResult.multiFaceLandmarks()[0](landmarkList.getLandmark(17).y - 1) * PROJECTION_SCALE - (landmarkList.getLandmark(0).y - 1) * PROJECTION_SCALE Filament调用setMorphWeights设置mouth节点权重，从而实现嘴部的开闭。 modelViewer.engine.renderableManager.setMorphWeights( modelViewer.engine.renderableManager.getInstance(modelViewer.asset!!.getFirstEntityByName(&quot;mouth&quot;)), floatArrayOf(1 - 5f * mouthHeight, 0f, 0f, 0f), 0) 在镜头前需要移动头部模型，那么如何实现模型的移动呢，我们决定选用鼻子顶部坐标来映射模型移动,即94处坐标。 Matrix.setIdentityM(transformMatrix, 0) Matrix.rotateM(transformMatrix, 0, 180f, 0f, 4f, 0f) Matrix.translateM(transformMatrix, 0, 0f, 0f, 4f) Matrix.translateM( transformMatrix, 0, -(noseCoord.x - ratio) * PROJECTION_SCALE, -(noseCoord.y - ratio) * PROJECTION_SCALE, 0f) Filament使用setTransform即可实现模型移动，如上可实现简易的animoji效果。","link":"/2022/07/25/Filament%E5%AE%9E%E7%8E%B0%E7%AE%80%E6%98%93Animoji/"},{"title":"云手机openvmi搭建","text":"虽然付费云手机服务比较多，但是开源项目非常少，本文将分享搭建openvmi过程，资源太少，希望有所帮助。 为了过程完整性，文章与openvmi文档会有所重复，着重讲解踩过的坑，特别是对于android开发者而言有所难度的k8s搭建过程。流程分为两部分：android镜像编译、k8s环境搭建/调试 Android镜像编译aosp的网络要求很高，笔者在这上面宰了很多跟头，mac m1、mac x86、ubuntu，最后在腾讯云买了8核32G、100m带宽的香港服务器（ubuntu18.04 x86 50G）10分钟搞定aosp拉取（竞价模式，谁用谁知道）。 安装依赖软件sudo apt install -y build-essential cmake cmake-data debhelper dbus google-mock \\ libboost-dev libboost-filesystem-dev libboost-log-dev libboost-iostreams-dev \\ libboost-program-options-dev libboost-system-dev libboost-test-dev \\ libboost-thread-dev libcap-dev libsystemd-dev libegl1-mesa-dev \\ libgles2-mesa-dev libglm-dev libgtest-dev liblxc1 \\ libproperties-cpp-dev libprotobuf-dev libsdl2-dev libsdl2-image-dev lxc-dev \\ pkg-config protobuf-compiler python-minimal linux-headers-`uname -r` 编译openvmi// 下载源码$ cd ~git clone https://github.com/DockDroid/openvmi.git// 编译与安装内核模块sudo mkdir -p /opt/openvmi/driver/cd ~/openvmi/kernel/binder/makesudo cp binder_linux.ko /opt/openvmi/driver/cd ../ashmem/makesudo cp ashmem_linux.ko /opt/openvmi/driver/// 编译与安装openvmicd ~/openvmimkdir buildcd buildcmake ..make -j8 installsudo mkdir -p /opt/openvmi/bin/ /opt/openvmi/libs/libswiftshadersudo cp bin/openvmi /opt/openvmi/bin/sudo cp ../libs/* /opt/openvmi/libs/libswiftshader/ 搭建Android镜像编译环境// 安装JDKsudo apt-get updatesudo apt-get install openjdk-8-jdk// 编译依赖库sudo apt-get install libx11-dev:i386 libreadline6-dev:i386 libgl1-mesa-dev g++-multilib sudo apt-get install -y git flex bison gperf build-essential libncurses5-dev:i386 sudo apt-get install tofrodos python-markdown libxml2-utils xsltproc zlib1g-dev:i386 sudo apt-get install dpkg-dev libsdl1.2-dev libesd0-devsudo apt-get install git-core gnupg flex bison gperf build-essential sudo apt-get install zip curl zlib1g-dev gcc-multilib g++-multilib sudo apt-get install libc6-dev-i386 sudo apt-get install lib32ncurses5-dev x11proto-core-dev libx11-dev sudo apt-get install libgl1-mesa-dev libxml2-utils xsltproc unzip m4sudo apt-get install lib32z-dev ccache// 独立输出目录易于提高IOexport OUT_DIR_COMMON_BASE=~/out//ccache有需要的可以选择，本人未安装 编译Android镜像此处注意 –depth=1 -c 与官方的不同，笔者在使用repo拉取源码的时候，拉到500G都不够用，这里要指定clone深度，不需要git协作的历史记录；只同步指定分支，不拉去其他分支。 // 安装repomkdir ~/binPATH=~/bin:$PATHcurl https://storage.googleapis.com/git-repo-downloads/repo &gt; ~/bin/repochmod a+x ~/bin/repo//拉取源码mkdir $HOME/openvmi-workcd $HOME/openvmi-workrepo init -u https://github.com/DockDroid/platform_manifests.git -b openvmi --depth=1repo sync -c -j32//100m/s，10分钟搞定，✿✿ヽ(°▽°)ノ✿ 编译Android源码并非单纯的aosp，还加入了openvmi二进制文件，即上面编译的openvmi // 拷贝openvmimkdir vendormv ~/openvmi ~/openvmi-work/vendor/openvmi// 初始化环境（这里输出两条日志，如没有，应该是sync不完全）source build/envsetup.sh// 构建lunchlunch openvmi_arm64-userdebug// 开整export LC_ALL=Cmake -j32//如遇到Jack server failed to ,SSL error when connecting to the Jack server. Try 'jack-diagnose'在 /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/security/java.security 中删除jdk.tls.disabledAlgorithms配置中的&quot;TLSv1,TLSv1.1&quot;。// 增加openvmi支持cd $HOME/openvmi-work/vendor/openvmiscripts/create-package.sh \\ /root/openvmi-work/out/target/product/arm64/ramdisk.img \\ /root/openvmi-work/out/target/product/arm64/system.img 至此，镜像编译完毕。 k8s平台搭建官方文档记录的很清晰，但由于网络环境的问题，导致Kube-flannel一直有问题，dns pod拉不起来，笔者在kube-flannel.yml文件找到image，一次单独pull后解决。由于资源有限，以单机k8s的形式搭建，若想搭建集群，按文档加入woker节点即可。 相关工具安装// 工具拉取cd ~git clone https://github.com/DockDroid/cloud-platform.git// docker安装// docker配置文件sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'{ &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;100m&quot; }, &quot;storage-driver&quot;: &quot;overlay2&quot;}EOF//添加阿里源sudo add-apt-repository &quot;deb [arch=arm64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;//安装dockersudo apt-get install docker-ce=5:18.09.9~3-0~ubuntu-bionic//k8s运行环境安装//关闭swap分区交换sudo swapoff -asudo vi /etc/fstab #以“#”注释swapfile打头的那一行//添加k8s相关命令行工具安装源curl https://mirrors.huaweicloud.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt; EOF | sudo tee /etc/apt/sources.list.d/kubernetes.listdeb https://mirrors.huaweicloud.com/kubernetes/apt kubernetes-xenial mainEOFsudo apt-get update// 安装k8s命令行工具sudo apt-get install kubelet=1.14.2-00 kubeadm=1.14.2-00 kubectl=1.14.2-00 kubernetes-cni=0.7.5-00 //手动拉取k8s Master运行组件的docker镜像docker pull docker.io/mirrorgooglecontainers/kube-apiserver-arm64:v1.14.2 docker pull docker.io/mirrorgooglecontainers/kube-controller-manager-arm64:v1.14.2 docker pull docker.io/mirrorgooglecontainers/kube-scheduler-arm64:v1.14.2 docker pull docker.io/mirrorgooglecontainers/kube-proxy-arm64:v1.14.2 docker pull docker.io/mirrorgooglecontainers/pause-arm64:3.1 docker pull docker.io/mirrorgooglecontainers/etcd-arm64:3.3.10 docker pull docker.io/coredns/coredns:1.3.1//修改tagdocker tag docker.io/mirrorgooglecontainers/kube-apiserver-arm64:v1.14.2 k8s.gcr.io/kube-apiserver:v1.14.2 docker tag docker.io/mirrorgooglecontainers/kube-controller-manager-arm64:v1.14.2 k8s.gcr.io/kube-controller-manager:v1.14.2 docker tag docker.io/mirrorgooglecontainers/kube-scheduler-arm64:v1.14.2 k8s.gcr.io/kube-scheduler:v1.14.2 docker tag docker.io/mirrorgooglecontainers/kube-proxy-arm64:v1.14.2 k8s.gcr.io/kube-proxy:v1.14.2 docker tag docker.io/mirrorgooglecontainers/pause-arm64:3.1 k8s.gcr.io/pause:3.1 docker tag docker.io/mirrorgooglecontainers/etcd-arm64:3.3.10 k8s.gcr.io/etcd:3.3.10 docker tag docker.io/coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1//删除原tagdocker rmi docker.io/mirrorgooglecontainers/kube-apiserver-arm64:v1.14.2 docker rmi docker.io/mirrorgooglecontainers/kube-controller-manager-arm64:v1.14.2 docker rmi docker.io/mirrorgooglecontainers/kube-scheduler-arm64:v1.14.2 docker rmi docker.io/mirrorgooglecontainers/kube-proxy-arm64:v1.14.2 docker rmi docker.io/mirrorgooglecontainers/pause-arm64:3.1 docker rmi docker.io/mirrorgooglecontainers/etcd-arm64:3.3.10 docker rmi docker.io/coredns/coredns:1.3.1 k8s安装sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --kubernetes-version=v1.14.2 --image-repository registry.aliyuncs.com/google_containers//kubectl执行权限sudo mkdir $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config k8s网络插件安装笔者使用kube-flannel 0.15.1版本，找到kube-flannel 中image配置，并提前pull镜像来规避网络环境造成dns,flannel-ads拉不起来的问题，步骤如下： docker pull rancher/mirrored-flannelcni-flannel-cni-plugin:v1.0.0-arm64docker pull quay.io/coreos/flannel:v0.15.1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.15.1/Documentation/kube-flannel.yml k8s状态查询这里提供一些好用的命令，用以查看K8s搭建情况，在实际搭建过程中，要注意如下命令的使用，如发现pod处于pending或者说非Running状态，大概率是镜像拉取失败，需考虑上面步骤是否执行成功 //查看本地镜像下载情况docker images//查看当前镜像运行情况docker ps -a//查看pod状态，非Running状态说明部署有问题kubectl get pods -A//查看node状态，需要Ready状态kubectl get node//查看指定pod，node详细状况kubectl describe node master-01kubectl describe pod coredns-000010-01 -n kube-system//查看相关服务状态及日志systemctl status kubelet openvmi服务配置编译openvmi镜像mkdir mount_dirsudo mount android.img ./mount_dircd ./mount_dirsudo tar --numeric-owner -cf- . | docker import - android:openvmi openvmi插件，运行环境管理服务安装cd cloud-platform/services/k8s-dev-plugin-servicesudo ./install.shcd cloud-platform/services/android-env-servicesudo ./install.sh dashboard配置//拉取dashboardkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc6/aio/deploy/recommended.yaml//修改端口类型，将type: ClusterIP改为NodePortkubectl edit svc kubernetes-dashboard -n kubernetes-dashboard//查看dashboard端口，这里需要修改安全组配置，开放该端口kubectl get svc -A 之后通过https://&lt;集群公网ip&gt;:&lt;上述端口&gt;，如使用chrome出现的非私密链接提示，在当前页面静默输入”thisisunsafe”即可规避。 配置tokentouch dashboard.yamlvim dashboard.yaml//键入如下内容apiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard// 创建账号kubectl apply -f dashboard.yaml// 生成tokenkubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')\\ 之后便可公网访问集群仪表盘，查看集群运行情况。 鲲鹏920服务器架构导致create失败目前华为云服务器均为鲲鹏920纯armv8架构，只支持64位App，需要修改如下配置后再编译系统镜像： device/openvmi/arm64// 修改BoardConfig.mk：#TARGET_2ND_ARCH := arm#TARGET_2ND_ARCH_VARIANT := armv7-a-neon#TARGET_2ND_CPU_VARIANT := cortex-a15#TARGET_2ND_CPU_ABI := armeabi-v7a#TARGET_2ND_CPU_ABI2 := armeabi// 修改device.mk：PRODUCT_COPY_FILES += system/core/rootdir/init.zygote64.rc:root/init.zygote64.rcPRODUCT_DEFAULT_PROPERTY_OVERRIDES += ro.zygote=zygote64TARGET_SUPPORTS_32_BIT_APPS := falseTARGET_SUPPORTS_64_BIT_APPS := true 使用相关脚本位于cloud-platform/tools/android-vm-manage.sh中，android虚拟机相关配置可自定义，根据服务器状况填写，默认是4核4G、720*1280，针对单机启动命令如下： // 在当前服务器创建一台编号为1的虚拟机，create成功后会自动开机，不需要执行startup操作./android-vm-manage create - 1//其他命令自行查找 以上基本能完成openvmi搭建，读者可以在云手机使用指导中学习如何远端使用云手机，并效验成果。","link":"/2022/11/11/%E4%BA%91%E6%89%8B%E6%9C%BAopenvmi%E6%90%AD%E5%BB%BA/"},{"title":"冲突中对中规中矩的认识","text":"每个个体之间必然存在冲突，对每件事物的理解，处理每件事情的方式，一万个人心中有一万个哈姆雷特，这才是有趣的地方，不一样才是有趣的部分，如果都一样岂不是机器。 冲突每个个体之间必然存在冲突，对每件事物的理解，处理每件事情的方式，一万个人心中有一万个哈姆雷特，这才是有趣的地方，不一样才是有趣的部分，如果都一样岂不是机器。 我们把关注点放在了个体不同造成的冲突上，紧接着看到的是冲突升级，如果我们不够强大，往往会成为千夫所指，因为允许冲突存在才是社会的游戏规则，每个人都在试图隐藏自己的不幸，并用各种方式去强调自己的长处，他们所认为自己拥有的长处，所用的手段更是千奇百怪。只要是不损害他人利益，手段就算是合理的，被默许，双方相安无事，各安其位。突然有一天出现一个人，指出这些手段是不合理的，并告诉大家这算不上是长处。然后静默被打破，每个人都担心自己的手段被挑明，大家人心惶惶… 不用多久,大家就会找到自己的方式去保护自己，所以千夫所指是必然。 观察并思考，然后实践。 进入这个团体，遵守所谓的规则，保护好自己的不同，任风雨袭来，我自巍然不动。 中规中矩&quot;选择生活，选择工作，选择职业，选择家庭。选择他妈的一个大电视。选择洗衣机，汽车，雷射唱机，电动开罐机。选择健康，低卡里路，低糖。选择固定利率房贷。选择起点，选择朋友，选择运动服和皮箱。选择一套他妈的三件套西装。……选择DIY，在一个星期天早上，他妈的搞不清自己是谁。选择在沙发上看无聊透顶的节目，往口里塞垃圾食物。选择腐朽，由你精子造出取代你的自私小鬼，可以说是最无耻的事了。选择你的未来，你的生活。但我干嘛要做？我选择不要生活，我选择其他。理由呢？没有理由。只要有海洛因，还要什么理由&quot; ---“猜火车” 刚毕业那会儿我以为我的一生就像是在建一座摩天大厦，稍有一点偏差就会轰然崩塌。慢慢的我发现，人们并不是小心翼翼的去对待自己，“差不多就行了”，好像成了大家的口头禅，得过且过。我却依然不这么认为，却有一些细微的改变，没有100%正确的决定，只需要在决定后努力完善，一步一步让决定变得更加正确。 “差不多就行？” “差不多就行！” 这句话应该是正确的，只是目标变成了别人。严于律己，宽以待人，懂得个体的不同，学会遵守规则，保留住彼此间的安全距离。一个人的世界观，取决于这个人的受教育程度和他的成长环境，而二者是不受他控制。他自己都改变不了的东西，旁人更无法指手画脚。 想要打破规则就应该学会中规中矩，不鸣则已，一鸣惊人。","link":"/2017/11/25/%E5%86%B2%E7%AA%81%E4%B8%AD%E5%AF%B9%E4%B8%AD%E8%A7%84%E4%B8%AD%E7%9F%A9%E7%9A%84%E8%AE%A4%E8%AF%86/"},{"title":"利用高阶函数简化策略模式、模板方法模式","text":"策略模式、模板方法模式解决的问题比较类似，并且都可以依靠Kotlin中的高阶函数特性进行改良。 遵循开闭原则：策略模式假设有一个表示游泳运动员的抽象类Swimmer，有一个游泳的方法swim： class Swimmer { fun swim() { println(&quot;I am swimming...&quot;) }}//invoke&gt;&gt;&gt; Swimmer().swim()I am swimming... 由于这位运动员在游泳方面很有天赋，他很快掌握了蛙泳、仰泳、自由泳多种姿势。所以我将对Swimmer进行改造： class Swimmer { fun breaststroke() { println(&quot;I am breaststroke...&quot;) } fun backstroke() { println(&quot;I am backstroke...&quot;) } fun freestyle() { println(&quot;I am freestyling...&quot;) }} 然而这并不是一个很好的设计。首先，并不是所有的游泳运动员都掌握了这3种游泳姿势，如果每个Swimmer类对象都可以调用所有方法，显得比较危险。其次，后续难免会有新的行为方法加入，通过修改Swimmer类的方式违背了开放封闭原则(open for extension,closed for modification。面向扩展开放，面向修改关闭)。 所以呢，更好的做法是将游泳这个行为封装成接口，根据不同的场景我们可以调用不同的游泳方法。比如这位游泳运动员计划周末游自由泳，其他时间则游蛙泳。策略模式就是一种解决这种场景很好的思路。 策略模式定义了算法族，分别封装起来，让他们之间可以相互替换，此模式让算法的变化独立于使用算法的用户 本质上，策略模式做的事情就是将不同的行为策略（Strategy）进行独立封装，与类的逻辑上解耦。然后根据不同的上下文（Context）切换选择不同的策略，然后用类对象进行调用： interface SwimStrategy { fun swim()}class Breaststroke: SwimStrategy { override fun swim() { println(&quot;I am breaststroking...&quot;) }}class Backstroke: SwimStrategy { override fun swim() { println(&quot;I am Backstroke...&quot;) }}class Freestyle: SwimStrategy { override fun swim() { println(&quot;I am freestyling...&quot;) }}class Swimmer(val strategy: SwimStrategy) { fun swim() { strategy.swim() }}//invokeSwimmer(Freestyle()).swim()Swimmer(Breaststroke()).swim() 这个方案实现了解耦和复用的目的，且很好实现了在不同场景切换采用不同的策略。然而，该版本的代码量也比之前多了很多。 高阶函数抽象算法如果用高阶函数的思路来重新思考下策略类，显然将策略封装成一个函数然后作为参数传递给Swimmer类会更加的简洁。由于策略类的目的非常明确，仅仅是针对行为算法的一种抽象，所以高阶函数式是一种很好的替代思路。 fun breaststroke() { ...}fun Backstroke() { ...}fun freestyle(){ ...}class Swimmer(val swimming: () -&gt; Unit) { fun swim() { swimming() }}//invokeSwimmer(::freestyle).swim 代码量一下子变少，而且结构上也更加容易阅读。由于策略算法都封装成了一个个函数，我们在初始化Swimmer类对象时，可以用函数引用的语法传递构造参数。当然，我们也可以把函数用val声明成Lambda表达式，那么在传递参数时会变得更加简洁直观。 模板方法模式：高阶函数代替继承另一个可用高阶函数改良的设计模式，就是模板方法模式。某种程度上，模板方法模式和策略模式要解决的问题是相似的，它们都可以分离通用的算法和具体的上下文。然而，如果说策略模式采用的思路是将酸奶法进行委托，那么传统的模板方法模式更多是基于继承的方法实现的。现在来看看模板方法模式的定义： 定义一个算法中的操作框架，而将一些步骤延迟到子类中，使得子类可以不改变算法的结构即可定义该算法的某些特定步骤。 与策略模式不同，模板方法模式的行为算法具有更明晰的大纲结构，其中完全相同的步骤会在抽象类中实现，可个性化的某些步骤则在某子类中进行定义。举个例子，如果我们去市民事务中心办事时，一般都会有以下几个具体的步骤： 1） 排队区号等待 2）根据自己的需求办理个性化的业务，如获取社保清单、申请市民卡、办理房产证 3）对服务人员的态度进行评价 这是一个典型的适用模板方法模式的场景，办事步骤整体是一个算法大纲，其中步骤1）和3）都是相同的算法，而步骤2）则可以根据实际需求个性化选择。接下来我们就用代码实现一个抽象类，它定义了这个例子的操作框架： abstract class CivicCenterTask { fun execute() { this.lineup() this.askForHelp() this.evaluate() } private fun lineup() { println(&quot;line up to take a number&quot;) } private fun evaluate() { println(&quot;evaluaten service attitude&quot;) } abstract fun askForHelp()} 其中askForHelp方法是一个抽象方法。接下来我们再定义具体的子类来继承CivicCenter-Task类，然后对抽象的步骤进行实现。 class PullSocialSecurity: CivicCenterTask { override fun askForHelp() { println(&quot;ask for pulling teh social security&quot;) }}class ApplyForCitizenCard: CiviCenterTask { override fun askForHelp() { println(&quot;apply for a citizen card&quot;) }}//invoke&gt;&gt;&gt; PullSocialSecurity().execute()line up to take a numberask for pulling the social securityevaluation service attitude&gt;&gt;&gt; ApplyForCitizenCard().execute()line up to take a numberapply for a citizen cardevaluaten service attitude 不出意料，两者的步骤2）的执行结果是不一样的。 不得不说，模板方法模式的代码复用性已经非常高了，但是我们还是得根据不同的业务场景都定义一个具体的子类。幸运的是，在Kotlin中我们同样可以用改造策略模式的类似思想，来简化模板方法模式。依靠高阶函数，我们可以在只需一个CivicCenterTask类的情况下，代替继承实现相同的效果。 class CivicCenterTask { fun execute(askForHelp: ()-&gt;Unit) { this.lineUp() askForHelp() this.evaluate() } private fun lineup(){ ... } private fun evaluate(){ ... } fun pullSocialSecurity() { ... } fun applyForCitizenCard() { ... }}//invoke&gt;&gt;&gt; CivicCenterTask().execute(::pullSocialSecurity)live up to take a numberask for pulling the social securityevaluaten service attitude&gt;&gt;&gt; CivicCenterTask().excute(::applyForCitizenCard)line up to take numberapply for a citizen cardevaluaten service attitude 如你所见，在高阶函数的帮助下，我们可以更加轻松地实现模板方法模式。","link":"/2020/01/17/%E5%88%A9%E7%94%A8%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0%E7%AE%80%E5%8C%96%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%E3%80%81%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/"},{"title":"又一次有意思的经历","text":"又是一次有意思的经历，踉踉跄跄的来，心无旁骛的走，我感到了前所未有的释然，从来都没有过的成长，这种成长并非职业技能上的成长，既是个人思想上的成长，如果对待无所不在的冲突呢？ 又是一次有意思的经历，踉踉跄跄的来，心无旁骛的走，我感到了前所未有的释然，从来都没有过的成长，这种成长并非职业技能上的成长，既是个人思想上的成长，如果对待无所不在的冲突呢？如何在冲突中找到属于自己的位置呢？要是自我职业技能成长迅速的话，如何在狂妄自大中平息哪一点虚无呢？要是身处混乱之中如何不去随波逐流进行自我救赎呢？ 冲突真的存在吗应该不存在吧，有既是有，没有既是没有，没有的你想有也就有了，有的东西你想没有也就没有了。对待各种欲望的爆发，就应该理性的思考，让思考超过非理性的情绪爆发，情绪化其实就是最大的弱点。冲突本不是问题的根本，冲突是人为的强制性添加的用来解决自己对当前事物无能处理的结果，应该在多次深入思考后去解决这种无能。 要是陷入了虚无的自大中，如何平息呢那其实应该是注焉而不满酌焉而不竭呢，老祖宗的这句对游刃有余的解释完美的解决了这个问题，自大源自自满，歧视来自无知。要是能有更大的眼界，站到更高的位置，应该只会感受到自己的渺小吧。其实如果对这个世界永远充满敬畏的话，那一定会一直处于自我反省的状态吧，怎么会自大呢。其实能承认自己自大也已经是很大的进步了，承认自己并非你所以为的那种人，称得上是一种相当可怕的经历。没那么可怕呀。 混乱之中如何自己救赎呢这里的混乱并非小丑嘴里的混乱，也不是什么名词，我觉得在我眼里应该是形容词吧，就是一堆乱麻而已，乱麻用来描述自己看到的方方面面，太过敏感，敏感并非贬义词，收到外界的信息以后，如何处理才是关键。你看吧，现在你可以收到好多别人接收不到的信息呢，可是你从不加工，从不过滤，就让它们摇摇晃晃的进来了，结果外面一片太平，而你自己痛不欲生。那不是应该反过来吗，再不济也应该是同样太平才对呀？ end前段时间终于看完了《尘埃落定》。就像傻子最后说的，“是的，上天叫我看见，叫我听见，叫我置身其中，又叫我超然物外。上天是为了这个目的，才让我看起来像个傻子的。”对啊，你也可以看见，你也可以听见，你也置身于这大千世界里，一定不能辜负上天对你的恩惠。你一定要学会超然物外，要是到不了这个境界，也应该给与有力的回击，不管是善意的回击还是恶意的，祝福你吧。","link":"/2019/03/22/%E5%8F%88%E4%B8%80%E6%AC%A1%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%BB%8F%E5%8E%86/"},{"title":"将Flutter添加到已有的项目中","text":"查看了网上各种方案也包括闲鱼团队的开源flutter_boot，都各有缺点。鉴于各种方案都需要制定相应的Flutter版本，考虑到Flutter更新速度，于是决定依靠官方的产物集成方式解决混合开发的问题，并进行远程SDK管理。 Flutter构建模式你可以在官方DOCS中看到详细介绍，下文打包命令都会产出对应三种模式的包以便满足各场景需要，我们简单记录下来： DebugDebug模式下，app可以被安装在物理设备、仿真器或者模拟器上进行调试。 热重载仅能在Debug模式下运行 仿真器、模拟器仅能在Debug模式下运行 Debug模式下，APP性能可能会掉帧或者卡顿 ProfileProfile模式下，一些调试能力是被保留的——足够分析你的app性能。在仿真器和模拟器上，Profile模式是不可用的。 ReleaseRelease模式对快速启动、快速执行和package的大小进行了优化，并禁用了调试、服务扩展等功能，Release模式做到了最大的优化以及最小的占用空间。 如上三种模式可以通过以下命令运行到设备上： flutter run --releaseflutter run --profileflutter run --debug ##iOS - 以framework的形式添加到你的既有iOS应用中，并上传CocoaPods 创建Flutter module你可以在Android Studio中创建 也可以通过命令行 flutter create --template module my_flutter 此module并不是Android里的模块或者iOS里的组件，而是Google提供的一种创建模式，可以独立运行。 产物集成flutter build ios-framework --output=somePath/ 产物目录结构如下： somePath/└── Flutter/ ├── Debug/ │ ├── Flutter.framework │ ├── App.framework │ ├── FlutterPluginRegistrant.framework (only if you have plugins with iOS platform code) │ └── example_plugin.framework (each plugin is a separate framework) ├── Profile/ │ ├── Flutter.framework │ ├── App.framework │ ├── FlutterPluginRegistrant.framework │ └── example_plugin.framework └── Release/ ├── Flutter.framework ├── App.framework ├── FlutterPluginRegistrant.framework └── example_plugin.framework Flutter.framework有400M-500M，并且不会经常变动，下文的pod lib处理中我们需要为它单独创建一个pod，剩下的framwork每次新增插件或变更代码都会跟着变动，放在一个pod。 cocoapods+git管理podfile无法直接引用framework，需要一个pod作为中转。首先我们需要依次进行如下操作： 创建一个名为flutter-plugin-lib私有库 pod lib create flutter-plugin-lib 打开flutter-plugin-lib.podspec，在end前加入： s.ios.vendored_frameworks = 'flutter_frameworks/App.framework', 'flutter_frameworks/FlutterPluginRegistrant.framework', 'flutter_frameworks/shared_preferences.framework' 在根目录创建flutter_frameworks文件夹，把刚生成Release的framework除去Flutter.framework cv过来。 创建git仓库并上传到云端。 git initgit remote add origin http://xxx/flutter-plugin-lib.gitgit add .git commit -m &quot;xxx&quot;git push -u origin master 在需要混编Flutter的native项目的podfile中添加： pod 'flutter-plugin-lib', :git =&gt; 'http://xxx/flutter-plugin-lib' :tag =&gt; 'v1.0.0' 同步cocoapods pod install Flutter.framework处理方案如上。 至此，完成了引入所有的frameworks，并且不需要组内其他成员在本地安装Flutter SDK和Cocoapods。 验证AppDelegate.swift import UIKitimport Flutterimport FlutterPluginRegistrant@UIApplicationMainclass AppDelegate: FlutterAppDelegate { lazy var flutterEngine = FlutterEngine(name: &quot;sup flutter engine&quot;) override func application(_ application: UIApplication,didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -&gt; Bool { flutterEngine.run() GeneratedPluginRegistrant.register(with: self.flutterEngine) return super.application(application, didFinishLaunchingWithOptions: launchOptions) }} ViewController.swift import UIKitimport Flutterclass ViewController: UIViewController { override func viewDidLoad() { super.viewDidLoad() let button = UIButton(type: UIButton.ButtonType.custom) button.addTarget(self, action: #selector(showFlutter), for: .touchUpInside) button.setTitle(&quot;Show Flutter!&quot;, for: UIControl.State.normal) button.frame = CGRect(x: 80.0, y: 200.0, width: 160.0, height: 40.0) button.backgroundColor = UIColor.blue self.view.addSubview(button) self.view.backgroundColor = UIColor.white } @objc func showFlutter() { present(FlutterViewController(engine: (UIApplication.shared.delegate as! AppDelegate).flutterEngine, nibName: nil, bundle: nil),animated: true,completion: nil) }} Android-以aar的形式上传到私有maven/本地依赖创建Flutter module你可以在上述iOS所创建的module中继续开发。 产物集成flutter build aar 产物目录结构如下： build/host/outputs/repo└── com └── example └── my_flutter ├── flutter_release │ ├── 1.0 │ │ ├── flutter_release-1.0.aar │ │ ├── flutter_release-1.0.aar.md5 │ │ ├── flutter_release-1.0.aar.sha1 │ │ ├── flutter_release-1.0.pom │ │ ├── flutter_release-1.0.pom.md5 │ │ └── flutter_release-1.0.pom.sha1 │ ├── maven-metadata.xml │ ├── maven-metadata.xml.md5 │ └── maven-metadata.xml.sha1 ├── flutter_profile │ ├── ... └── flutter_debug └── ... maven管理你可以按照google DOCS 中描述的方式在本地依赖aar。 android { // ...}repositories { maven { url 'somepath/my_flutter/build/host/outputs/repo' // This is relative to the location of the build.gradle file // if using a relative path. } maven { url 'https://storage.googleapis.com/download.flutter.io' }}dependencies { // ... debugImplementation 'com.example.flutter_module:flutter_debug:1.0' profileImplementation 'com.example.flutter_module:flutter_profile:1.0' releaseImplementation 'com.example.flutter_module:flutter_release:1.0'} 但是这样对native开发组员来说接入成本还是过高，于是我考虑到用maven私服管理的方式处理依赖，当然首先公司应该有服务器，以下基于mac搭建maven仓库： brew install nexusnexus start&gt;&gt;http://localhost:8081/nexus/nexus stop 在flutter module中编写sh脚本处理上传动作： #!/bin/bashflutter cleanflutter build aar# 定义用于aar、pom文件目录存放的数组aars=()poms=()# 指定打包后本地仓库的目录，由于这里将此脚本放在flutter module根目录，因此直接配置了flutter module根目录下相对目录targetPath=&quot;build/host/outputs/repo&quot;# 定义遍历找到所有pom文件和aar文件的函数# 参数$1：当前查找的目录名function findAarPom(){ echo &quot;查找此目录是否有aar及pom：$1&quot; targetDir=`ls $1` for fileName in $targetDir do if [[ -d $1&quot;/&quot;$fileName ]]; then # 还是目录，则递归找下一级 findAarPom $1&quot;/&quot;$fileName else # 如果是文件，判断后缀，如果符合期望，则将文件路径拼接好放于对应数组最后一位 if [[ ${fileName:0-4} == '.aar' ]]; then aars[${#aars[@]}]=$1&quot;/&quot;$fileName elif [[ ${fileName:0-4} == '.pom' ]]; then poms[${#poms[@]}]=$1&quot;/&quot;$fileName fi fi done}findAarPom $targetPathecho &quot;============&quot;echo &quot;aar有：《共${#aars[@]}个》&quot;echo &quot;${aars[@]}&quot;echo &quot;pom有：《共${#poms[@]}个》&quot;echo &quot;${poms[@]}&quot;echo &quot;============&quot;# 一个aar文件必然对应会有一个pom文件，如果数量不对，一定是打包出错if [[ ${#aars[@]} -ne ${#poms[@]} ]]; then echo &quot;-- !!! pom文件与aar不对称，请检查aar打包配置，上传任务 退出 !!! --&quot; exit 1fiif [[ ${#aars[@]} == 0 ]]; then echo &quot;-- !!! 未找到aar文件，请检查aar打包配置，上传任务 退出 !!! --&quot; exit 1fi# 定义将目标pom及aar上传到maven指定仓库的函数# 参数$1：为pom文件# 参数$2：为aar文件function upload(){ echo &quot;开始上传：&quot; echo $1 echo $2 # mvn上传命令，这里由于将上传用户名密码配置于全局maven settings.xml，则无需再指定用户名密码 mvn deploy:deploy-file \\ -DpomFile=&quot;$1&quot; \\ -DgeneratePom=false \\ -Dfile=&quot;$2&quot; \\ -Durl=&quot;http://localhost:8081/nexus/content/repositories/releases&quot; \\ -DrepositoryId=&quot;nexus&quot; \\ -Dpackaging=aar \\ -s=&quot;mvn-settings.xml&quot;}# 循环上传for (( i=0;i&lt;${#aars[@]};i++ )); do echo &quot;正在处理第$[$i+1]个，共${#aars[@]}个&quot; upload &quot;${poms[$i]}&quot; &quot;${aars[$i]}&quot;done 其中mvn-settings.xml是单独对当前项目配置mvn，修改admin password为maven的账号密码，默认为admin/admin123 在Android项目app模块中的build.gradle android { // ... buildTypes { ... profile { initWith debug } } compileOptions { sourceCompatibility JavaVersion.VERSION_1_8 targetCompatibility JavaVersion.VERSION_1_8 }}repositories { maven { url 'http://localhost:8081/nexus/content/repositories/releases' // This is relative to the location of the build.gradle file // if using a relative path. } maven { url 'https://storage.googleapis.com/download.flutter.io' }}dependencies { // ... debugImplementation 'com.example.flutter_module:flutter_debug:1.0' profileImplementation 'com.example.flutter_module:flutter_profile:1.0' releaseImplementation 'com.example.flutter_module:flutter_release:1.0'} 如此即可完成maven私服上传，如果你有服务器可以修改ip地址完成上传、依赖。由于我司没有私服，于是决定把host/文件夹拖入项目的方式实现依赖： 根目录build.gradle: buildscript { ... repositories { ... maven { url '../host/outputs/repo' } maven { url 'https://storage.flutter-io.cn/download.flutter.io' } }}allprojects { repositories { ... maven { url '../host/outputs/repo' } maven { url 'https://storage.flutter-io.cn/download.flutter.io' } }} 至此，完成了引入所有的aar，并且不需要组内其他成员在本地安装Flutter SDK。网上也有通过fat-aar来合并aar包进行依赖处理的方法，我认为对于迭代频繁的新技术应该尽量用官方推荐的方式去实现，以免之后的迭代、版本升级带来不必要的开发成本。 验证App.kt class App : Application() { override fun onCreate() { super.onCreate() FlutterMain.startInitialization(this) }} FlutterActivity.kt class FlutterActivity : FlutterFragmentActivity() { var engine:FlutterEngine? = null override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) engine = FlutterEngine(this) engine?.dartExecutor?.executeDartEntrypoint(DartExecutor.DartEntrypoint.createDefault()) setContentView(R.layout.activity_flutter) fv_container.attachToFlutterEngine(engine!!) } override fun onResume() { super.onResume() engine?.lifecycleChannel?.appIsResumed() } override fun onPause() { super.onPause() engine?.lifecycleChannel?.appIsInactive() } override fun onStop() { super.onStop() engine?.lifecycleChannel?.appIsPaused() } override fun onDestroy() { fv_container.detachFromFlutterEngine() super.onDestroy() }} End","link":"/2020/09/19/%E5%B0%86Flutter%E6%B7%BB%E5%8A%A0%E5%88%B0%E5%B7%B2%E6%9C%89%E7%9A%84%E9%A1%B9%E7%9B%AE%E4%B8%AD/"},{"title":"工厂方法模式","text":"利用Kotlin中的伴生对象增强工厂模式 一些地方会把工厂模式细分为简单工厂、工厂方法模式以及抽象工厂,一下分类说明： 简单工厂简单工厂的核心作用就是通过一个工厂类隐藏对象实例的创建逻辑，而不是暴露给客户端。典型的使用场景就是当拥有一个父类与多个子类的时候，我们可以通过这种模式来创建子类对象。 例如现在有一个电脑加工厂，同时生产个人电脑和服务器主机。用java中的思维逻辑来实现工厂模式 interface Computer { val cpu: String}class PC(override val cpu: String = &quot;Core&quot;): Computerclass Server(override val cpu: String = &quot;Xeon&quot;): Computerenum class ComputerType { PC,Server}class ComputerFactory { fun produce(type: ComputerType): Computer { return when(type) { ComputerType.PC -&gt; PC() ComputerType.Server -&gt; Server() } }} 以上代码通过调用ComputerFactory类的produce方法来创建不同的Computer子类对象，这样我们就把创建实例的逻辑与客户端之间实现解耦，当对象创建的逻辑发生变化时，该模式只需要修改produce函数内部的代码即可，相比直接创建对象的方法更加利于维护。 虽然它改善了程序的可维护性，但创建对象的的表达上却显得不够简洁。当我们在不同的地方创建Computer的子类对象时，我们都需要先创建一个ComputerFactory类对象，而Kotlin天生支持了单例，接下来我们就用object关键字以及相关的特性来进一步简化以上的代码设计。 用单例代替工厂类Kotlin支持用object来实现Java中的单例模式，所以我们可以实现一个ComputerFactory单例，而不是一个工厂类。 object ComputerFatory { fun produce(type: ComputerType): Computer { return when(type) { ComputerType.PC -&gt; PC() ComputerType.Server -&gt; Server() } }}//调用ComputerFactory.produce(ComputerType.PC) 此外，由于我们通过传入Computer类型来创建不同的对象，所以这里的produce又显得多余，而Kotlin支持运算符重载，因此我们可以通过operator操作符重载invoke方法来代替produce，从而进一步简化表达： object ComputerFactory { operator fun invoke(type: ComputerType): Computer { return when(type) { ComputerType.PC -&gt; PC() ComputerType.Server -&gt; Server() } }}//调用ComputerFactory(ComputerType.PC) 依靠Kotlin这一特性，我们再创建一个Computer对象就显得非常简洁，与直接创建一个具体类实例显得没有太大区别。 伴生对象创建静态工厂方法当前的工厂模式实现已经足够优雅，而你依旧觉得不够完美：我们是否可以直接通过Computer()而不是ComputerFactory()来创建一个实例呢？ 《Effective Java》一书的第一条指导原则：考虑用静态工厂方法代替构造器。 Kotlin中的伴生对象，代替了Java中的static，同时在功能和表达上拥有更强的能力。通过在Computer接口中定义一个伴生对象，我们就能实现以上的需求： interface Computer { val cpu: String companion object Factory { operator fun invoke(type: ComputerType): Computer { return when(type) { ComputerType.PC -&gt; PC() ComputerType.Server -&gt; Server() } } }}//调用Computer.Factory(ComputerType.PC) 扩展伴生对象方法依赖伴生对象的特性，已经实现了经典的工厂模式。同时这种方式还有一个优势，它比原有Java中的设计更加强大。假如实际业务中我们是Computer借口的使用者，比如它是工程引入的第三方类库，所有类的实现细节都得到了很好地隐藏。那么，如果希望进一步改造其中的逻辑，Kotlin中伴生对象的方式同样可以依靠其扩展函数的特性，很好地实现这一需求。 比如我们需要给Computer添加一种功能，通过CPU型号来判断电脑类型，那么久可以如下实现： fun Computer.Companion.fromCPU(cpu: String): ComputerType? { return when (cpu) { &quot;Core&quot; -&gt; ComputerType.PC &quot;Xeon&quot; -&gt; ComputerType.Server else -&gt; null }} 如果指定了伴生对象的名字为Factory,那么就可以如下实现： fun Computer.Factory.fromCPU(cpu: String): ComputerType? = ... 内联函数简化抽象工厂工厂模式已经能够很好地处理一个产品等级结构的问题，上述简单工厂已经解决了电脑厂商生产服务器、PC机的问题。进一步思考，当问题上升到多个产品等级结构的时候，比如现在引入了品牌商的概念，我们有好几个不同的电脑品牌，比如Dell,Asus,Acer，那么就有必要再增加一个工厂类。然而，我们并不希望对每个模型都建立一个工厂，这会让代码变得难以维护，所以这时候需要引入抽象工厂模式。 抽象工厂模式为创建一组相关或相互依赖的对象提供一个接口，而且无须指定它们的具体类。 在抽象工厂的定义中，我们也可以把“一组相关或相互依赖的对象”称作“产品族”，在上述的例子中，我们就提到了3个代表不同电脑品牌的产品族。下面我们就利用抽象工厂，来实现具体需求： interface Computerclass Dell: Computerclass Asus: Computerclass Acer: Computerclass DellFactory: AbstractFactory() { override fun produce() = Dell()}class AsusFactory: AbstractFactory() { override fun produce() = Asus()}class AcerFactory: AbstractFactory() { override fun produce() = Acer()}abstract class AbstractFactory { abstract fun produce(): Computer companion object { operator fun invoke(factory: AbstractFactory) = factory }}//调用AbstractFactory(DellFactory()).produce() 每个电脑品牌拥有一个代表电脑产品的类，它们都实现了Computer接口。此外每个品牌也还有一个用于生产电脑的AbstractFactory子类，可通过AbstractFactory类的伴生对象中的invoke方法，来构造具体品牌的工厂类对象。 由于Kotlin语法的简洁，以上例子的抽象工厂类的设计也比较直观。然而，当你每次创建具体的工厂类时，都需要传入一个具体的工厂类对象作为参数进行构造，这个在语法上显然不是很优雅。所以，我们可以用内联函数来改善这一情况： abstract class AbstractFactory { abstract fun produce(): Computer companion object { inline operator fun &lt;reified T : Computer&gt; invoke() = when(T::class) { Dell::class -&gt; DellFactory() Asus::class -&gt; AsusFactory() Acer::class -&gt; AcerFactory() else -&gt; throw IllegalArgumentException() } }}//调用AbstractFactory&lt;Dell&gt;().produce() 通过将invoke方法定义为内联函数，我们就可以引入reified关键字，使用具体化参数类型的语法特性 要具体化的参数类型为Computer,在invoke方法中我们通过判断它的具体类型，来返回对于的工厂类对象 参考: Dive in Kotlin","link":"/2020/01/17/%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/"},{"title":"教诲","text":"演员这个职业特别残忍，因为你是被挑选的，人家没人来找你，你就没活干。因为我做了37年的演员，大概从三十四五岁开始，就没有人找我演戏了，然后你眼看着你周围的朋友，这个红了，那个红了。一点不重要，因为你要从事的行业不是一年两年，你要从事一生。 &amp;emsp;&amp;emsp;演员这个职业特别残忍，因为你是被挑选的，人家没人来找你，你就没活干。因为我做了37年的演员，大概从三十四五岁开始，就没有人找我演戏了，然后你眼看着你周围的朋友，这个红了，那个红了。一点不重要，因为你要从事的行业不是一年两年，你要从事一生。&amp;emsp;&amp;emsp;所以，能够做一个不急不躁的演员，永远谦逊，看得见真正棒的人，欣赏得了真正好的表演，永远不去比拼粉丝，你一定会成为一个大演员的，你很棒!","link":"/2017/11/26/%E6%95%99%E8%AF%B2/"},{"title":"从自己身上找问题或许是一种妥协","text":"问题一定存在，没有完人，我常从自己身上找问题。问题常常存在，但不是非黑即白，不代表一方有问题. 问题所在问题一定存在，没有完人，我常从自己身上找问题。问题常常存在，但不是非黑即白，不代表一方有问题，另一方就万事大吉，往往是一个巴掌拍不响的事实。可是结果往往是，方桌的那一边让自己高高在上，毫无污点，占领道德的至高峰，开始狂轰滥炸，每每善良的承认过失后，带来的结果往往不堪设想。 为什么会出现这种局面，问题所在何处？ 每个人都是一个完整的个体，人出现在这个世界上，就开始为了活四处奔波，为了喝到第一口奶拼命哭喊，为了摆脱现有阶级努力读书，为了繁育后代结婚生子，为了养活家人在世上受尽委屈。完整的个体应该有一套独特的自我防御机制，兵来将挡水来土掩。自我成长应该是隐藏在外壳里的，外壳应该是圆滑坚硬的，不应该透漏出任何弱点。因为一旦弱点出现，就会受到外部的猛烈攻击，这是丛林法则，不容质疑。 那现在知道问题所在了，自我反省是不可或缺的，反省的过程，改正的过程正是自我成长的过程，也就是在外壳里的自我脱变。蜕变只属于自己，一旦被外界获知，特别是利益相关，轻则人格受到侮辱，失去信任，丢掉工作，重则自我防御机制被攻破，无法自我修复，终生抑郁不得志。 那么得到的是一个世俗的结果： 披着虚伪的皮在这世间行走，以后，柔软的一面只属于自己，直到面具长进肉里，深入灵魂。不必沮丧，自己承受这一切，照顾好身边的亲人，祝你活出自己。","link":"/2018/02/02/%E4%BB%8E%E8%87%AA%E5%B7%B1%E8%BA%AB%E4%B8%8A%E6%89%BE%E9%97%AE%E9%A2%98%E6%88%96%E8%AE%B8%E6%98%AF%E4%B8%80%E7%A7%8D%E5%A6%A5%E5%8D%8F/"},{"title":"无题","text":"自我羞耻感在你心底闪闪发光知道比孤独可怕千万倍我对每个人都友善，却从未切实感受过“友情”差自己的一句抱歉，说给了世界 ————《人间失格》太宰治","link":"/2020/02/25/%E6%97%A0%E9%A2%98/"},{"title":"用偏函数实现责任链模式","text":"责任链模式的目的就是避免请求的发送者和接收者之间的耦合关系，将这个对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。 假如你遇到这样的业务需求场景：希望使得多个对象都有机会处理某种类型的请求，那么可能就需要考虑是否可以采用责任链模式。 典型的例子就是Servlet中的Filter和FilterChain接口，它们就采用了责任链模式。利用责任链模式我们可以在接收到一个Web请求时，先进行各种filter逻辑的操作，filter都处理完之后才执行servlet。在这个例子中，不同的filter代表了不同的职责，最终它们形成了一个责任链。 简单来说，责任链模式的目的就是避免请求的发送者和接收者之间的耦合关系，将这个对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。 现在举一个更具体的🌰。计算机学院的学生会管理了一个学生会基金，用于各种活动和组织人员工作的开支。当要发生一笔支出时，如果金额在100元之内，可由各个分部长审批；如果金额超过了100元，那么就需要会长同意；但假使金额较大，达到了500元以上，那么就需要学院的辅导员陈老师批准。此外，学院里还有一个不宣的规定，经费的上限为1000元，如果超出则默认打回申请。 当然我们可以用最简单的if-else来实现经费审批的需求。然而根据开闭原则，我们需要将其中的逻辑进行解耦。下面我们就用面向对象的思路结合责任链模式，来设计一个程序。 data class ApplyEvent(val monkey: Int, val title: String)interface ApplyHandler { val successor: ApplyHandler? fun handleEvent(event: ApplyEvent)}class GroupLeader(override val successor: ApplyHandler?): ApplyHandler { override fun handleEvent(event: ApplyEvent) { when { event.monkey &lt;= 100 -&gt; print(&quot;Group Leader handled application: ${event.title}&quot;) else -&gt; when(successor) { is ApplyHandler -&gt; successor.handleEvent(event) else -&gt; print(&quot;Group Leader: This application cannot be handdle&quot;) } } }}class President(override val successor: ApplyHandler?) :ApplyHandler { override fun handleEvent(event: ApplyEvent) { when { event.monkey &lt;= 500 -&gt; print(&quot;President handled application: ${event.title}&quot;) else -&gt; when(successor) { is ApplyHandler -&gt; successor.handleEvent(event) else -&gt; print(&quot;President: This application cannot be handdle.&quot;) } } }}class College(override val successor: ApplyHandler?) :ApplyHandler { override fun handleEvent(event: ApplyEvent) { when { event.monkey &gt; 1000 -&gt; print(&quot;College: This application is refused.&quot;) else -&gt; print(&quot;College handled application: ${event.title}.&quot;) } }} 我们声明了GroupLeader,President,College三个类来代表学生会部长，分会长，会长及学院，它们都实现了ApplyHandler接口。接口包含了一个可空的后继者对象successor，以及对申请事件的处理方法handleEvent。 当我们把一个申请经费的事件传递给GroupLeader对象进行处理时，它会根据具体的经费金额来判断金额来判断是否将申请转交给successor对象，也就是President类来处理。以此类推，最终形成了一个责任链机制： st=&gt;start: 经费申请op1=&gt;operation: 学生部长op2=&gt;operation: 分会长op3=&gt;operation: 会长op4=&gt;operation: 学院cond1=&gt;condition: 是否小于100元cond2=&gt;condition: 是否小于500元cond3=&gt;condition: 是否小于1000元e1=&gt;end: 申请成功e2=&gt;end: 申请失败st-&gt;cond1cond1(yes)-&gt;op1-&gt;e1cond1(no)-&gt;cond2cond2(yes)-&gt;op2-&gt;e1cond2(no)-&gt;cond3cond3(yes)-&gt;op3-&gt;e1cond3(no)-&gt;e2 fun main(args: Array&lt;String&gt;) { val College = College(null) val president = President(college) val groupLeader = GroupLeader(president) groupLeader.handleEvent(ApplyEvent(10,&quot;buy a pen&quot;)) groupLeader.handleEvent(ApplyEvent(200,&quot;team building&quot;)) groupLeader.handleEvent(ApplyEvent(600,&quot;hold a debate match&quot;)) groupLeader.handleEvent(ApplyEvent(1200,&quot;annual meeting of the college&quot;))}//resultGroup Leader handled application: buy a pen.President handled application: team building.College handled applicatioon: hold a debate match.College: This application is refused. 梳理一下责任链的机制，整个链条的每个处理环节都有对其输入参数的效验标准，当输入参数处于某个责任链环节的有效接收范围之内，该环节才能对其作出正常的处理操作。在编程语言中，我们有一个专门的术语来描述这种情况，这就是“偏函数”。 实现偏函数类型：PartialFunction那，什么是偏函数呢？ 偏函数是数学中的概念，指的是定义域X中可能存在某些值在值域Y中没有对应的值。 为了方便理解，我们可以把偏函数与普通函数进行比较。在一个普通函数中，我们可以给指定类型的参数传入任意该类型的值，比如(Int)-&gt;Unit，可以接收任何Int值。而在一个偏函数中，指定类型的参数并不接收任意该类型的值，比如： fun mustGreaterThan5(x: Int): Boolean { if (x &gt; 5) { return true } throw Exception(&quot;x must be greator than 5&quot;)}&gt;&gt;&gt; mustGreatorThan5(6)true&gt;&gt;&gt; mustGreatorThan5(1)java.lang.Exception: x must be greatoor than 5 at Line17.mustGreatorThan5(Unknow Source) 之所以提高偏函数是因为在一些函数式编程语言中，如Scala，有一种PartialFunction类型，我们可以用它来简化责任链模式的实现。由于Kotlin的语言特性足够灵活强大，虽然它的标准库没有支持PartialFunction，然而一些开源库（如Arrow）已经实现了这个功能。我们来定义一个PartialFunction类型： class PartialFunction&lt;in P1, out R&gt; ( private val definetAt: (P1) -&gt; Boolean, private val f: (P1) -&gt; R) : (P1) -&gt; R { override fun invoke(p1: P1): R { if (definetAt(p1)) { return f(p1) } throw IllegalArgumentException(&quot;Value: ($p1) isn't supported by this function&quot;) } fun isDefinetAt(p1: P1) = definetAt(p1)} 现在来分析下PartialFunction类的具体作用： 声明类对象时需要接收两个构造参数，definetAt为效验函数，f为处理函数 当PartialFunction类对象执行invoke方法时，definetAt会对输出参数p1进行有效性效验 如果效验结果通过，则执行f函数，同时将p1作为参数传递给它，反之抛出异常 如上PartialFunction类已经可以处理责任链模式中各个环节对于输入的效验及处理逻辑的问题，但是依旧有一个问题，就是如何将请求在整个链条中进行传递。 接下来我们利用Kotlin的扩展函数给PartialFunction类增加一个orElse方法。在此之前，我们先注意下这个类中的isDefinedAt方法，它其实并没有什么特殊之处，仅仅只是作为拷贝definetAt的一个内部方法，为了在orElse方法中能够被调用。 infix fun &lt;P1, R&gt; PartialFunction&lt;P1, R&gt;.orElse(that: PartialFunction&lt;P1, R&gt;): PartialFunction&lt;P1, R&gt; { return PartialFunction({ this.isDefinedAt(it) || that.isDefinedAt(it) }) { when { this.isDefinedAt(it) -&gt; this(it) else -&gt; that(it) } }} 在orElse方法中可以传入另一个PartialFunction类对象that，它也就是责任链模式中的后继者。当isDefinedAt方法执行结果为false的时候，那么就调用that对象来处理申请。 这里用infix关键字来让orElse成为一个中辍函数，从而让链式调用变得更加直观。 用orElse构建责任链接下来我们就用设计好的PartialFunction类及扩展的orElse方法，来重新实现以下最开始的例子。首先来看看如何用PartialFunction定义groupLeader对象： data class ApplyEvent(val money: Int, val title: String)val groupLeader = { val definetAt: (ApplyEvent) -&gt; Boolean = { it.money &lt;= 200 } val handler: (ApplyEvent) -&gt; Unit = { println(&quot;Group Leader handled application: ${it.title}.&quot;) } PartialFunction(definetAt, handler)}() 这里我们借助了自运行Lambda的语法来构建一个PartialFunction的对象groupLeader。definetAt用于效验申请的经费金额是否在学生会部长可审批的范围之内，handler函数用来处理童年各国金额效验后的审批操作。 同理，我们用类似的方法再定义剩下的president和college对象： val president = { val definetAt: (ApplyEvent) -&gt; Boolean = { it.money &lt;= 500 } val handler: (ApplyEvent) -&gt; Unit = { println(&quot;President handled application: ${it.title}.&quot;) } PartialFunction(definetAt, handler)}()val college = { val definetAt: (ApplyEvent) -&gt; Boolean = { true } val handler: (ApplyEvent) -&gt; Unit = { when { it.money &gt; 1000 -&gt; println(&quot;College: This application is refused.&quot;) else -&gt; println(&quot;College handled application: ${it.title}.&quot;) } } PartialFunction(definetAt,handler)} 最后我们用orElse来构建一个基于责任链模式和PartialFunction类型的中辍表达式applyChain: val applyChain = groupLeader orElse president orElse college&gt;&gt;&gt; applyChain(ApplyEvent(600,&quot;hold a debate match&quot;))College handled application: hold a debate match 借助PartialFunction类的封装，我们不仅大幅度减少了程序的代码量，而且在构建责任链时，可以用orElse获得更好的语法表达。","link":"/2020/01/19/%E7%94%A8%E5%81%8F%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E8%B4%A3%E4%BB%BB%E9%93%BE%E6%A8%A1%E5%BC%8F/"},{"title":"生而为人，对不起","text":"不要吸食香烟，若非节日，也别饮酒，长大后，请多加爱惜那性格内向，不爱浓妆的姑娘。 &amp;emsp;&amp;emsp;“从小了解了人类的种种卑劣之处，对这种卑劣深深厌恶，但他随后发现自己身上也有这种卑劣，进而对自己也产生了这种厌恶。但是，叶藏并没有力量去阻止，甚至没有力量去努力，只好随波逐流，做一个被动的人。为了脱离这种罪恶感，他的精神领域里产生了一种自我切割现象，把这些卑劣，具现化成了那个黑色的小人。那个小人，可以说是叶藏自己的罪，但也可以说是叶藏自己。一般人，其实是终生和小人共生下去的，但叶藏对于内心世界有很高的追求，希望把这件事情思考透彻，所以最后只好陷入了极端，选择放弃自己的生命。其实，了解到人性本恶这一点，其实不一定要走向这样的极端，不如放弃掉对人性的虚假预期，想着怎样在每件事情上努力变好，可能是更积极的做法。不过，人并不能总是这么坚强与阳光，所以叶藏并不难取得读者某种程度上的共情，因为实际上很多人都曾经尝试过像叶藏这样思考。只是，最后不一定会选择这么极端的方式。一部文学作品如果能够引起广泛的人的共情，那么就是有价值的。只是，我这边任然希望读者看到这部作品以后，是有“别人也有同样经历，能够理解自己”的治愈，而不要在这种厌恶的情绪中继续发展。叶藏纤细而罪恶的灵魂，因为他本身的自我反省和罪恶感，连同他的诚实和不愿意伤害任何人的出发点，具有一种值得称颂的价值，这也是最后说“如同神一样的好孩子”的原因。无论如何，人的生命宝贵，千万要珍惜。人性不美好，却正是我们每天努力的价值，黑夜给了我黑色的眼睛，我却用他去寻找光明。” &amp;emsp;&amp;emsp;大多数人连选择极端方式的权利都没有，碌碌终生，浑浑噩噩。我有敏感的内心，能够看到种种卑劣，却以“too low”的方式在思维层面藐视一切，仿佛自己看透世事，不争不抢。却是一种妥协，更如同阿Q一般的懦弱，真正的坦然应该如同钻石一般璀璨，自顾独自散发光辉，晶莹剔透，却坚韧不摧。 &amp;emsp;&amp;emsp;要是有一颗美好的种子被风吹起，应该放任他随风飘落，风儿停下来的地方便是他的家，一定要饱受风吹雨打，风越狂，根茎越是深入土壤，雨越大，树枝更加苍劲有力。 哎，真是树欲止而风不净静！ &amp;emsp;&amp;emsp;要是一只鸟，叫得像鸭子，游得像鸭子，走得也像鸭子，长着羽毛也有脚蹼，还和鸭子一起，那当然会判断这只鸟就是一只鸭子，对吗？ &amp;emsp;&amp;emsp;人性不美好，却正是我们每天努力的价值，黑夜给了我黑色的眼睛，我却用他去寻找光明。","link":"/2019/01/27/%E7%94%9F%E8%80%8C%E4%B8%BA%E4%BA%BA%EF%BC%8C%E5%AF%B9%E4%B8%8D%E8%B5%B7/"},{"title":"第一篇","text":"我这种应该是不会去写博客的，自己的印象笔记也是信手捏来，想到拿就写到拿，凌乱的很。恰好看到有人用Hexo，就弄一个玩玩。不过只支持md，正好Bear下下来从来没有用过，借此拿出来用一下。希望坚持下去多总结，多分享。","link":"/2017/11/23/%E7%AC%AC%E4%B8%80%E7%AF%87/"},{"title":"运算符重载和迭代器模式","text":"所谓迭代器，它的核心作用就是将遍历和实现分离开来，在遍历的同时不需要暴露对象的内部表示。 迭代器（iterator）是Java中我们非常熟悉的东西了，数据结构如List和Set都内置了迭代器，我们可以用它提供的方法来顺序地访问一个聚合对象中每个元素。 有时候，我们会定义某些容器类，这些类中包含了大量相同类型的对象。如果你想给这个容器类的对象直接提供迭代的方法，如hasNext,next,first等，那么就可以自定义一个迭代器。然而通常情况下，我们不需要自己再实现一个迭代器，因为Java标准库提供了java.util.Iterator接口，你可以用容器类实现该接口，然后再实现需要的迭代器方法。 这种设计模式就是迭代器模式，它的核心作用就是将遍历和实现分离开来，在遍历的同时不需要暴露对象的内部表示。迭代器模式非常容易理解，你可能已经非常熟悉。但我们还是举个具体的例子来介绍下这种模式，接着引出Kotlin中相关的语法特性，继而进行改良。 方案1：实现Iterator接口data class Book(val name: String)class Bookcase(val books: list&lt;Book&gt;): Iterator&lt;Book&gt; { private val iterator: Iterator&lt;Book&gt; init { this.iterator = books.iterator() } override fun hasNext() = this.iterator.hasNext() override fun next() = this.iterator.next()}//invokeval bookcase = Bookcase(listOf(Book(&quot;Dive into Kotlin&quot;),Book(&quot;Thinking in Java&quot;)))while(bookcase.hasNext()) { println(&quot;The book is ${bookcase.next().name}&quot;)}The book name is Dive into KotlinThe book name is Thinking in Java 由于Bookcase对象拥有与List实例相同的迭代器，我们就可以直接调用后者迭代器所有的方法。一种更简洁的遍历打印方式如下： for(book in bookcase) { println(&quot;The book name is ${book.name}&quot;)} 方案2：重载iterator方法Kotlin却有更好的解决方案。Kotlin有一个非常强大的语言特性，那就是利用operator关键字内置了很多运算符重载功能。我们就可以通过重载Bookcase类的iterator方法，实现一种语法上更加精简的版本： data class Book(val name: String)class Bookcase(val books:List&lt;Book&gt;) { operator fun iterator(): Iterator&lt;Book&gt; = this.books.iterator()} 这样我们用一行代码就实现了以上所有效果。由于Kotlin支持扩展函数，这意味着我们可以给所有的对象都内置一个迭代器。 方案3：通过扩展函数假设现在的Book是引入的一个类，你并不能修改它的源码，那么如何用扩展的语法来给Bookcase类对象增加迭代的功能： data class Book(val name: String) {}class Bookcase(val Books: list&lt;Book&gt;) {}operator fun Bookcase.iterator(): Iterator&lt;Book&gt; = books.iterator() 代码依旧非常简洁，假如想对迭代器的逻辑有更多的控制权，那么也可以通过object表达式来实现： operator fun Bookcase.iterator():Iterator&lt;Book&gt; = object: Iterator&lt;Book&gt; { val iterator = books.iterator() override fun hasNext() = iterator.hasNext() override fun next() = iterator.next()} 总的来说，迭代器模式并不是一种很常用的设计模式，但通过它我们可以进一步了解Kotlin中的扩展函数的应用，以及运算符重载功能的强大之处。","link":"/2020/01/19/%E8%BF%90%E7%AE%97%E7%AC%A6%E9%87%8D%E8%BD%BD%E5%92%8C%E8%BF%AD%E4%BB%A3%E5%99%A8%E6%A8%A1%E5%BC%8F/"},{"title":"github访问及clone过慢问题","text":"墙相关 前言Github是开源世界的一扇大门，伟大的墙对GitHub网开一面，没有像对Google那样直接斩尽杀绝，但是对它做了严格的限速。git clone（有些地区较快，有些地区较慢）；但总体来说,基本都在10KiB/s-40KiB/s之间。在拉取Flutter源码时出现5-9KiB/s的情况，所以有了这篇文章 修改端口 首先需要打开ShadowSocks，我不会配置全局模式，一是每月流量问题，二是访问一些国内网站反而会慢，所以对于PAC自动模式来说可以如下配置 git config --global http.https://github.com.proxy https://127.0.0.1:xxxxgit config --global https.https://github.com.proxy https://127.0.0.1:xxxx 其中端口号，可以在ShadowSocks上查找socks5端口，然后就享受40M/s的畅快吧。 指定IP如果修改端口的方法也无济于事，还可以通过指定host的方式，指定最近dns服务商以达到快速的效果。 DNS工作方式当我们访问github.com时，浏览器并不知道这个域名对应的真实ip地址，最先回去询问本地dns缓存，是否认识这个域名的门牌号，如果不认识接着往上问，当地运行商也不认识这个域名的话，继续问上级，知道问出来github.com的门牌号为止！这个问路过程称为DNS寻址，如果问路的时间过长，那么返回速度自然很慢。那么如果我们直接告诉浏览器目的地，那么浏览器也就不会一步步去问路了，所以也就能达到加快访问速度的要求。 正常来说，网站的主域名下会存在多个子域名，由这些域名组合在一起提供完整的服务。所以我们不仅要告诉本机github.com的主域名，还要把相关的子域名也告诉本机。就像现实中，每个人都有自己的家，而这个家有具体的地址，一些有钱人可能有几个家。 对应到计算机世界中，如果域名是用户，那么ip地址就是用户的家，同一个域名可以对应多个ip地址，同一个ip地址也可以有多多个域名。域名到ip地址的过程同样需要找人询问，这个信息一般会存在dns服务商那里，就像我们的地址登记到相关机构一样. 子域名信息ipaddress.com 网站为例，查询下 github.com 网站的相关信息。得到如下域名信息。 就近cdn加速大型网站服务器都不会只有一台服务器，而是多台服务器组成集群一起对外提供服务。全世界都在使用 ** github，如果每次都访问美国服务器，即使浏览器知道目的地，但是距离太远还是会很慢，所以能够就近访问github.com**就能大幅提高访问速度了。tool.chinaz.com 网站为例，查询下 github.com 各子域名的就近地址。 所以只需要选择TTL值最小的作为优化标准，并写入/etc/hosts(macOS)文件中 # github related website 13.250.177.223 github.com 31.13.83.8 github.global.ssl.fastly.net 203.98.7.65 gist.github.com 13.229.189.0 codeload.github.com 185.199.109.153 desktop.github.com 185.199.108.153 guides.github.com 185.199.108.153 blog.github.com 18.204.240.114 status.github.com 185.199.108.153 developer.github.com 185.199.108.153 services.github.com 140.82.113.22 enterprise.github.com 34.195.49.195 education.github.com 185.199.108.153 pages.github.com 34.196.237.103 classroom.github.com 最后进行dns刷新 sudo dscacheutil -flushcache","link":"/2019/07/16/%E9%83%A8%E5%88%86%E5%9C%B0%E5%8C%BAgithub%E6%8B%89%E5%8F%96%E4%BB%A3%E7%A0%81%E5%A5%87%E6%85%A2%E9%97%AE%E9%A2%98/"},{"title":"静默安装、自启动实践","text":"最近在做门禁机内嵌app，门禁机基于api22原生系统，涉及到的技术点包括静默安装、自启动。翻阅大量源码、blog以后得到解决方案，因此记录下来。 静默安装 静默安装，即不弹窗系统安装界面的情况下，完成安装动作，在用户不知情的情况下完成安装动作显然是非常危险的行为 ，因此静默安装不会开发给开发者，不过在Google Play商店已经自定义Rom上都有开发此权限，充分说明拥有权限的重要 性，自家的系统想咋样就咋样。 权限 &lt;uses-permission android:name=&quot;android.permission.INSTALL_PACKAGES&quot; /&gt; 安装代码有用push到system/app去调用@hide实现的，有用doc命令实现的，对项目需求而言二者皆可，考虑到项目包含过多.so文件，而.so文件需要push到/system/lib赋予权限，故直接采用了第二种策略使用doc命令搞定，即（在子线程安装，避免ANR，需要root权限）： pm install -r 实现代码 /** * install slient * * @param context * @param filePath * @return 0 means normal, 1 means file not exist, 2 means other exception error */ public static int installSlient(Context context, String filePath) { File file = new File(filePath); if (filePath == null || filePath.length() == 0 || (file = new File(filePath)) == null || file.length() &lt;= 0 || !file.exists() || !file.isFile()) { return 1; } String[] args = { &quot;pm&quot;, &quot;install&quot;, &quot;-r&quot;, filePath }; ProcessBuilder processBuilder = new ProcessBuilder(args); Process process = null; BufferedReader successResult = null; BufferedReader errorResult = null; StringBuilder successMsg = new StringBuilder(); StringBuilder errorMsg = new StringBuilder(); int result; try { process = processBuilder.start(); successResult = new BufferedReader(new InputStreamReader(process.getInputStream())); errorResult = new BufferedReader(new InputStreamReader(process.getErrorStream())); String s; while ((s = successResult.readLine()) != null) { successMsg.append(s); } while ((s = errorResult.readLine()) != null) { errorMsg.append(s); } } catch (IOException e) { e.printStackTrace(); result = 2; } catch (Exception e) { e.printStackTrace(); result = 2; } finally { try { if (successResult != null) { successResult.close(); } if (errorResult != null) { errorResult.close(); } } catch (IOException e) { e.printStackTrace(); } if (process != null) { process.destroy(); } } // TODO should add memory is not enough here if (successMsg.toString().contains(&quot;Success&quot;) || successMsg.toString().contains(&quot;success&quot;)) { result = 0; } else { result = 2; } Log.d(&quot;installSlient&quot;, &quot;successMsg:&quot; + successMsg + &quot;, ErrorMsg:&quot; + errorMsg); return result; } 自启动无非就是静态广播拉起laucherActivity,不过这里需要注意的问题是，但静默安装成功后，app进程会被杀死，这个过程中自身无法接收到广播,这时候就需要辅助app了，这个app用来处理接收android.intent.action.PACKAGE_ADDED广播，并发送广播给宿主app从而实现自启动，这个过程代码是很简单的。如果想要优化的话可以去做定时轮询相互检查进程是否存活，并相互拉起。如何定时轮询，如何检查，整个策略，会在后续中编写… 参考:Trinea-Android常用代码之普通及系统权限静默安装APKpm.java","link":"/2018/05/26/%E9%9D%99%E9%BB%98%E5%AE%89%E8%A3%85%E3%80%81%E8%87%AA%E5%90%AF%E5%8A%A8%E5%AE%9E%E8%B7%B5/"},{"title":"软键盘相关","text":"Android软键盘相关处理 项目中有类似陌陌输入框的UI，因为Edittext不在输入框底部，导致软键盘弹出时显示在Edittext底部而遮挡掉输入框部分UI，目前采用的方法是对Viewtree进行监听，软键盘弹起来时，进行scrollTo移动输入框 private void configureViewTreeObserver(View root, View scrollToView) { root.getViewTreeObserver().addOnGlobalLayoutListener(() -&gt; { Rect rect = new Rect(); //获取到界面可见区域的矩形 root.getWindowVisibleDisplayFrame(rect); //根布局高度 - 可见区域高度 = 软键盘高度 int keyboardHeight = root.getRootView().getHeight() - rect.bottom; if (keyboardHeight &gt; 100) { //如果像素数大于100极大可能是软键盘弹出 mIsKeyboardOpened = true; //处理Edittext不在输入框底部的情况 int[] location = new int[2]; scrollToView.getLocationInWindow(location); //输入框在界面上Y轴坐标 + 输入框高度 - 可见区域底部 root.scrollTo(0, location[1] + scrollToView.getHeight() - rect.bottom); } else { //软键盘收起 root.scrollTo(0, 0); if (mIsKeyboardOpened) { mIsKeyboardOpened = false; DanmakuInputDialog.this.dismiss(); } } }); } ##android:windowSoftInputMode属性输入框以DialogFragment的方式进行视图叠加实现，没有用到windowSoftInputMode属性，不过这里也总结一下 &lt;activity android:windowSoftInputMode=&quot;stateVisible|adjustResize&quot;. . . &gt; activity主窗口与软键盘的交互模式，可以用来避免输入法面板遮挡问题，Android1.5后的一个新特性。这个属性能影响两件事情： 当有焦点产生时，软键盘是隐藏还是显示 是否减少活动主窗口大小以便腾出空间放软键盘 各值如下： stateUnspecified：软键盘的状态并没有指定，系统将选择一个合适的状态或依赖于主题的设置 stateUnchanged：当这个activity出现时，软键盘将一直保持在上一个activity里的状态，无论是隐藏还是显示 stateHidden：用户选择activity时，软键盘总是被隐藏 stateAlwaysHidden：当该Activity主窗口获取焦点时，软键盘也总是被隐藏的 stateVisible：软键盘通常是可见的 stateAlwaysVisible：用户选择activity时，软键盘总是显示的状态 adjustUnspecified：默认设置，通常由系统自行决定是隐藏还是显示 adjustResize：该Activity总是调整屏幕的大小以便留出软键盘的空间 adjustPan：当前窗口的内容将自动移动以便当前焦点从不被键盘覆盖和用户能总是看到输入内容的部分 ##android:imeOptinos属性Android在横屏时唤起软键盘会默认全屏展示，当使用imeOptions时可以解决问题。 &lt;Edittext android:imeOptions=&quot;flagNoExtractUi&quot;&gt; android:imeOptions=”flagNoExtractUi” //使软键盘不全屏显示，只占用一部分屏幕 同时,这个属性还能控件软键盘右下角按键的显示内容,默认情况下为回车键 android:imeOptions=”actionNone” //输入框右侧不带任何提示 android:imeOptions=”actionGo” //右下角按键内容为’开始’ android:imeOptions=”actionSearch” //右下角按键为放大镜图片，搜索 android:imeOptions=”actionSend” //右下角按键内容为’发送’ android:imeOptions=”actionNext” //右下角按键内容为’下一步’ android:imeOptions=”actionDone” //右下角按键内容为’完成’","link":"/2018/12/03/%E8%BD%AF%E9%94%AE%E7%9B%98%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Kotlin","slug":"Kotlin","link":"/tags/Kotlin/"},{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"C&#x2F;C++","slug":"C-C","link":"/tags/C-C/"},{"name":"Test automation","slug":"Test-automation","link":"/tags/Test-automation/"},{"name":"Flutter","slug":"Flutter","link":"/tags/Flutter/"},{"name":"Android Studio","slug":"Android-Studio","link":"/tags/Android-Studio/"},{"name":"Tensorflow","slug":"Tensorflow","link":"/tags/Tensorflow/"},{"name":"dart","slug":"dart","link":"/tags/dart/"},{"name":"gradle","slug":"gradle","link":"/tags/gradle/"},{"name":"View","slug":"View","link":"/tags/View/"},{"name":"adb","slug":"adb","link":"/tags/adb/"},{"name":"cocoaPods","slug":"cocoaPods","link":"/tags/cocoaPods/"},{"name":"system","slug":"system","link":"/tags/system/"},{"name":"syntactic sugar","slug":"syntactic-sugar","link":"/tags/syntactic-sugar/"},{"name":"life","slug":"life","link":"/tags/life/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"随意","slug":"随意","link":"/tags/%E9%9A%8F%E6%84%8F/"},{"name":"git","slug":"git","link":"/tags/git/"}],"categories":[{"name":"Android","slug":"Android","link":"/categories/Android/"},{"name":"WebRTC","slug":"WebRTC","link":"/categories/WebRTC/"},{"name":"Flutter","slug":"Flutter","link":"/categories/Flutter/"},{"name":"Tools","slug":"Tools","link":"/categories/Tools/"},{"name":"MediaPipe","slug":"MediaPipe","link":"/categories/MediaPipe/"},{"name":"iOS","slug":"iOS","link":"/categories/iOS/"},{"name":"Kotlin","slug":"Kotlin","link":"/categories/Kotlin/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"openGL","slug":"openGL","link":"/categories/openGL/"},{"name":"vm","slug":"vm","link":"/categories/vm/"},{"name":"life","slug":"life","link":"/categories/life/"}],"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"about","text":"I'm Hanniballol! All work and no play makes jack a dull boy. Resume Of Me 🤭幽然拿铁真他娘的好喝！","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}]}